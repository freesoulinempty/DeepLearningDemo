{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌲 树木检测项目 - Tree Detection Project\n",
    "\n",
    "## 项目概述 Project Overview\n",
    "\n",
    "本项目使用YOLOv5和PyTorch实现基于航空影像的树木检测任务  \n",
    "This project implements tree detection from aerial imagery using YOLOv5 and PyTorch\n",
    "\n",
    "**数据集 Dataset**: NeonTreeEvaluation Benchmark  \n",
    "**目标 Goal**: 检测航空正射影像中的树木 - Detect trees in aerial orthoimagery  \n",
    "**平台 Platform**: Google Colab (自动GPU检测 - Auto GPU detection)\n",
    "\n",
    "## 技术栈 Tech Stack\n",
    "- **深度学习框架 Deep Learning**: PyTorch + YOLOv5\n",
    "- **数据处理 Data Processing**: OpenCV, PIL, pandas\n",
    "- **可视化 Visualization**: matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 环境检测和基础设置 - Environment Detection and Basic Setup\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"🚀 环境检测 Environment Detection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 检查Python版本 Check Python version\n",
    "print(f\"Python版本 Python Version: {sys.version}\")\n",
    "\n",
    "# 检查操作系统 Check OS\n",
    "print(f\"操作系统 Operating System: {platform.system()}\")\n",
    "\n",
    "# 检查是否在Colab环境 Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"✅ 运行环境: Google Colab - Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"❌ 运行环境: 本地环境 - Running locally\")\n",
    "\n",
    "# 检查GPU可用性 Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU可用 GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA版本 CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   GPU数量 GPU Count: {torch.cuda.device_count()}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"⚠️  GPU不可用，将使用CPU - GPU not available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorch版本 PyTorch Version: {torch.__version__}\")\n",
    "print(f\"设备 Device: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 安装必要依赖 - Install Required Dependencies\n",
    "print(\"📦 安装依赖包 Installing Dependencies...\")\n",
    "\n",
    "# 安装YOLOv5和相关依赖 Install YOLOv5 and dependencies\n",
    "!pip install -q ultralytics\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q Pillow\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q pandas\n",
    "!pip install -q tqdm\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q PyYAML\n",
    "\n",
    "# 如果在Colab环境，挂载Google Drive (可选)\n",
    "# Mount Google Drive in Colab (optional)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/drive')  # 取消注释以挂载Drive - Uncomment to mount Drive\n",
    "\n",
    "print(\"✅ 依赖安装完成 Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 导入所需库 - Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置matplotlib中文字体支持 Set matplotlib Chinese font support\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子 Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"✅ 库导入完成 Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 数据下载和解压 - Data Download and Extraction\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    下载文件的函数 Function to download files\n",
    "    \"\"\"\n",
    "    print(f\"🔄 开始下载 Starting download: {filename}\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filename, 'wb') as file, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    print(f\"✅ 下载完成 Download completed: {filename}\")\n",
    "\n",
    "# 创建数据目录 Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# 数据集URL Dataset URLs\n",
    "DATASET_URL = \"https://zenodo.org/records/5914554/files/evaluation.zip?download=1\"\n",
    "ANNOTATIONS_URL = \"https://zenodo.org/records/5914554/files/annotations.zip?download=1\"\n",
    "\n",
    "# 下载数据集 Download datasets\n",
    "if not os.path.exists('data/raw/evaluation.zip'):\n",
    "    download_file(DATASET_URL, 'data/raw/evaluation.zip')\n",
    "else:\n",
    "    print(\"✅ 评估数据集已存在 Evaluation dataset already exists\")\n",
    "\n",
    "if not os.path.exists('data/raw/annotations.zip'):\n",
    "    download_file(ANNOTATIONS_URL, 'data/raw/annotations.zip')\n",
    "else:\n",
    "    print(\"✅ 标注数据已存在 Annotations already exist\")\n",
    "\n",
    "print(\"📁 数据下载完成 Data download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 解压数据集 - Extract Datasets\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    解压ZIP文件的函数 Function to extract ZIP files\n",
    "    \"\"\"\n",
    "    print(f\"📦 解压文件 Extracting: {zip_path}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    print(f\"✅ 解压完成 Extraction completed: {extract_to}\")\n",
    "\n",
    "# 解压评估数据集 Extract evaluation dataset\n",
    "if not os.path.exists('data/raw/evaluation'):\n",
    "    extract_zip('data/raw/evaluation.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"✅ 评估数据集已解压 Evaluation dataset already extracted\")\n",
    "\n",
    "# 解压标注数据 Extract annotations\n",
    "if not os.path.exists('data/raw/annotations'):\n",
    "    extract_zip('data/raw/annotations.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"✅ 标注数据已解压 Annotations already extracted\")\n",
    "\n",
    "# 查看数据结构 Explore data structure\n",
    "print(\"\\n📊 数据结构分析 Data Structure Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 检查评估数据集结构 Check evaluation dataset structure\n",
    "eval_path = Path('data/raw/evaluation')\n",
    "if eval_path.exists():\n",
    "    print(f\"评估数据集路径 Evaluation dataset path: {eval_path}\")\n",
    "    subdirs = [d for d in eval_path.iterdir() if d.is_dir()]\n",
    "    print(f\"子目录数量 Number of subdirectories: {len(subdirs)}\")\n",
    "    for subdir in subdirs[:5]:  # 显示前5个子目录 Show first 5 subdirectories\n",
    "        print(f\"  - {subdir.name}\")\n",
    "    if len(subdirs) > 5:\n",
    "        print(f\"  ... 还有 {len(subdirs)-5} 个目录 and {len(subdirs)-5} more directories\")\n",
    "\n",
    "# 检查标注数据结构 Check annotations structure\n",
    "ann_path = Path('data/raw/annotations')\n",
    "if ann_path.exists():\n",
    "    print(f\"\\n标注数据路径 Annotations path: {ann_path}\")\n",
    "    ann_files = list(ann_path.glob('*.csv'))\n",
    "    print(f\"CSV标注文件数量 Number of CSV annotation files: {len(ann_files)}\")\n",
    "    for ann_file in ann_files[:3]:  # 显示前3个标注文件 Show first 3 annotation files\n",
    "        print(f\"  - {ann_file.name}\")\n",
    "\n",
    "print(\"\\n✅ 数据解压和结构分析完成 Data extraction and structure analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 数据格式转换 - Data Format Conversion (NeonTree -> YOLO)\n",
    "class NeonTreeToYOLO:\n",
    "    \"\"\"\n",
    "    将NeonTree数据集转换为YOLO格式的类\n",
    "    Class to convert NeonTree dataset to YOLO format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, output_root):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.output_root = Path(output_root)\n",
    "        self.annotations_path = Path('data/raw/annotations')\n",
    "        \n",
    "        # 创建输出目录 Create output directories\n",
    "        self.create_yolo_structure()\n",
    "    \n",
    "    def create_yolo_structure(self):\n",
    "        \"\"\"创建YOLO数据集目录结构 Create YOLO dataset directory structure\"\"\"\n",
    "        folders = [\n",
    "            'images/train', 'images/val', 'images/test',\n",
    "            'labels/train', 'labels/val', 'labels/test'\n",
    "        ]\n",
    "        \n",
    "        for folder in folders:\n",
    "            (self.output_root / folder).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"✅ YOLO目录结构创建完成 YOLO directory structure created\")\n",
    "    \n",
    "    def load_annotations(self):\n",
    "        \"\"\"加载标注数据 Load annotation data\"\"\"\n",
    "        annotation_files = list(self.annotations_path.glob('*.csv'))\n",
    "        all_annotations = []\n",
    "        \n",
    "        for ann_file in annotation_files:\n",
    "            try:\n",
    "                df = pd.read_csv(ann_file)\n",
    "                df['site'] = ann_file.stem  # 添加站点信息 Add site information\n",
    "                all_annotations.append(df)\n",
    "                print(f\"✅ 加载标注文件 Loaded annotations: {ann_file.name} ({len(df)} 条记录 records)\")\n",
    "            except Exception as e:\n",
    "                print(f\"❌ 无法加载 Could not load: {ann_file.name} - {e}\")\n",
    "        \n",
    "        if all_annotations:\n",
    "            combined_df = pd.concat(all_annotations, ignore_index=True)\n",
    "            print(f\"📊 总标注数量 Total annotations: {len(combined_df)}\")\n",
    "            return combined_df\n",
    "        else:\n",
    "            print(\"❌ 未找到有效的标注数据 No valid annotation data found\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def find_rgb_images(self):\n",
    "        \"\"\"查找RGB图像文件 Find RGB image files\"\"\"\n",
    "        rgb_images = []\n",
    "        \n",
    "        # 搜索evaluation目录下的RGB文件夹 Search for RGB folders in evaluation directory\n",
    "        for site_dir in self.data_root.iterdir():\n",
    "            if site_dir.is_dir():\n",
    "                rgb_dir = site_dir / 'RGB'\n",
    "                if rgb_dir.exists():\n",
    "                    # 查找图像文件 Find image files\n",
    "                    for img_ext in ['*.jpg', '*.jpeg', '*.png', '*.tif', '*.tiff']:\n",
    "                        rgb_images.extend(list(rgb_dir.glob(img_ext)))\n",
    "        \n",
    "        print(f\"🖼️  找到RGB图像 Found RGB images: {len(rgb_images)}\")\n",
    "        return rgb_images\n",
    "    \n",
    "    def convert_bbox_to_yolo(self, bbox, img_width, img_height):\n",
    "        \"\"\"\n",
    "        将边界框坐标转换为YOLO格式\n",
    "        Convert bounding box coordinates to YOLO format\n",
    "        \n",
    "        YOLO格式: [class_id, x_center, y_center, width, height] (归一化 normalized)\n",
    "        \"\"\"\n",
    "        # 假设bbox格式为 [x_min, y_min, x_max, y_max]\n",
    "        # Assume bbox format is [x_min, y_min, x_max, y_max]\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        \n",
    "        # 计算中心点和宽高 Calculate center point and dimensions\n",
    "        x_center = (x_min + x_max) / 2.0\n",
    "        y_center = (y_min + y_max) / 2.0\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        \n",
    "        # 归一化 Normalize\n",
    "        x_center /= img_width\n",
    "        y_center /= img_height\n",
    "        width /= img_width\n",
    "        height /= img_height\n",
    "        \n",
    "        return [0, x_center, y_center, width, height]  # 类别ID为0 (树木 tree)\n",
    "    \n",
    "    def process_annotations(self, annotations_df, rgb_images):\n",
    "        \"\"\"\n",
    "        处理标注数据并转换为YOLO格式\n",
    "        Process annotations and convert to YOLO format\n",
    "        \"\"\"\n",
    "        processed_count = 0\n",
    "        \n",
    "        # 创建图像名称到路径的映射 Create mapping from image names to paths\n",
    "        img_name_to_path = {}\n",
    "        for img_path in rgb_images:\n",
    "            img_name = img_path.stem\n",
    "            img_name_to_path[img_name] = img_path\n",
    "        \n",
    "        print(f\"📝 开始处理标注 Starting annotation processing...\")\n",
    "        \n",
    "        for _, row in tqdm(annotations_df.iterrows(), total=len(annotations_df)):\n",
    "            try:\n",
    "                # 获取图像信息 Get image information\n",
    "                site = row.get('site', '')\n",
    "                \n",
    "                # 尝试匹配图像文件 Try to match image file\n",
    "                img_path = None\n",
    "                for img_name, path in img_name_to_path.items():\n",
    "                    if site in str(path) or img_name == site:\n",
    "                        img_path = path\n",
    "                        break\n",
    "                \n",
    "                if img_path is None:\n",
    "                    continue\n",
    "                \n",
    "                # 读取图像获取尺寸 Read image to get dimensions\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    continue\n",
    "                    \n",
    "                img_height, img_width = img.shape[:2]\n",
    "                \n",
    "                # 提取边界框信息 Extract bounding box information\n",
    "                # 根据实际的CSV列名调整 Adjust according to actual CSV column names\n",
    "                if 'xmin' in row and 'ymin' in row and 'xmax' in row and 'ymax' in row:\n",
    "                    bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]\n",
    "                elif 'left' in row and 'top' in row and 'right' in row and 'bottom' in row:\n",
    "                    bbox = [row['left'], row['top'], row['right'], row['bottom']]\n",
    "                else:\n",
    "                    # 如果找不到标准的边界框列，跳过\n",
    "                    # Skip if standard bounding box columns are not found\n",
    "                    continue\n",
    "                \n",
    "                # 转换为YOLO格式 Convert to YOLO format\n",
    "                yolo_bbox = self.convert_bbox_to_yolo(bbox, img_width, img_height)\n",
    "                \n",
    "                # 保存图像和标签 Save image and label\n",
    "                self.save_image_and_label(img_path, img, yolo_bbox, processed_count)\n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"处理标注时出错 Error processing annotation: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ 处理完成 Processing completed: {processed_count} 个样本 samples\")\n",
    "        return processed_count\n",
    "    \n",
    "    def save_image_and_label(self, original_img_path, img, yolo_bbox, index):\n",
    "        \"\"\"\n",
    "        保存图像和对应的YOLO标签\n",
    "        Save image and corresponding YOLO label\n",
    "        \"\"\"\n",
    "        # 确定数据集分割 Determine dataset split\n",
    "        if index % 10 < 7:  # 70% 训练集 training set\n",
    "            split = 'train'\n",
    "        elif index % 10 < 9:  # 20% 验证集 validation set\n",
    "            split = 'val'\n",
    "        else:  # 10% 测试集 test set\n",
    "            split = 'test'\n",
    "        \n",
    "        # 保存图像 Save image\n",
    "        img_filename = f\"tree_{index:06d}.jpg\"\n",
    "        img_save_path = self.output_root / 'images' / split / img_filename\n",
    "        cv2.imwrite(str(img_save_path), img)\n",
    "        \n",
    "        # 保存标签 Save label\n",
    "        label_filename = f\"tree_{index:06d}.txt\"\n",
    "        label_save_path = self.output_root / 'labels' / split / label_filename\n",
    "        \n",
    "        with open(label_save_path, 'w') as f:\n",
    "            # YOLO格式: class_id x_center y_center width height\n",
    "            f.write(f\"{yolo_bbox[0]} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f} {yolo_bbox[4]:.6f}\\n\")\n",
    "\n",
    "# 执行数据转换 Execute data conversion\n",
    "print(\"🔄 开始数据格式转换 Starting data format conversion...\")\n",
    "converter = NeonTreeToYOLO('data/raw/evaluation', 'data/processed')\n",
    "\n",
    "# 加载标注数据 Load annotation data\n",
    "annotations_df = converter.load_annotations()\n",
    "\n",
    "if not annotations_df.empty:\n",
    "    # 查找RGB图像 Find RGB images\n",
    "    rgb_images = converter.find_rgb_images()\n",
    "    \n",
    "    if rgb_images:\n",
    "        # 处理标注 Process annotations\n",
    "        processed_samples = converter.process_annotations(annotations_df, rgb_images)\n",
    "        print(f\"✅ 数据转换完成 Data conversion completed: {processed_samples} 个样本 samples\")\n",
    "    else:\n",
    "        print(\"❌ 未找到RGB图像 No RGB images found\")\n",
    "else:\n",
    "    print(\"❌ 未找到标注数据 No annotation data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 创建YOLO配置文件 - Create YOLO Configuration Files\n",
    "def create_dataset_yaml():\n",
    "    \"\"\"\n",
    "    创建YOLO数据集配置文件\n",
    "    Create YOLO dataset configuration file\n",
    "    \"\"\"\n",
    "    dataset_config = {\n",
    "        'train': 'data/processed/images/train',\n",
    "        'val': 'data/processed/images/val',\n",
    "        'test': 'data/processed/images/test',\n",
    "        'nc': 1,  # 类别数量 number of classes\n",
    "        'names': ['tree']  # 类别名称 class names\n",
    "    }\n",
    "    \n",
    "    # 保存配置文件 Save configuration file\n",
    "    with open('data/tree_dataset.yaml', 'w') as f:\n",
    "        yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(\"✅ YOLO数据集配置文件创建完成 YOLO dataset configuration file created\")\n",
    "    print(\"📄 配置文件路径 Configuration file path: data/tree_dataset.yaml\")\n",
    "    \n",
    "    return 'data/tree_dataset.yaml'\n",
    "\n",
    "# 创建配置文件 Create configuration file\n",
    "dataset_yaml_path = create_dataset_yaml()\n",
    "\n",
    "# 显示配置文件内容 Display configuration file content\n",
    "with open(dataset_yaml_path, 'r') as f:\n",
    "    config_content = f.read()\n",
    "    print(\"\\n📋 数据集配置内容 Dataset Configuration Content:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(config_content)\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. YOLOv5模型训练 - YOLOv5 Model Training\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 检查预训练模型 Check pre-trained model\n",
    "pretrained_model_path = 'best.pt'\n",
    "if os.path.exists(pretrained_model_path):\n",
    "    print(f\"✅ 找到预训练模型 Pre-trained model found: {pretrained_model_path}\")\n",
    "    model = YOLO(pretrained_model_path)\n",
    "else:\n",
    "    print(\"⚠️  未找到best.pt，使用YOLOv5s预训练模型 best.pt not found, using YOLOv5s pre-trained model\")\n",
    "    model = YOLO('yolov5s.pt')\n",
    "\n",
    "# 训练参数设置 Training parameters\n",
    "training_args = {\n",
    "    'data': dataset_yaml_path,        # 数据集配置文件 dataset configuration file\n",
    "    'epochs': 50,                     # 训练轮数 training epochs (可根据需要调整 adjust as needed)\n",
    "    'imgsz': 640,                     # 输入图像尺寸 input image size\n",
    "    'batch': 16,                      # 批次大小 batch size (根据GPU内存调整 adjust based on GPU memory)\n",
    "    'optimizer': 'AdamW',             # 优化器 optimizer\n",
    "    'lr0': 0.001,                     # 初始学习率 initial learning rate\n",
    "    'weight_decay': 0.0005,           # 权重衰减 weight decay\n",
    "    'warmup_epochs': 3,               # 预热轮数 warmup epochs\n",
    "    'patience': 10,                   # 早停耐心值 early stopping patience\n",
    "    'save_period': 10,                # 模型保存间隔 model save interval\n",
    "    'device': device,                 # 设备 device\n",
    "    'workers': 2,                     # 数据加载工作进程数 data loading workers\n",
    "    'project': 'runs/detect',         # 项目目录 project directory\n",
    "    'name': 'tree_detection',         # 实验名称 experiment name\n",
    "    'exist_ok': True,                 # 覆盖已存在的实验 overwrite existing experiment\n",
    "    'pretrained': True,               # 是否使用预训练权重 whether to use pre-trained weights\n",
    "    'verbose': True                   # 详细输出 verbose output\n",
    "}\n",
    "\n",
    "print(\"🚀 开始训练模型 Starting model training...\")\n",
    "print(f\"训练参数 Training parameters: {training_args}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 开始训练 Start training\n",
    "try:\n",
    "    results = model.train(**training_args)\n",
    "    print(\"✅ 模型训练完成 Model training completed!\")\n",
    "    print(f\"训练结果保存在 Training results saved in: runs/detect/tree_detection\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 训练过程中出现错误 Error during training: {e}\")\n",
    "    print(\"请检查数据集是否正确准备 Please check if dataset is properly prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 模型推理和结果可视化 - Model Inference and Result Visualization\n",
    "def load_trained_model():\n",
    "    \"\"\"\n",
    "    加载训练好的模型\n",
    "    Load trained model\n",
    "    \"\"\"\n",
    "    # 查找最新的训练结果 Find latest training results\n",
    "    model_paths = [\n",
    "        'runs/detect/tree_detection/weights/best.pt',\n",
    "        'runs/detect/tree_detection/weights/last.pt',\n",
    "        'best.pt',  # 原始预训练模型 original pre-trained model\n",
    "        'yolov5s.pt'  # 默认模型 default model\n",
    "    ]\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"✅ 加载模型 Loading model: {model_path}\")\n",
    "            return YOLO(model_path)\n",
    "    \n",
    "    print(\"❌ 未找到可用模型 No available model found\")\n",
    "    return None\n",
    "\n",
    "def visualize_predictions(model, test_images, output_dir='results'):\n",
    "    \"\"\"\n",
    "    可视化预测结果\n",
    "    Visualize prediction results\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"🖼️  开始推理 Starting inference on {len(test_images)} images...\")\n",
    "    \n",
    "    for i, img_path in enumerate(test_images[:10]):  # 限制为前10张图像 limit to first 10 images\n",
    "        try:\n",
    "            # 进行推理 Perform inference\n",
    "            results = model(str(img_path))\n",
    "            \n",
    "            # 获取预测结果 Get prediction results\n",
    "            result = results[0]\n",
    "            \n",
    "            # 在图像上绘制检测框 Draw detection boxes on image\n",
    "            annotated_img = result.plot()\n",
    "            \n",
    "            # 保存结果图像 Save result image\n",
    "            output_path = os.path.join(output_dir, f'detection_result_{i+1}.jpg')\n",
    "            cv2.imwrite(output_path, annotated_img)\n",
    "            \n",
    "            # 显示检测信息 Display detection information\n",
    "            boxes = result.boxes\n",
    "            if boxes is not None:\n",
    "                print(f\"图像 {i+1} Image {i+1}: 检测到 {len(boxes)} 棵树 trees detected\")\n",
    "                for j, box in enumerate(boxes):\n",
    "                    conf = box.conf[0].item()\n",
    "                    print(f\"  树木 Tree {j+1}: 置信度 confidence = {conf:.3f}\")\n",
    "            else:\n",
    "                print(f\"图像 {i+1} Image {i+1}: 未检测到树木 No trees detected\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"处理图像 {i+1} 时出错 Error processing image {i+1}: {e}\")\n",
    "    \n",
    "    print(f\"✅ 推理完成 Inference completed! 结果保存在 Results saved in: {output_dir}\")\n",
    "\n",
    "def display_sample_results(results_dir='results', num_samples=4):\n",
    "    \"\"\"\n",
    "    显示样本检测结果\n",
    "    Display sample detection results\n",
    "    \"\"\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        print(\"❌ 结果目录不存在 Results directory does not exist\")\n",
    "        return\n",
    "        \n",
    "    result_images = [f for f in os.listdir(results_dir) if f.endswith('.jpg')]\n",
    "    \n",
    "    if not result_images:\n",
    "        print(\"❌ 未找到结果图像 No result images found\")\n",
    "        return\n",
    "    \n",
    "    # 创建子图 Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('🌲 树木检测结果 Tree Detection Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(num_samples, len(result_images))):\n",
    "        img_path = os.path.join(results_dir, result_images[i])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'检测结果 Detection Result {i+1}', fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # 隐藏未使用的子图 Hide unused subplots\n",
    "    for i in range(len(result_images), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 加载模型并进行推理 Load model and perform inference\n",
    "model = load_trained_model()\n",
    "\n",
    "if model is not None:\n",
    "    # 获取测试图像 Get test images\n",
    "    test_image_dir = Path('data/processed/images/test')\n",
    "    if test_image_dir.exists():\n",
    "        test_images = list(test_image_dir.glob('*.jpg'))\n",
    "        if test_images:\n",
    "            # 进行推理和可视化 Perform inference and visualization\n",
    "            visualize_predictions(model, test_images)\n",
    "            \n",
    "            # 显示样本结果 Display sample results\n",
    "            display_sample_results()\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ 测试图像目录为空 Test image directory is empty\")\n",
    "    else:\n",
    "        print(\"❌ 测试图像目录不存在 Test image directory does not exist\")\n",
    "        \n",
    "        # 使用一些示例图像进行测试 Use some sample images for testing\n",
    "        eval_images = list(Path('data/raw/evaluation').rglob('*.jpg'))\n",
    "        if eval_images:\n",
    "            print(f\"📸 使用原始评估图像进行测试 Using original evaluation images for testing: {len(eval_images)}\")\n",
    "            visualize_predictions(model, eval_images[:5])  # 使用前5张图像 use first 5 images\n",
    "            display_sample_results()\n",
    "else:\n",
    "    print(\"❌ 无法加载模型进行推理 Cannot load model for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 模型评估和性能分析 - Model Evaluation and Performance Analysis\n",
    "def evaluate_model_performance(model, test_images):\n",
    "    \"\"\"\n",
    "    评估模型性能\n",
    "    Evaluate model performance\n",
    "    \"\"\"\n",
    "    print(\"📊 开始模型性能评估 Starting model performance evaluation...\")\n",
    "    \n",
    "    total_images = len(test_images)\n",
    "    total_detections = 0\n",
    "    confidence_scores = []\n",
    "    \n",
    "    # 统计检测结果 Count detection results\n",
    "    for i, img_path in enumerate(test_images):\n",
    "        try:\n",
    "            results = model(str(img_path))\n",
    "            result = results[0]\n",
    "            \n",
    "            if result.boxes is not None:\n",
    "                num_detections = len(result.boxes)\n",
    "                total_detections += num_detections\n",
    "                \n",
    "                # 收集置信度分数 Collect confidence scores\n",
    "                for box in result.boxes:\n",
    "                    conf = box.conf[0].item()\n",
    "                    confidence_scores.append(conf)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"评估图像 {i+1} 时出错 Error evaluating image {i+1}: {e}\")\n",
    "    \n",
    "    # 计算统计信息 Calculate statistics\n",
    "    avg_detections_per_image = total_detections / total_images if total_images > 0 else 0\n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "    max_confidence = np.max(confidence_scores) if confidence_scores else 0\n",
    "    min_confidence = np.min(confidence_scores) if confidence_scores else 0\n",
    "    \n",
    "    print(\"\\n📈 模型性能统计 Model Performance Statistics\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"总测试图像数量 Total test images: {total_images}\")\n",
    "    print(f\"总检测数量 Total detections: {total_detections}\")\n",
    "    print(f\"平均每张图像检测数量 Average detections per image: {avg_detections_per_image:.2f}\")\n",
    "    print(f\"平均置信度 Average confidence: {avg_confidence:.3f}\")\n",
    "    print(f\"最高置信度 Maximum confidence: {max_confidence:.3f}\")\n",
    "    print(f\"最低置信度 Minimum confidence: {min_confidence:.3f}\")\n",
    "    \n",
    "    # 绘制置信度分布图 Plot confidence distribution\n",
    "    if confidence_scores:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(confidence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        plt.title('🎯 检测置信度分布 Detection Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('置信度 Confidence Score')\n",
    "        plt.ylabel('频次 Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'total_images': total_images,\n",
    "        'total_detections': total_detections,\n",
    "        'avg_detections_per_image': avg_detections_per_image,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'avg_confidence': avg_confidence\n",
    "    }\n",
    "\n",
    "# 如果有测试图像，进行性能评估 If test images exist, perform performance evaluation\n",
    "if model is not None:\n",
    "    test_image_dir = Path('data/processed/images/test')\n",
    "    if test_image_dir.exists():\n",
    "        test_images = list(test_image_dir.glob('*.jpg'))\n",
    "        if test_images:\n",
    "            performance_stats = evaluate_model_performance(model, test_images)\n",
    "        else:\n",
    "            # 使用原始评估图像 Use original evaluation images\n",
    "            eval_images = list(Path('data/raw/evaluation').rglob('*.jpg'))\n",
    "            if eval_images:\n",
    "                performance_stats = evaluate_model_performance(model, eval_images[:20])  # 限制为20张图像\n",
    "    else:\n",
    "        print(\"⚠️  跳过性能评估，未找到测试图像 Skipping performance evaluation, no test images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 项目总结 Project Summary\n",
    "\n",
    "### 完成的功能 Completed Features\n",
    "1. ✅ **环境检测** - 自动检测GPU和Colab环境\n",
    "2. ✅ **数据下载** - 自动下载NeonTreeEvaluation数据集\n",
    "3. ✅ **数据转换** - 将NeonTree格式转换为YOLO格式\n",
    "4. ✅ **模型训练** - 使用YOLOv5进行树木检测训练\n",
    "5. ✅ **结果推理** - 对测试图像进行树木检测\n",
    "6. ✅ **结果可视化** - 显示检测结果和性能统计\n",
    "\n",
    "### 主要参数说明 Key Parameter Explanations\n",
    "- **训练轮数 Epochs**: 50轮（可根据效果调整）\n",
    "- **批次大小 Batch Size**: 16（根据GPU内存调整）\n",
    "- **图像尺寸 Image Size**: 640x640像素\n",
    "- **学习率 Learning Rate**: 0.001（AdamW优化器）\n",
    "- **数据分割 Data Split**: 70%训练/20%验证/10%测试\n",
    "\n",
    "### 使用说明 Usage Instructions\n",
    "1. 在Google Colab中运行所有代码单元\n",
    "2. 确保GPU环境可用以加速训练\n",
    "3. 根据需要调整训练参数\n",
    "4. 查看results目录中的检测结果图像\n",
    "\n",
    "### 故障排除 Troubleshooting\n",
    "- 如果内存不足，减小batch_size参数\n",
    "- 如果训练时间过长，减少epochs数量\n",
    "- 如果检测效果不佳，尝试增加训练轮数\n",
    "- 确保数据集正确下载和解压\n",
    "\n",
    "### 进一步改进 Further Improvements\n",
    "- 数据增强技术提升模型泛化能力\n",
    "- 超参数调优优化模型性能\n",
    "- 多尺度训练提高检测精度\n",
    "- 模型集成提升整体效果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
