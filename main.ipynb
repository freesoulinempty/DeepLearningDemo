{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌲 树木检测项目 - Tree Detection Project\n",
    "\n",
    "## 项目概述 Project Overview\n",
    "\n",
    "本项目使用YOLOv5和PyTorch实现基于航空影像的树木检测任务  \n",
    "This project implements tree detection from aerial imagery using YOLOv5 and PyTorch\n",
    "\n",
    "**数据集 Dataset**: NeonTreeEvaluation Benchmark  \n",
    "**目标 Goal**: 检测航空正射影像中的树木 - Detect trees in aerial orthoimagery  \n",
    "**平台 Platform**: Google Colab (自动GPU检测 - Auto GPU detection)\n",
    "\n",
    "## 技术栈 Tech Stack\n",
    "- **深度学习框架 Deep Learning**: PyTorch + YOLOv5\n",
    "- **数据处理 Data Processing**: OpenCV, PIL, pandas\n",
    "- **可视化 Visualization**: matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 1. 环境检测和基础设置 - Environment Detection and Basic Setup\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"🚀 环境检测 Environment Detection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 检查Python版本 Check Python version\n",
    "print(f\"Python版本 Python Version: {sys.version}\")\n",
    "\n",
    "# 检查操作系统 Check OS\n",
    "print(f\"操作系统 Operating System: {platform.system()}\")\n",
    "\n",
    "# 检查是否在Colab环境 Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"✅ 运行环境: Google Colab - Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"❌ 运行环境: 本地环境 - Running locally\")\n",
    "\n",
    "# 检查GPU可用性 Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU可用 GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA版本 CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   GPU数量 GPU Count: {torch.cuda.device_count()}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"⚠️  GPU不可用，将使用CPU - GPU not available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorch版本 PyTorch Version: {torch.__version__}\")\n",
    "print(f\"设备 Device: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 安装必要依赖 - Install Required Dependencies\n",
    "print(\"📦 安装依赖包 Installing Dependencies...\")\n",
    "\n",
    "# 安装YOLOv5和相关依赖 Install YOLOv5 and dependencies\n",
    "!pip install -q ultralytics\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q Pillow\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q pandas\n",
    "!pip install -q tqdm\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q PyYAML\n",
    "\n",
    "# 如果在Colab环境，挂载Google Drive (可选)\n",
    "# Mount Google Drive in Colab (optional)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/drive')  # 取消注释以挂载Drive - Uncomment to mount Drive\n",
    "\n",
    "print(\"✅ 依赖安装完成 Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 导入所需库 - Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置matplotlib中文字体支持 Set matplotlib Chinese font support\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子 Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"✅ 库导入完成 Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 数据下载和解压 - Data Download and Extraction\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    下载文件的函数 Function to download files\n",
    "    \"\"\"\n",
    "    print(f\"🔄 开始下载 Starting download: {filename}\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filename, 'wb') as file, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    print(f\"✅ 下载完成 Download completed: {filename}\")\n",
    "\n",
    "# 创建数据目录 Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# 数据集URL Dataset URLs\n",
    "DATASET_URL = \"https://zenodo.org/records/5914554/files/evaluation.zip?download=1\"\n",
    "ANNOTATIONS_URL = \"https://zenodo.org/records/5914554/files/annotations.zip?download=1\"\n",
    "\n",
    "# 下载数据集 Download datasets\n",
    "if not os.path.exists('data/raw/evaluation.zip'):\n",
    "    download_file(DATASET_URL, 'data/raw/evaluation.zip')\n",
    "else:\n",
    "    print(\"✅ 评估数据集已存在 Evaluation dataset already exists\")\n",
    "\n",
    "if not os.path.exists('data/raw/annotations.zip'):\n",
    "    download_file(ANNOTATIONS_URL, 'data/raw/annotations.zip')\n",
    "else:\n",
    "    print(\"✅ 标注数据已存在 Annotations already exist\")\n",
    "\n",
    "print(\"📁 数据下载完成 Data download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 解压数据集 - Extract Datasets\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    解压ZIP文件的函数 Function to extract ZIP files\n",
    "    \"\"\"\n",
    "    print(f\"📦 解压文件 Extracting: {zip_path}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    print(f\"✅ 解压完成 Extraction completed: {extract_to}\")\n",
    "\n",
    "# 解压评估数据集 Extract evaluation dataset\n",
    "if not os.path.exists('data/raw/evaluation'):\n",
    "    extract_zip('data/raw/evaluation.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"✅ 评估数据集已解压 Evaluation dataset already extracted\")\n",
    "\n",
    "# 解压标注数据 Extract annotations\n",
    "if not os.path.exists('data/raw/annotations'):\n",
    "    extract_zip('data/raw/annotations.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"✅ 标注数据已解压 Annotations already extracted\")\n",
    "\n",
    "# 查看数据结构 Explore data structure\n",
    "print(\"\\n📊 数据结构分析 Data Structure Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 检查评估数据集结构 Check evaluation dataset structure\n",
    "eval_path = Path('data/raw/evaluation')\n",
    "if eval_path.exists():\n",
    "    print(f\"评估数据集路径 Evaluation dataset path: {eval_path}\")\n",
    "    subdirs = [d for d in eval_path.iterdir() if d.is_dir()]\n",
    "    print(f\"子目录数量 Number of subdirectories: {len(subdirs)}\")\n",
    "    for subdir in subdirs[:5]:  # 显示前5个子目录 Show first 5 subdirectories\n",
    "        print(f\"  - {subdir.name}\")\n",
    "    if len(subdirs) > 5:\n",
    "        print(f\"  ... 还有 {len(subdirs)-5} 个目录 and {len(subdirs)-5} more directories\")\n",
    "\n",
    "# 检查标注数据结构 Check annotations structure\n",
    "ann_path = Path('data/raw/annotations')\n",
    "if ann_path.exists():\n",
    "    print(f\"\\n标注数据路径 Annotations path: {ann_path}\")\n",
    "    ann_files = list(ann_path.glob('*.csv'))\n",
    "    print(f\"CSV标注文件数量 Number of CSV annotation files: {len(ann_files)}\")\n",
    "    for ann_file in ann_files[:3]:  # 显示前3个标注文件 Show first 3 annotation files\n",
    "        print(f\"  - {ann_file.name}\")\n",
    "\n",
    "print(\"\\n✅ 数据解压和结构分析完成 Data extraction and structure analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 6. 数据格式转换 - Data Format Conversion (NeonTree -> YOLO) - 使用真实图像\nimport xml.etree.ElementTree as ET\n\nclass NeonTreeToYOLO:\n    \"\"\"\n    将NeonTree数据集转换为YOLO格式的类\n    Class to convert NeonTree dataset to YOLO format\n    \"\"\"\n    \n    def __init__(self, annotations_root, evaluation_root, output_root):\n        self.annotations_root = Path(annotations_root)\n        self.evaluation_root = Path(evaluation_root)\n        self.output_root = Path(output_root)\n        \n        # 创建输出目录 Create output directories\n        self.create_yolo_structure()\n        \n        # 创建图像文件名映射 Create image filename mapping\n        self.create_image_mapping()\n    \n    def create_yolo_structure(self):\n        \"\"\"创建YOLO数据集目录结构 Create YOLO dataset directory structure\"\"\"\n        folders = [\n            'images/train', 'images/val', 'images/test',\n            'labels/train', 'labels/val', 'labels/test'\n        ]\n        \n        for folder in folders:\n            (self.output_root / folder).mkdir(parents=True, exist_ok=True)\n        \n        print(\"✅ YOLO目录结构创建完成 YOLO directory structure created\")\n    \n    def create_image_mapping(self):\n        \"\"\"\n        创建图像文件名到路径的映射\n        Create mapping from image filenames to paths\n        \"\"\"\n        self.image_mapping = {}\n        \n        # 检查 evaluation/RGB 目录\n        rgb_dir = self.evaluation_root / 'RGB'\n        if rgb_dir.exists():\n            for img_file in rgb_dir.glob('*.tif'):\n                # 去掉扩展名作为key\n                base_name = img_file.stem\n                self.image_mapping[base_name] = img_file\n                \n                # 同时添加不带扩展名的映射\n                if base_name.endswith('.tif'):\n                    base_name_no_ext = base_name[:-4]\n                    self.image_mapping[base_name_no_ext] = img_file\n        \n        print(f\"📋 创建图像映射 Created image mapping with {len(self.image_mapping)} entries\")\n    \n    def find_matching_image(self, xml_filename):\n        \"\"\"\n        根据XML文件名查找对应的RGB图像文件\n        Find matching RGB image file based on XML filename\n        \"\"\"\n        # 从XML文件名中提取基础名称\n        base_name = xml_filename.replace('.xml', '')\n        \n        # 直接匹配\n        if base_name in self.image_mapping:\n            return self.image_mapping[base_name]\n        \n        # 尝试模糊匹配 - 查找包含base_name的图像文件\n        for img_name, img_path in self.image_mapping.items():\n            # 检查是否匹配（去掉年份等变化部分）\n            if self.files_match(base_name, img_name):\n                return img_path\n        \n        return None\n    \n    def files_match(self, xml_name, img_name):\n        \"\"\"\n        判断XML文件名和图像文件名是否匹配\n        Check if XML filename matches image filename\n        \"\"\"\n        # 移除常见的后缀和前缀\n        xml_clean = xml_name.lower()\n        img_clean = img_name.lower()\n        \n        # 提取核心站点代码\n        xml_parts = xml_clean.split('_')\n        img_parts = img_clean.split('_')\n        \n        # 如果站点代码匹配\n        if len(xml_parts) >= 2 and len(img_parts) >= 2:\n            xml_site = xml_parts[0]\n            img_site = img_parts[0]\n            \n            if xml_site == img_site:\n                # 进一步检查编号匹配\n                if len(xml_parts) >= 3 and len(img_parts) >= 2:\n                    try:\n                        xml_num = xml_parts[1]\n                        img_num = img_parts[1]\n                        return xml_num == img_num\n                    except:\n                        pass\n        \n        # 检查是否包含相同的关键部分\n        return any(part in img_clean for part in xml_parts if len(part) > 2)\n    \n    def parse_xml_annotation(self, xml_file):\n        \"\"\"\n        解析XML标注文件\n        Parse XML annotation file\n        \"\"\"\n        try:\n            tree = ET.parse(xml_file)\n            root = tree.getroot()\n            \n            # 获取图像信息 Get image information\n            filename_elem = root.find('filename')\n            if filename_elem is not None:\n                filename = filename_elem.text\n            else:\n                filename = xml_file.stem + '.tif'  # 默认扩展名\n            \n            size = root.find('size')\n            width = int(size.find('width').text)\n            height = int(size.find('height').text)\n            \n            # 获取所有树木对象 Get all tree objects\n            objects = []\n            for obj in root.findall('object'):\n                name_elem = obj.find('name')\n                if name_elem is not None and name_elem.text.lower() == 'tree':\n                    bndbox = obj.find('bndbox')\n                    xmin = int(float(bndbox.find('xmin').text))\n                    ymin = int(float(bndbox.find('ymin').text))\n                    xmax = int(float(bndbox.find('xmax').text))\n                    ymax = int(float(bndbox.find('ymax').text))\n                    \n                    # 验证边界框有效性 Validate bounding box\n                    if xmax > xmin and ymax > ymin:\n                        objects.append({\n                            'xmin': xmin,\n                            'ymin': ymin,\n                            'xmax': xmax,\n                            'ymax': ymax\n                        })\n            \n            return {\n                'filename': filename,\n                'width': width,\n                'height': height,\n                'objects': objects\n            }\n            \n        except Exception as e:\n            print(f\"解析XML文件时出错 Error parsing XML file {xml_file}: {e}\")\n            return None\n    \n    def convert_bbox_to_yolo(self, bbox, img_width, img_height):\n        \"\"\"\n        将边界框坐标转换为YOLO格式\n        Convert bounding box coordinates to YOLO format\n        \n        YOLO格式: [class_id, x_center, y_center, width, height] (归一化 normalized)\n        \"\"\"\n        xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']\n        \n        # 计算中心点和宽高 Calculate center point and dimensions\n        x_center = (xmin + xmax) / 2.0\n        y_center = (ymin + ymax) / 2.0\n        width = xmax - xmin\n        height = ymax - ymin\n        \n        # 归一化 Normalize\n        x_center /= img_width\n        y_center /= img_height\n        width /= img_width\n        height /= img_height\n        \n        return [0, x_center, y_center, width, height]  # 类别ID为0 (树木 tree)\n    \n    def process_annotations(self):\n        \"\"\"\n        处理所有XML标注文件并转换为YOLO格式\n        Process all XML annotation files and convert to YOLO format\n        \"\"\"\n        xml_files = list(self.annotations_root.glob('*.xml'))\n        processed_count = 0\n        skipped_count = 0\n        \n        print(f\"📝 找到 {len(xml_files)} 个XML标注文件 Found {len(xml_files)} XML annotation files\")\n        \n        for i, xml_file in enumerate(tqdm(xml_files, desc=\"处理标注 Processing annotations\")):\n            try:\n                # 解析XML文件 Parse XML file\n                annotation_data = self.parse_xml_annotation(xml_file)\n                \n                if annotation_data is None or not annotation_data['objects']:\n                    skipped_count += 1\n                    continue\n                \n                # 查找对应的RGB图像文件 Find corresponding RGB image file\n                img_path = self.find_matching_image(xml_file.name)\n                \n                if img_path is None:\n                    # 打印调试信息，但不中断处理 Print debug info but don't interrupt\n                    if processed_count < 5:  # 只打印前5个错误\n                        print(f\"⚠️  未找到对应图像 No matching image for: {xml_file.name}\")\n                    skipped_count += 1\n                    continue\n                \n                # 读取真实图像 Load real image\n                img = cv2.imread(str(img_path))\n                if img is None:\n                    print(f\"⚠️  无法读取图像 Cannot read image: {img_path}\")\n                    skipped_count += 1\n                    continue\n                \n                # 验证图像尺寸 Verify image dimensions\n                actual_height, actual_width = img.shape[:2]\n                if actual_width != annotation_data['width'] or actual_height != annotation_data['height']:\n                    # 使用实际图像尺寸 Use actual image dimensions\n                    annotation_data['width'] = actual_width\n                    annotation_data['height'] = actual_height\n                \n                # 确定数据集分割 Determine dataset split\n                if i % 10 < 7:  # 70% 训练集 training set\n                    split = 'train'\n                elif i % 10 < 9:  # 20% 验证集 validation set\n                    split = 'val'\n                else:  # 10% 测试集 test set\n                    split = 'test'\n                \n                # 保存图像 Save image\n                img_filename = f\"tree_{processed_count:06d}.jpg\"\n                img_save_path = self.output_root / 'images' / split / img_filename\n                cv2.imwrite(str(img_save_path), img)\n                \n                # 保存YOLO格式标签 Save YOLO format labels\n                label_filename = f\"tree_{processed_count:06d}.txt\"\n                label_save_path = self.output_root / 'labels' / split / label_filename\n                \n                with open(label_save_path, 'w') as f:\n                    for obj in annotation_data['objects']:\n                        yolo_bbox = self.convert_bbox_to_yolo(\n                            obj, annotation_data['width'], annotation_data['height']\n                        )\n                        # YOLO格式: class_id x_center y_center width height (修复换行符)\n                        f.write(f\"{yolo_bbox[0]} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f} {yolo_bbox[4]:.6f}\\n\")\n                \n                processed_count += 1\n                \n            except Exception as e:\n                print(f\"处理文件 {xml_file.name} 时出错 Error processing file {xml_file.name}: {e}\")\n                skipped_count += 1\n                continue\n        \n        print(f\"✅ 处理完成 Processing completed: {processed_count} 个样本 samples processed, {skipped_count} 个跳过 skipped\")\n        return processed_count\n\n# 执行数据转换 Execute data conversion\nprint(\"🔄 开始数据格式转换 Starting data format conversion...\")\nprint(\"📋 使用XML格式的标注数据和真实RGB图像 Using XML annotations with real RGB images\")\n\n# 创建data目录 Create data directories\nos.makedirs('data', exist_ok=True)\nos.makedirs('data/processed', exist_ok=True)\n\n# 检查annotations和evaluation数据是否存在 Check if annotations and evaluation data exist\nannotation_paths = ['data/raw/annotations', 'annotations', '/content/annotations']\neval_paths = ['data/raw/evaluation', 'evaluation', '/content/evaluation']\n\nannotations_root = None\neval_root = None\n\n# 查找annotations目录 Find annotations directory\nfor path in annotation_paths:\n    if Path(path).exists():\n        annotations_root = Path(path)\n        xml_count = len(list(annotations_root.glob('*.xml')))\n        if xml_count > 0:\n            print(f\"✅ 找到annotations数据集 Found annotations dataset at: {annotations_root} ({xml_count} XML files)\")\n            break\n\nif annotations_root is None:\n    print(\"❌ 未找到annotations数据集或XML文件\")\n    print(\"📁 请确保annotations目录存在且包含XML文件\")\n\n# 查找evaluation目录 Find evaluation directory  \nfor path in eval_paths:\n    if Path(path).exists():\n        eval_root = Path(path)\n        # 检查RGB子目录\n        rgb_dir = eval_root / 'RGB'\n        if rgb_dir.exists():\n            img_count = len(list(rgb_dir.glob('*.tif')))\n            print(f\"✅ 找到evaluation数据集 Found evaluation dataset at: {eval_root} ({img_count} images)\")\n            break\n\nif eval_root is None:\n    print(\"❌ 未找到evaluation数据集，请确保已下载并解压evaluation.zip\")\n\n# 只有当两个数据集都找到时才进行转换 Only proceed if both datasets are found\nif annotations_root is not None and eval_root is not None:\n    # 初始化转换器 Initialize converter\n    converter = NeonTreeToYOLO(annotations_root, eval_root, 'data/processed')\n    \n    # 处理标注数据 Process annotation data\n    processed_samples = converter.process_annotations()\n    \n    if processed_samples > 0:\n        print(f\"✅ 数据转换完成 Data conversion completed: {processed_samples} 个样本 samples\")\n        \n        # 显示数据集统计 Show dataset statistics\n        train_images = len(list(Path('data/processed/images/train').glob('*.jpg')))\n        val_images = len(list(Path('data/processed/images/val').glob('*.jpg')))\n        test_images = len(list(Path('data/processed/images/test').glob('*.jpg')))\n        \n        print(f\"📊 数据集统计 Dataset Statistics:\")\n        print(f\"   训练集 Training: {train_images} 张图像\")\n        print(f\"   验证集 Validation: {val_images} 张图像\")\n        print(f\"   测试集 Testing: {test_images} 张图像\")\n        print(f\"   总计 Total: {train_images + val_images + test_images} 张图像\")\n    else:\n        print(\"❌ 未能处理任何样本 No samples were processed\")\n        print(\"请检查XML文件和图像文件的匹配关系\")\nelse:\n    print(\"❌ 缺少必要的数据集，无法进行转换\")\n    print(\"请确保以下目录存在：\")\n    print(\"  - annotations 目录（包含XML文件）\")\n    print(\"  - evaluation/RGB 目录（包含.tif图像文件）\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 7. 创建YOLO配置文件 - Create YOLO Configuration Files (修复路径问题)\nimport os\nfrom pathlib import Path\nimport yaml\n\ndef create_dataset_yaml():\n    \"\"\"\n    创建YOLO数据集配置文件\n    Create YOLO dataset configuration file\n    \"\"\"\n    # 获取当前工作目录 Get current working directory\n    current_dir = os.getcwd()\n    \n    # 构建绝对路径 Build absolute paths\n    processed_dir = os.path.join(current_dir, 'data/processed')\n    train_path = os.path.join(processed_dir, 'images/train')\n    val_path = os.path.join(processed_dir, 'images/val')\n    test_path = os.path.join(processed_dir, 'images/test')\n    \n    # 检查路径是否存在 Check if paths exist\n    paths_info = {\n        'train': train_path,\n        'val': val_path,\n        'test': test_path\n    }\n    \n    print(\"📋 检查数据集路径 Checking dataset paths:\")\n    existing_paths = {}\n    \n    for split, path in paths_info.items():\n        if os.path.exists(path):\n            img_count = len([f for f in os.listdir(path) if f.endswith('.jpg')])\n            print(f\"✅ {split}: {path} ({img_count} 张图像 images)\")\n            existing_paths[split] = path\n        else:\n            print(f\"❌ {split}: {path} (路径不存在 path does not exist)\")\n    \n    # 确保至少有训练集存在 Ensure at least training set exists\n    if 'train' not in existing_paths:\n        print(\"❌ 错误：未找到训练集 Error: Training set not found\")\n        return None\n    \n    # 创建数据集配置 Create dataset configuration\n    dataset_config = {\n        'path': processed_dir,  # 数据集根目录 dataset root dir\n        'train': 'images/train',  # 相对于path的训练图像路径 train images relative to path\n        'val': 'images/val' if 'val' in existing_paths else 'images/train',  # 验证集，如果不存在则使用训练集\n        'test': 'images/test' if 'test' in existing_paths else 'images/train',  # 测试集，如果不存在则使用训练集\n        'nc': 1,                  # 类别数量 number of classes\n        'names': ['tree']         # 类别名称 class names\n    }\n    \n    # 特别处理验证集路径\n    if 'val' not in existing_paths:\n        print(\"⚠️  验证集不存在，使用训练集作为验证集 Validation set not found, using training set as validation\")\n        dataset_config['val'] = 'images/train'\n    \n    if 'test' not in existing_paths:\n        print(\"⚠️  测试集不存在，使用训练集作为测试集 Test set not found, using training set as test\")\n        dataset_config['test'] = 'images/train'\n    \n    # 确保配置目录存在\n    os.makedirs('data', exist_ok=True)\n    \n    # 保存配置文件 Save configuration file\n    config_path = 'data/tree_dataset.yaml'\n    with open(config_path, 'w') as f:\n        yaml.dump(dataset_config, f, default_flow_style=False)\n    \n    print(f\"✅ YOLO数据集配置文件创建完成 YOLO dataset configuration file created: {config_path}\")\n    \n    # 验证配置文件内容\n    print(\"\\n🔍 验证配置文件内容 Verifying configuration file:\")\n    for key, value in dataset_config.items():\n        if key in ['train', 'val', 'test']:\n            full_path = os.path.join(processed_dir, value)\n            exists = os.path.exists(full_path)\n            print(f\"   {key}: {value} -> {full_path} ({'✅' if exists else '❌'})\")\n        else:\n            print(f\"   {key}: {value}\")\n    \n    return config_path\n\ndef fix_data_structure():\n    \"\"\"\n    修复数据结构问题\n    Fix data structure issues\n    \"\"\"\n    print(\"\\n🔧 检查并修复数据结构 Checking and fixing data structure...\")\n    \n    processed_dir = Path('data/processed')\n    \n    if not processed_dir.exists():\n        print(\"❌ data/processed 目录不存在 data/processed directory does not exist\")\n        return False\n    \n    # 检查必要的目录结构\n    required_dirs = [\n        'images/train', 'images/val', 'images/test',\n        'labels/train', 'labels/val', 'labels/test'\n    ]\n    \n    missing_dirs = []\n    for dir_path in required_dirs:\n        full_path = processed_dir / dir_path\n        if not full_path.exists():\n            missing_dirs.append(dir_path)\n    \n    if missing_dirs:\n        print(f\"⚠️  缺少目录 Missing directories: {missing_dirs}\")\n        \n        # 如果只是验证集和测试集缺失，创建它们\n        for missing_dir in missing_dirs:\n            if 'val' in missing_dir or 'test' in missing_dir:\n                (processed_dir / missing_dir).mkdir(parents=True, exist_ok=True)\n                print(f\"✅ 创建目录 Created directory: {missing_dir}\")\n    \n    # 检查训练集是否有数据\n    train_images = list((processed_dir / 'images/train').glob('*.jpg'))\n    train_labels = list((processed_dir / 'labels/train').glob('*.txt'))\n    \n    print(f\"📊 训练集统计 Training set statistics:\")\n    print(f\"   图像文件 Image files: {len(train_images)}\")\n    print(f\"   标签文件 Label files: {len(train_labels)}\")\n    \n    if len(train_images) == 0:\n        print(\"❌ 训练集为空，请先运行数据转换 Training set is empty, please run data conversion first\")\n        return False\n    \n    return True\n\n# 修复数据结构\ndata_structure_ok = fix_data_structure()\n\nif data_structure_ok:\n    # 创建配置文件 Create configuration file\n    dataset_yaml_path = create_dataset_yaml()\n    \n    if dataset_yaml_path:\n        # 显示配置文件内容 Display configuration file content\n        with open(dataset_yaml_path, 'r') as f:\n            config_content = f.read()\n            print(\"\\n📋 数据集配置内容 Dataset Configuration Content:\")\n            print(\"=\" * 40)\n            print(config_content)\n            print(\"=\" * 40)\n        \n        # 最终验证\n        print(\"\\n✅ 配置文件创建并验证完成 Configuration file created and verified\")\n    else:\n        print(\"❌ 配置文件创建失败 Failed to create configuration file\")\nelse:\n    print(\"❌ 数据结构修复失败，请检查数据转换步骤 Data structure fix failed, please check data conversion step\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 8. YOLOv5模型训练 - YOLOv5 Model Training (解决resume参数问题)\nimport torch\nimport subprocess\nimport sys\nimport pickle\nimport gc  # 垃圾回收\n\n# 首先安装YOLOv5 First install YOLOv5\nprint(\"📦 安装YOLOv5... Installing YOLOv5...\")\ntry:\n    # 克隆YOLOv5仓库 Clone YOLOv5 repository\n    if not os.path.exists('yolov5'):\n        !git clone https://github.com/ultralytics/yolov5.git\n        os.chdir('yolov5')\n        !pip install -r requirements.txt\n        os.chdir('..')\n    print(\"✅ YOLOv5安装完成 YOLOv5 installation completed\")\nexcept Exception as e:\n    print(f\"YOLOv5安装警告 YOLOv5 installation warning: {e}\")\n\n# 添加yolov5到系统路径 Add yolov5 to system path\nif os.path.exists('yolov5'):\n    sys.path.append('yolov5')\n\ndef check_system_resources():\n    \"\"\"\n    检查系统资源\n    Check system resources\n    \"\"\"\n    print(\"💻 检查系统资源 Checking system resources...\")\n    \n    # 检查GPU内存\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3\n        gpu_reserved = torch.cuda.memory_reserved(0) / 1024**3\n        \n        print(f\"🎮 GPU内存 GPU Memory:\")\n        print(f\"   总内存 Total: {gpu_memory:.1f} GB\")\n        print(f\"   已分配 Allocated: {gpu_allocated:.1f} GB\")\n        print(f\"   已保留 Reserved: {gpu_reserved:.1f} GB\")\n        print(f\"   可用 Available: {gpu_memory - gpu_reserved:.1f} GB\")\n        \n        # 如果可用内存少于2GB，建议降低batch size\n        if (gpu_memory - gpu_reserved) < 2.0:\n            print(\"⚠️  GPU内存不足，建议降低batch_size\")\n            return 'low_memory'\n    \n    # 检查磁盘空间\n    import shutil\n    disk_usage = shutil.disk_usage('/')\n    free_space = disk_usage.free / 1024**3\n    \n    print(f\"💾 磁盘空间 Disk Space:\")\n    print(f\"   可用空间 Free space: {free_space:.1f} GB\")\n    \n    if free_space < 2.0:\n        print(\"⚠️  磁盘空间不足\")\n        return 'low_disk'\n    \n    return 'ok'\n\ndef fix_best_pt_compatibility(model_path, output_path='best_fixed.pt'):\n    \"\"\"\n    修复best.pt模型的兼容性问题 (内存优化版)\n    Fix compatibility issues with best.pt model (memory optimized)\n    \"\"\"\n    print(f\"🔧 尝试修复模型兼容性 Attempting to fix model compatibility: {model_path}\")\n    \n    try:\n        # 方法1: 使用weights_only=False加载\n        print(\"   方法1: 使用weights_only=False加载 Method 1: Load with weights_only=False\")\n        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n        \n        if 'model' in checkpoint:\n            print(\"   ✅ 成功加载模型 Successfully loaded model\")\n            \n            # 提取模型权重和关键信息\n            model_state = checkpoint['model']\n            \n            # 创建新的兼容检查点 Create new compatible checkpoint\n            new_checkpoint = {\n                'model': model_state,\n                'epoch': checkpoint.get('epoch', 0),\n                'best_fitness': checkpoint.get('best_fitness', 0.0),\n            }\n            \n            # 清理不必要的内存占用\n            del checkpoint\n            gc.collect()\n            \n            # 保存修复后的模型 Save fixed model\n            torch.save(new_checkpoint, output_path)\n            print(f\"   ✅ 修复后的模型已保存 Fixed model saved: {output_path}\")\n            return output_path\n            \n    except Exception as e1:\n        print(f\"   ❌ 方法1失败 Method 1 failed: {e1}\")\n        # 清理内存\n        gc.collect()\n        \n        try:\n            # 方法3: 仅提取权重字典 (最轻量)\n            print(\"   方法2: 仅提取state_dict Method 2: Extract state_dict only\")\n            \n            checkpoint = torch.load(model_path, map_location='cpu')\n            \n            # 尝试各种可能的结构\n            state_dict = None\n            if 'model' in checkpoint:\n                if hasattr(checkpoint['model'], 'state_dict'):\n                    state_dict = checkpoint['model'].state_dict()\n                elif isinstance(checkpoint['model'], dict):\n                    state_dict = checkpoint['model']\n            elif 'state_dict' in checkpoint:\n                state_dict = checkpoint['state_dict']\n            \n            if state_dict is not None:\n                # 保存纯权重文件\n                torch.save({'model': state_dict}, output_path)\n                print(f\"   ✅ state_dict提取成功 state_dict extraction successful: {output_path}\")\n                return output_path\n                    \n        except Exception as e3:\n            print(f\"   ❌ 方法2失败 Method 2 failed: {e3}\")\n    \n    print(\"   ❌ 所有修复方法都失败了 All fix methods failed\")\n    return None\n\n# 检查系统资源\nresource_status = check_system_resources()\n\n# 根据资源情况调整参数\nif resource_status == 'low_memory':\n    batch_size = 4\n    img_size = 416  # 降低图像尺寸\n    epochs = 10     # 减少训练轮数\n    workers = 1\n    print(\"🔧 检测到低内存，使用轻量化配置\")\nelif resource_status == 'low_disk':\n    batch_size = 8\n    img_size = 640\n    epochs = 15\n    workers = 1\n    print(\"🔧 检测到磁盘空间不足，减少epochs\")\nelse:\n    batch_size = 8\n    img_size = 640\n    epochs = 20  # 减少默认epochs避免长时间运行\n    workers = 2\n\n# 检查并修复预训练模型 Check and fix pre-trained model\npretrained_model_path = 'best.pt'\nuse_custom_model = False\nfixed_model_path = None\n\nif os.path.exists(pretrained_model_path):\n    print(f\"✅ 找到预训练模型 Pre-trained model found: {pretrained_model_path}\")\n    \n    # 尝试修复模型兼容性\n    fixed_model_path = fix_best_pt_compatibility(pretrained_model_path)\n    \n    if fixed_model_path and os.path.exists(fixed_model_path):\n        print(f\"🎯 使用修复后的模型 Using fixed model: {fixed_model_path}\")\n        weights_path = fixed_model_path\n        use_custom_model = True\n    else:\n        print(\"⚠️  模型修复失败，将使用YOLOv5s作为基础模型\")\n        weights_path = 'yolov5s.pt'\nelse:\n    print(\"⚠️  未找到best.pt，将使用YOLOv5s预训练模型\")\n    weights_path = 'yolov5s.pt'\n\nprint(\"\\n⚙️  训练参数配置 Training parameters configuration:\")\nprint(f\"   图像尺寸 Image size: {img_size}\")\nprint(f\"   批次大小 Batch size: {batch_size}\")\nprint(f\"   训练轮数 Epochs: {epochs}\")\nprint(f\"   数据集配置 Dataset config: {dataset_yaml_path}\")\nprint(f\"   模型权重 Model weights: {weights_path}\")\nprint(f\"   设备 Device: {device}\")\nprint(f\"   资源状态 Resource status: {resource_status}\")\nprint(\"=\" * 60)\n\n# 构建训练命令 Build training command (修复resume参数问题)\ntraining_command = [\n    sys.executable, 'yolov5/train.py',\n    '--img', str(img_size),\n    '--batch', str(batch_size),\n    '--epochs', str(epochs),\n    '--data', dataset_yaml_path,\n    '--weights', weights_path,\n    '--project', 'runs/train',\n    '--name', 'tree_detection_light',  # 轻量版命名\n    '--patience', '5',    # 更早的早停\n    '--save-period', '3', # 更频繁保存\n    '--workers', str(workers),\n    '--cache', 'ram',     # 使用RAM缓存而不是磁盘\n    '--exist-ok'\n    # 移除了有问题的 --resume 参数\n]\n\n# 如果有GPU，添加device参数 If GPU available, add device parameter\nif torch.cuda.is_available():\n    training_command.extend(['--device', '0'])\n\nprint(\"🚀 开始YOLOv5轻量化训练 Starting YOLOv5 lightweight training...\")\nprint(f\"训练命令 Training command: {' '.join(training_command)}\")\nprint(\"💡 已优化内存使用并修复resume参数问题 Memory usage optimized and resume parameter issue fixed\")\nprint(\"=\" * 60)\n\n# 清理内存\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# 开始训练 Start training\ntry:\n    print(\"⏳ 训练开始，请耐心等待... Training started, please wait...\")\n    result = subprocess.run(training_command, \n                          capture_output=True, \n                          text=True, \n                          cwd='.',\n                          timeout=1800)  # 30分钟超时\n    \n    if result.returncode == 0:\n        print(\"✅ YOLOv5模型训练完成 YOLOv5 model training completed!\")\n        print(f\"训练结果保存在 Training results saved in: runs/train/tree_detection_light\")\n        \n        # 显示训练输出的最后几行 Show last few lines of training output\n        output_lines = result.stdout.split('\\n')\n        print(\"\\n📊 训练输出摘要 Training output summary:\")\n        for line in output_lines[-10:]:\n            if line.strip():\n                print(f\"   {line}\")\n                \n    else:\n        print(f\"❌ 训练失败 Training failed with return code: {result.returncode}\")\n        print(\"错误输出 Error output:\")\n        print(result.stderr[:1000])  # 限制错误输出长度\n\nexcept subprocess.TimeoutExpired:\n    print(\"⏰ 训练超时，可能需要减少epochs或batch_size\")\nexcept KeyboardInterrupt:\n    print(\"⚠️  训练被用户中断 Training interrupted by user\")\nexcept Exception as e:\n    print(f\"❌ 训练过程中出现错误 Error during training: {e}\")\n\n# 检查训练结果 Check training results\nweights_dir = Path('runs/train/tree_detection_light/weights')\nif weights_dir.exists():\n    best_pt = weights_dir / 'best.pt'\n    last_pt = weights_dir / 'last.pt'\n    \n    if best_pt.exists():\n        print(f\"✅ 最佳模型保存于 Best model saved at: {best_pt}\")\n        # 设置模型路径供后续使用\n        model_path = str(best_pt)\n    elif last_pt.exists():\n        print(f\"✅ 最后模型保存于 Last model saved at: {last_pt}\")\n        model_path = str(last_pt)\n    else:\n        model_path = None\n        \nelse:\n    print(\"⚠️  未找到训练结果目录 Training results directory not found\")\n    model_path = None\n\n# 清理内存\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"🧹 内存清理完成 Memory cleanup completed\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 9. 防崩溃GPU优化推理 - Crash-Resistant GPU Optimized Inference\nimport torch\nimport gc\nimport os\nimport sys\nimport subprocess\nimport time\nimport signal\nfrom pathlib import Path\n\n# 设置进程超时处理\ndef timeout_handler(signum, frame):\n    print(\"⏰ 推理进程超时，强制结束\")\n    raise TimeoutError(\"Inference process timeout\")\n\ndef comprehensive_cleanup():\n    \"\"\"\n    综合内存和GPU清理 - 增强版\n    Enhanced comprehensive memory and GPU cleanup\n    \"\"\"\n    try:\n        print(\"🧹 执行深度内存清理...\")\n        \n        # 1. Python垃圾回收\n        collected = gc.collect()\n        print(f\"   垃圾回收释放对象: {collected}\")\n        \n        # 2. CUDA缓存清理\n        if torch.cuda.is_available():\n            # 清空所有GPU缓存\n            torch.cuda.empty_cache()\n            \n            # 同步所有CUDA流\n            torch.cuda.synchronize()\n            \n            # 清理IPC缓存\n            torch.cuda.ipc_collect()\n            \n            # 重置峰值内存统计\n            torch.cuda.reset_peak_memory_stats()\n            \n            # 获取清理后的内存状态\n            allocated = torch.cuda.memory_allocated() / 1024**3\n            reserved = torch.cuda.memory_reserved() / 1024**3\n            print(f\"   CUDA内存清理后: 已分配 {allocated:.1f}GB, 已保留 {reserved:.1f}GB\")\n        \n        # 3. 强制系统内存释放\n        import psutil\n        process = psutil.Process(os.getpid())\n        memory_mb = process.memory_info().rss / 1024 / 1024\n        print(f\"   系统内存使用: {memory_mb:.1f} MB\")\n        \n        # 4. 删除临时变量\n        import sys\n        local_vars = list(locals().keys())\n        for var in local_vars:\n            if var.startswith('temp_') or var.startswith('_'):\n                try:\n                    del locals()[var]\n                except:\n                    pass\n        \n        return True\n        \n    except Exception as e:\n        print(f\"⚠️  清理过程中出现警告: {e}\")\n        return False\n\ndef check_system_stability():\n    \"\"\"\n    检查系统稳定性\n    Check system stability\n    \"\"\"\n    print(\"🔍 检查系统稳定性...\")\n    \n    try:\n        # 检查内存使用率\n        import psutil\n        memory = psutil.virtual_memory()\n        memory_usage = memory.percent\n        \n        print(f\"   系统内存使用率: {memory_usage:.1f}%\")\n        \n        if memory_usage > 90:\n            print(\"⚠️  系统内存使用率过高，可能导致崩溃\")\n            return 'critical'\n        elif memory_usage > 80:\n            print(\"⚠️  系统内存使用率较高\")\n            return 'warning'\n        \n        # 检查GPU状态\n        if torch.cuda.is_available():\n            try:\n                # 简单的GPU测试\n                test_tensor = torch.zeros(10, 10).cuda()\n                test_result = test_tensor.sum()\n                del test_tensor\n                torch.cuda.empty_cache()\n                \n                gpu_memory = torch.cuda.get_device_properties(0).total_memory\n                allocated = torch.cuda.memory_allocated()\n                usage_percent = (allocated / gpu_memory) * 100\n                \n                print(f\"   GPU内存使用率: {usage_percent:.1f}%\")\n                \n                if usage_percent > 90:\n                    return 'critical'\n                elif usage_percent > 70:\n                    return 'warning'\n                    \n            except Exception as e:\n                print(f\"   GPU健康检查失败: {e}\")\n                return 'gpu_error'\n        \n        # 检查磁盘空间\n        disk = psutil.disk_usage('/')\n        disk_usage = (disk.used / disk.total) * 100\n        \n        print(f\"   磁盘使用率: {disk_usage:.1f}%\")\n        \n        if disk_usage > 95:\n            return 'critical'\n        \n        print(\"✅ 系统状态良好\")\n        return 'stable'\n        \n    except Exception as e:\n        print(f\"❌ 系统稳定性检查失败: {e}\")\n        return 'error'\n\ndef safe_gpu_test():\n    \"\"\"\n    安全的GPU测试\n    Safe GPU test\n    \"\"\"\n    print(\"🧪 执行安全GPU测试...\")\n    \n    try:\n        if not torch.cuda.is_available():\n            print(\"❌ CUDA不可用\")\n            return False\n        \n        # 测试1: 基础张量操作\n        test1 = torch.randn(100, 100, device='cuda')\n        result1 = test1 @ test1.T\n        del test1, result1\n        \n        # 测试2: 内存分配和释放\n        for size in [100, 500, 1000]:\n            test_tensor = torch.zeros(size, size, device='cuda')\n            del test_tensor\n            torch.cuda.empty_cache()\n        \n        # 测试3: 设备同步\n        torch.cuda.synchronize()\n        \n        print(\"✅ GPU测试通过\")\n        return True\n        \n    except Exception as e:\n        print(f\"❌ GPU测试失败: {e}\")\n        comprehensive_cleanup()\n        return False\n\ndef ultra_safe_inference(model_path, test_image_path, output_dir='ultra_safe_results'):\n    \"\"\"\n    超安全推理模式 - 单张图像处理\n    Ultra-safe inference mode - single image processing\n    \"\"\"\n    print(f\"🛡️  超安全推理模式: {test_image_path}\")\n    \n    try:\n        # 创建独立的输出目录\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # 设置最保守的参数\n        safe_params = {\n            'img_size': 320,  # 最小图像尺寸\n            'conf': 0.5,      # 较高置信度阈值\n            'device': '0' if torch.cuda.is_available() else 'cpu'\n        }\n        \n        print(f\"   安全参数: {safe_params}\")\n        \n        # 构建超保守的推理命令\n        cmd = [\n            sys.executable, 'yolov5/detect.py',\n            '--weights', model_path,\n            '--source', str(test_image_path),\n            '--project', output_dir,\n            '--name', 'safe_detect',\n            '--img', str(safe_params['img_size']),\n            '--conf', str(safe_params['conf']),\n            '--device', safe_params['device'],\n            '--save-txt',\n            '--line-thickness', '1',\n            '--exist-ok',\n            '--nosave'  # 不保存图像，只要检测结果\n        ]\n        \n        print(f\"   执行命令: {' '.join(cmd)}\")\n        \n        # 设置超时处理\n        start_time = time.time()\n        \n        # 执行推理\n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            timeout=60,  # 1分钟超时\n            cwd='.'\n        )\n        \n        inference_time = time.time() - start_time\n        \n        if result.returncode == 0:\n            print(f\"✅ 推理成功 (耗时: {inference_time:.1f}s)\")\n            \n            # 检查结果\n            result_dir = Path(output_dir) / 'safe_detect'\n            detection_count = 0\n            \n            if result_dir.exists():\n                label_dir = result_dir / 'labels'\n                if label_dir.exists():\n                    for label_file in label_dir.glob('*.txt'):\n                        with open(label_file, 'r') as f:\n                            lines = f.readlines()\n                            detection_count = len([l for l in lines if l.strip()])\n            \n            print(f\"   检测到 {detection_count} 个对象\")\n            return {\n                'success': True,\n                'detections': detection_count,\n                'time': inference_time,\n                'output': result_dir\n            }\n        else:\n            print(f\"❌ 推理失败: {result.stderr[:200]}\")\n            return {'success': False, 'error': result.stderr}\n            \n    except subprocess.TimeoutExpired:\n        print(\"⏰ 推理超时\")\n        return {'success': False, 'error': 'timeout'}\n    except Exception as e:\n        print(f\"❌ 推理异常: {e}\")\n        return {'success': False, 'error': str(e)}\n    finally:\n        # 强制清理\n        comprehensive_cleanup()\n\ndef progressive_inference_test(model_path, test_images_dir):\n    \"\"\"\n    渐进式推理测试 - 逐步增加复杂度\n    Progressive inference test - gradually increase complexity\n    \"\"\"\n    print(\"🔄 开始渐进式推理测试...\")\n    \n    # 获取测试图像\n    test_images = list(Path(test_images_dir).glob('*.jpg'))\n    if not test_images:\n        print(\"❌ 未找到测试图像\")\n        return False\n    \n    print(f\"找到 {len(test_images)} 张测试图像\")\n    \n    results = []\n    max_images = min(3, len(test_images))  # 最多测试3张\n    \n    for i in range(max_images):\n        print(f\"\\n--- 测试图像 {i+1}/{max_images} ---\")\n        \n        # 每次推理前检查系统状态\n        stability = check_system_stability()\n        \n        if stability in ['critical', 'error']:\n            print(f\"⚠️  系统状态异常 ({stability})，停止测试\")\n            break\n        \n        # 执行超安全推理\n        img_path = test_images[i]\n        result = ultra_safe_inference(model_path, img_path, f'progressive_test_{i+1}')\n        \n        if result['success']:\n            results.append(result)\n            print(f\"✅ 图像 {i+1} 处理成功\")\n        else:\n            print(f\"❌ 图像 {i+1} 处理失败: {result.get('error', 'unknown')}\")\n            # 失败后进行额外清理\n            comprehensive_cleanup()\n            time.sleep(2)  # 休息2秒\n        \n        # 强制清理和短暂休息\n        comprehensive_cleanup()\n        time.sleep(1)\n    \n    # 汇总结果\n    success_count = len(results)\n    print(f\"\\n📊 渐进式测试汇总:\")\n    print(f\"   成功率: {success_count}/{max_images} ({success_count/max_images*100:.1f}%)\")\n    \n    if results:\n        total_detections = sum(r['detections'] for r in results)\n        avg_time = sum(r['time'] for r in results) / len(results)\n        print(f\"   总检测数: {total_detections}\")\n        print(f\"   平均推理时间: {avg_time:.1f}s\")\n    \n    return success_count > 0\n\n# 主执行流程 - 防崩溃版本\nprint(\"🚀 启动防崩溃GPU推理系统...\")\n\n# 初始深度清理\ncomprehensive_cleanup()\n\n# 系统稳定性检查\nstability_status = check_system_stability()\n\nif stability_status in ['critical', 'error']:\n    print(\"❌ 系统状态不稳定，无法安全进行推理\")\n    print(\"建议重启内核或减少其他程序的资源占用\")\nelse:\n    # GPU安全测试\n    gpu_safe = safe_gpu_test()\n    \n    if not gpu_safe:\n        print(\"❌ GPU测试失败，将使用CPU模式\")\n    \n    # 查找可用模型\n    model_candidates = [\n        'runs/train/tree_detection_light/weights/best.pt',\n        'runs/train/tree_detection/weights/best.pt', \n        'best_fixed.pt',\n        'yolov5s.pt'\n    ]\n    \n    selected_model = None\n    for model_path in model_candidates:\n        if os.path.exists(model_path):\n            model_size = os.path.getsize(model_path) / (1024 * 1024)\n            print(f\"🎯 找到模型: {model_path} ({model_size:.1f}MB)\")\n            selected_model = model_path\n            break\n    \n    if not selected_model:\n        print(\"❌ 未找到可用模型\")\n    else:\n        # 查找测试图像\n        test_dirs = [\n            'data/processed/images/test',\n            'data/processed/images/val',\n            'data/processed/images/train'\n        ]\n        \n        test_images_dir = None\n        for test_dir in test_dirs:\n            if os.path.exists(test_dir):\n                img_count = len(list(Path(test_dir).glob('*.jpg')))\n                if img_count > 0:\n                    test_images_dir = test_dir\n                    print(f\"📁 使用测试图像: {test_dir} ({img_count}张)\")\n                    break\n        \n        if test_images_dir:\n            # 执行渐进式推理测试\n            if progressive_inference_test(selected_model, test_images_dir):\n                print(\"✅ 防崩溃推理测试成功完成\")\n            else:\n                print(\"❌ 推理测试失败\")\n        else:\n            print(\"❌ 未找到测试图像\")\n\n# 最终系统清理\nprint(\"\\n🧹 执行最终系统清理...\")\ncomprehensive_cleanup()\n\n# 最终状态检查\nfinal_status = check_system_stability()\nprint(f\"📋 最终系统状态: {final_status}\")\nprint(\"✅ 防崩溃推理系统运行完成\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 10. YOLOv5模型评估和性能分析 - YOLOv5 Model Evaluation and Performance Analysis\ndef evaluate_yolov5_performance(model_path, test_images):\n    \"\"\"\n    评估YOLOv5模型性能\n    Evaluate YOLOv5 model performance\n    \"\"\"\n    print(\"📊 开始YOLOv5模型性能评估 Starting YOLOv5 model performance evaluation...\")\n    \n    total_images = len(test_images)\n    total_detections = 0\n    confidence_scores = []\n    processing_times = []\n    \n    # 创建临时目录存储评估结果 Create temporary directory for evaluation results\n    eval_dir = 'temp_eval'\n    os.makedirs(eval_dir, exist_ok=True)\n    \n    print(f\"🔍 评估 {total_images} 张图像 Evaluating {total_images} images...\")\n    \n    # 对每张图像进行检测 Detect on each image\n    for i, img_path in enumerate(test_images):\n        try:\n            import time\n            start_time = time.time()\n            \n            # 构建检测命令 Build detection command\n            detect_command = [\n                sys.executable, 'yolov5/detect.py',\n                '--weights', model_path,\n                '--source', str(img_path),\n                '--project', eval_dir,\n                '--name', f'eval_{i}',\n                '--save-txt',  # 保存检测结果 save detection results\n                '--save-conf', # 保存置信度 save confidence scores\n                '--exist-ok',\n                '--nosave'     # 不保存图像，只要文本结果 don't save images, only text results\n            ]\n            \n            # 运行检测 Run detection\n            result = subprocess.run(detect_command, \n                                  capture_output=True, \n                                  text=True, \n                                  cwd='.')\n            \n            processing_time = time.time() - start_time\n            processing_times.append(processing_time)\n            \n            if result.returncode == 0:\n                # 读取检测结果 Read detection results\n                result_dir = Path(eval_dir) / f'eval_{i}' / 'labels'\n                if result_dir.exists():\n                    for label_file in result_dir.glob('*.txt'):\n                        with open(label_file, 'r') as f:\n                            lines = f.readlines()\n                            for line in lines:\n                                if line.strip():\n                                    parts = line.strip().split()\n                                    if len(parts) >= 6:  # class x y w h conf\n                                        confidence = float(parts[5])\n                                        confidence_scores.append(confidence)\n                                        total_detections += 1\n                                        \n            if (i + 1) % 10 == 0 or i == total_images - 1:\n                print(f\"   已处理 {i+1}/{total_images} 张图像 Processed {i+1}/{total_images} images\")\n                \n        except Exception as e:\n            print(f\"评估图像 {i+1} 时出错 Error evaluating image {i+1}: {e}\")\n            continue\n    \n    # 清理临时目录 Clean up temporary directory\n    import shutil\n    if os.path.exists(eval_dir):\n        shutil.rmtree(eval_dir)\n    \n    # 计算统计信息 Calculate statistics\n    avg_detections_per_image = total_detections / total_images if total_images > 0 else 0\n    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n    max_confidence = np.max(confidence_scores) if confidence_scores else 0\n    min_confidence = np.min(confidence_scores) if confidence_scores else 0\n    avg_processing_time = np.mean(processing_times) if processing_times else 0\n    \n    print(\"\\n📈 YOLOv5模型性能统计 YOLOv5 Model Performance Statistics\")\n    print(\"=\" * 60)\n    print(f\"总测试图像数量 Total test images: {total_images}\")\n    print(f\"总检测数量 Total detections: {total_detections}\")\n    print(f\"平均每张图像检测数量 Average detections per image: {avg_detections_per_image:.2f}\")\n    print(f\"平均置信度 Average confidence: {avg_confidence:.3f}\")\n    print(f\"最高置信度 Maximum confidence: {max_confidence:.3f}\")\n    print(f\"最低置信度 Minimum confidence: {min_confidence:.3f}\")\n    print(f\"平均处理时间 Average processing time: {avg_processing_time:.3f} 秒/张 seconds per image\")\n    \n    # 创建可视化图表 Create visualization charts\n    if confidence_scores or processing_times:\n        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # 置信度分布图 Confidence distribution\n        if confidence_scores:\n            axes[0].hist(confidence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n            axes[0].set_title('🎯 检测置信度分布 Detection Confidence Distribution', fontsize=12, fontweight='bold')\n            axes[0].set_xlabel('置信度 Confidence Score')\n            axes[0].set_ylabel('频次 Frequency')\n            axes[0].grid(True, alpha=0.3)\n            \n            # 添加统计线 Add statistical lines\n            axes[0].axvline(avg_confidence, color='red', linestyle='--', \n                          label=f'平均值 Mean: {avg_confidence:.3f}')\n            axes[0].legend()\n        else:\n            axes[0].text(0.5, 0.5, '无置信度数据\\nNo Confidence Data', \n                       ha='center', va='center', transform=axes[0].transAxes)\n        \n        # 处理时间分布图 Processing time distribution\n        if processing_times:\n            axes[1].hist(processing_times, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n            axes[1].set_title('⏱️ 处理时间分布 Processing Time Distribution', fontsize=12, fontweight='bold')\n            axes[1].set_xlabel('处理时间 Processing Time (seconds)')\n            axes[1].set_ylabel('频次 Frequency')\n            axes[1].grid(True, alpha=0.3)\n            \n            # 添加统计线 Add statistical lines\n            axes[1].axvline(avg_processing_time, color='red', linestyle='--', \n                          label=f'平均值 Mean: {avg_processing_time:.3f}s')\n            axes[1].legend()\n        else:\n            axes[1].text(0.5, 0.5, '无处理时间数据\\nNo Processing Time Data', \n                       ha='center', va='center', transform=axes[1].transAxes)\n        \n        plt.tight_layout()\n        plt.show()\n    \n    return {\n        'total_images': total_images,\n        'total_detections': total_detections,\n        'avg_detections_per_image': avg_detections_per_image,\n        'confidence_scores': confidence_scores,\n        'avg_confidence': avg_confidence,\n        'processing_times': processing_times,\n        'avg_processing_time': avg_processing_time\n    }\n\ndef run_yolov5_validation(model_path, dataset_yaml):\n    \"\"\"\n    运行YOLOv5官方验证脚本\n    Run YOLOv5 official validation script\n    \"\"\"\n    print(\"🔬 运行YOLOv5官方验证 Running YOLOv5 official validation...\")\n    \n    try:\n        # 构建验证命令 Build validation command\n        val_command = [\n            sys.executable, 'yolov5/val.py',\n            '--weights', model_path,\n            '--data', dataset_yaml,\n            '--img', '640',\n            '--batch', '8',\n            '--conf', '0.001',  # 低置信度阈值以获得更多检测 low confidence threshold for more detections\n            '--iou', '0.6',     # IoU阈值 IoU threshold\n            '--task', 'val',\n            '--device', '0' if torch.cuda.is_available() else 'cpu',\n            '--save-txt',\n            '--save-conf',\n            '--project', 'runs/val',\n            '--name', 'tree_detection_val',\n            '--exist-ok'\n        ]\n        \n        print(f\"验证命令 Validation command: {' '.join(val_command)}\")\n        \n        # 运行验证 Run validation\n        result = subprocess.run(val_command, \n                              capture_output=True, \n                              text=True, \n                              cwd='.')\n        \n        if result.returncode == 0:\n            print(\"✅ YOLOv5官方验证完成 YOLOv5 official validation completed!\")\n            \n            # 显示验证输出的关键信息 Show key information from validation output\n            output_lines = result.stdout.split('\\n')\n            print(\"\\n📊 验证结果摘要 Validation Results Summary:\")\n            print(\"-\" * 50)\n            \n            for line in output_lines:\n                if any(keyword in line.lower() for keyword in ['precision', 'recall', 'map', 'f1']):\n                    print(f\"   {line.strip()}\")\n            \n            # 检查是否生成了结果文件 Check if result files were generated\n            val_results_dir = Path('runs/val/tree_detection_val')\n            if val_results_dir.exists():\n                print(f\"\\n📁 验证结果保存在 Validation results saved in: {val_results_dir}\")\n                \n                # 列出生成的文件 List generated files\n                result_files = list(val_results_dir.glob('*'))\n                for f in result_files:\n                    if f.is_file():\n                        print(f\"   {f.name}\")\n            \n        else:\n            print(f\"❌ YOLOv5验证失败 YOLOv5 validation failed\")\n            print(f\"错误输出 Error output: {result.stderr}\")\n            \n    except Exception as e:\n        print(f\"❌ 验证过程中出现错误 Error during validation: {e}\")\n\n# 如果有训练好的模型，进行性能评估 If trained model exists, perform performance evaluation\nif 'model_path' in locals() and model_path is not None and os.path.exists(model_path):\n    print(f\"🎯 使用模型进行性能评估 Using model for performance evaluation: {model_path}\")\n    \n    # 获取测试图像 Get test images\n    test_image_dir = Path('data/processed/images/test')\n    eval_images = []\n    \n    if test_image_dir.exists():\n        eval_images = list(test_image_dir.glob('*.jpg'))\n    \n    # 如果没有处理过的测试图像，使用原始评估图像 If no processed test images, use original evaluation images\n    if not eval_images:\n        eval_rgb_dir = Path('data/raw/evaluation/RGB')\n        if eval_rgb_dir.exists():\n            eval_images = list(eval_rgb_dir.glob('*.tif'))[:20]  # 限制为20张图像\n            print(f\"📸 使用原始评估图像 Using original evaluation images: {len(eval_images)}\")\n    \n    if eval_images:\n        # 进行性能评估 Perform performance evaluation\n        performance_stats = evaluate_yolov5_performance(model_path, eval_images)\n        \n        # 运行官方验证（如果数据集配置存在）Run official validation if dataset config exists\n        if 'dataset_yaml_path' in locals() and os.path.exists(dataset_yaml_path):\n            run_yolov5_validation(model_path, dataset_yaml_path)\n        \n    else:\n        print(\"⚠️  未找到测试图像，跳过性能评估 No test images found, skipping performance evaluation\")\n        \nelse:\n    print(\"⚠️  未找到训练好的模型，跳过性能评估 No trained model found, skipping performance evaluation\")\n    print(\"请先完成模型训练 Please complete model training first\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 项目总结 Project Summary\n",
    "\n",
    "### 完成的功能 Completed Features\n",
    "1. ✅ **环境检测** - 自动检测GPU和Colab环境\n",
    "2. ✅ **数据下载** - 自动下载NeonTreeEvaluation数据集\n",
    "3. ✅ **数据转换** - 将NeonTree格式转换为YOLO格式\n",
    "4. ✅ **模型训练** - 使用YOLOv5进行树木检测训练\n",
    "5. ✅ **结果推理** - 对测试图像进行树木检测\n",
    "6. ✅ **结果可视化** - 显示检测结果和性能统计\n",
    "\n",
    "### 主要参数说明 Key Parameter Explanations\n",
    "- **训练轮数 Epochs**: 50轮（可根据效果调整）\n",
    "- **批次大小 Batch Size**: 16（根据GPU内存调整）\n",
    "- **图像尺寸 Image Size**: 640x640像素\n",
    "- **学习率 Learning Rate**: 0.001（AdamW优化器）\n",
    "- **数据分割 Data Split**: 70%训练/20%验证/10%测试\n",
    "\n",
    "### 使用说明 Usage Instructions\n",
    "1. 在Google Colab中运行所有代码单元\n",
    "2. 确保GPU环境可用以加速训练\n",
    "3. 根据需要调整训练参数\n",
    "4. 查看results目录中的检测结果图像\n",
    "\n",
    "### 故障排除 Troubleshooting\n",
    "- 如果内存不足，减小batch_size参数\n",
    "- 如果训练时间过长，减少epochs数量\n",
    "- 如果检测效果不佳，尝试增加训练轮数\n",
    "- 确保数据集正确下载和解压\n",
    "\n",
    "### 进一步改进 Further Improvements\n",
    "- 数据增强技术提升模型泛化能力\n",
    "- 超参数调优优化模型性能\n",
    "- 多尺度训练提高检测精度\n",
    "- 模型集成提升整体效果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}