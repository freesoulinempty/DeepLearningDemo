{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ² æ ‘æœ¨æ£€æµ‹é¡¹ç›® - Tree Detection Project\n",
    "\n",
    "## é¡¹ç›®æ¦‚è¿° Project Overview\n",
    "\n",
    "æœ¬é¡¹ç›®ä½¿ç”¨YOLOv5å’ŒPyTorchå®ç°åŸºäºèˆªç©ºå½±åƒçš„æ ‘æœ¨æ£€æµ‹ä»»åŠ¡  \n",
    "This project implements tree detection from aerial imagery using YOLOv5 and PyTorch\n",
    "\n",
    "**æ•°æ®é›† Dataset**: NeonTreeEvaluation Benchmark  \n",
    "**ç›®æ ‡ Goal**: æ£€æµ‹èˆªç©ºæ­£å°„å½±åƒä¸­çš„æ ‘æœ¨ - Detect trees in aerial orthoimagery  \n",
    "**å¹³å° Platform**: Google Colab (è‡ªåŠ¨GPUæ£€æµ‹ - Auto GPU detection)\n",
    "\n",
    "## æŠ€æœ¯æ ˆ Tech Stack\n",
    "- **æ·±åº¦å­¦ä¹ æ¡†æ¶ Deep Learning**: PyTorch + YOLOv5\n",
    "- **æ•°æ®å¤„ç† Data Processing**: OpenCV, PIL, pandas\n",
    "- **å¯è§†åŒ– Visualization**: matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ç¯å¢ƒæ£€æµ‹å’ŒåŸºç¡€è®¾ç½® - Environment Detection and Basic Setup\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸš€ ç¯å¢ƒæ£€æµ‹ Environment Detection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥Pythonç‰ˆæœ¬ Check Python version\n",
    "print(f\"Pythonç‰ˆæœ¬ Python Version: {sys.version}\")\n",
    "\n",
    "# æ£€æŸ¥æ“ä½œç³»ç»Ÿ Check OS\n",
    "print(f\"æ“ä½œç³»ç»Ÿ Operating System: {platform.system()}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦åœ¨Colabç¯å¢ƒ Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… è¿è¡Œç¯å¢ƒ: Google Colab - Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"âŒ è¿è¡Œç¯å¢ƒ: æœ¬åœ°ç¯å¢ƒ - Running locally\")\n",
    "\n",
    "# æ£€æŸ¥GPUå¯ç”¨æ€§ Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPUå¯ç”¨ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDAç‰ˆæœ¬ CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   GPUæ•°é‡ GPU Count: {torch.cuda.device_count()}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU - GPU not available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"è®¾å¤‡ Device: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. å®‰è£…å¿…è¦ä¾èµ– - Install Required Dependencies\n",
    "print(\"ğŸ“¦ å®‰è£…ä¾èµ–åŒ… Installing Dependencies...\")\n",
    "\n",
    "# å®‰è£…YOLOv5å’Œç›¸å…³ä¾èµ– Install YOLOv5 and dependencies\n",
    "!pip install -q ultralytics\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q Pillow\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q pandas\n",
    "!pip install -q tqdm\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q PyYAML\n",
    "\n",
    "# å¦‚æœåœ¨Colabç¯å¢ƒï¼ŒæŒ‚è½½Google Drive (å¯é€‰)\n",
    "# Mount Google Drive in Colab (optional)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/drive')  # å–æ¶ˆæ³¨é‡Šä»¥æŒ‚è½½Drive - Uncomment to mount Drive\n",
    "\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. å¯¼å…¥æ‰€éœ€åº“ - Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ Set matplotlib Chinese font support\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥å®Œæˆ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. æ•°æ®ä¸‹è½½å’Œè§£å‹ - Data Download and Extraction\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    ä¸‹è½½æ–‡ä»¶çš„å‡½æ•° Function to download files\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ å¼€å§‹ä¸‹è½½ Starting download: {filename}\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filename, 'wb') as file, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    print(f\"âœ… ä¸‹è½½å®Œæˆ Download completed: {filename}\")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®ç›®å½• Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# æ•°æ®é›†URL Dataset URLs\n",
    "DATASET_URL = \"https://zenodo.org/records/5914554/files/evaluation.zip?download=1\"\n",
    "ANNOTATIONS_URL = \"https://zenodo.org/records/5914554/files/annotations.zip?download=1\"\n",
    "\n",
    "# ä¸‹è½½æ•°æ®é›† Download datasets\n",
    "if not os.path.exists('data/raw/evaluation.zip'):\n",
    "    download_file(DATASET_URL, 'data/raw/evaluation.zip')\n",
    "else:\n",
    "    print(\"âœ… è¯„ä¼°æ•°æ®é›†å·²å­˜åœ¨ Evaluation dataset already exists\")\n",
    "\n",
    "if not os.path.exists('data/raw/annotations.zip'):\n",
    "    download_file(ANNOTATIONS_URL, 'data/raw/annotations.zip')\n",
    "else:\n",
    "    print(\"âœ… æ ‡æ³¨æ•°æ®å·²å­˜åœ¨ Annotations already exist\")\n",
    "\n",
    "print(\"ğŸ“ æ•°æ®ä¸‹è½½å®Œæˆ Data download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. è§£å‹æ•°æ®é›† - Extract Datasets\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    è§£å‹ZIPæ–‡ä»¶çš„å‡½æ•° Function to extract ZIP files\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“¦ è§£å‹æ–‡ä»¶ Extracting: {zip_path}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    print(f\"âœ… è§£å‹å®Œæˆ Extraction completed: {extract_to}\")\n",
    "\n",
    "# è§£å‹è¯„ä¼°æ•°æ®é›† Extract evaluation dataset\n",
    "if not os.path.exists('data/raw/evaluation'):\n",
    "    extract_zip('data/raw/evaluation.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"âœ… è¯„ä¼°æ•°æ®é›†å·²è§£å‹ Evaluation dataset already extracted\")\n",
    "\n",
    "# è§£å‹æ ‡æ³¨æ•°æ® Extract annotations\n",
    "if not os.path.exists('data/raw/annotations'):\n",
    "    extract_zip('data/raw/annotations.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"âœ… æ ‡æ³¨æ•°æ®å·²è§£å‹ Annotations already extracted\")\n",
    "\n",
    "# æŸ¥çœ‹æ•°æ®ç»“æ„ Explore data structure\n",
    "print(\"\\nğŸ“Š æ•°æ®ç»“æ„åˆ†æ Data Structure Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥è¯„ä¼°æ•°æ®é›†ç»“æ„ Check evaluation dataset structure\n",
    "eval_path = Path('data/raw/evaluation')\n",
    "if eval_path.exists():\n",
    "    print(f\"è¯„ä¼°æ•°æ®é›†è·¯å¾„ Evaluation dataset path: {eval_path}\")\n",
    "    subdirs = [d for d in eval_path.iterdir() if d.is_dir()]\n",
    "    print(f\"å­ç›®å½•æ•°é‡ Number of subdirectories: {len(subdirs)}\")\n",
    "    for subdir in subdirs[:5]:  # æ˜¾ç¤ºå‰5ä¸ªå­ç›®å½• Show first 5 subdirectories\n",
    "        print(f\"  - {subdir.name}\")\n",
    "    if len(subdirs) > 5:\n",
    "        print(f\"  ... è¿˜æœ‰ {len(subdirs)-5} ä¸ªç›®å½• and {len(subdirs)-5} more directories\")\n",
    "\n",
    "# æ£€æŸ¥æ ‡æ³¨æ•°æ®ç»“æ„ Check annotations structure\n",
    "ann_path = Path('data/raw/annotations')\n",
    "if ann_path.exists():\n",
    "    print(f\"\\næ ‡æ³¨æ•°æ®è·¯å¾„ Annotations path: {ann_path}\")\n",
    "    ann_files = list(ann_path.glob('*.csv'))\n",
    "    print(f\"CSVæ ‡æ³¨æ–‡ä»¶æ•°é‡ Number of CSV annotation files: {len(ann_files)}\")\n",
    "    for ann_file in ann_files[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ ‡æ³¨æ–‡ä»¶ Show first 3 annotation files\n",
    "        print(f\"  - {ann_file.name}\")\n",
    "\n",
    "print(\"\\nâœ… æ•°æ®è§£å‹å’Œç»“æ„åˆ†æå®Œæˆ Data extraction and structure analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. æ•°æ®æ ¼å¼è½¬æ¢ - Data Format Conversion (NeonTree -> YOLO)\n",
    "class NeonTreeToYOLO:\n",
    "    \"\"\"\n",
    "    å°†NeonTreeæ•°æ®é›†è½¬æ¢ä¸ºYOLOæ ¼å¼çš„ç±»\n",
    "    Class to convert NeonTree dataset to YOLO format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_root, output_root):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.output_root = Path(output_root)\n",
    "        self.annotations_path = Path('data/raw/annotations')\n",
    "        \n",
    "        # åˆ›å»ºè¾“å‡ºç›®å½• Create output directories\n",
    "        self.create_yolo_structure()\n",
    "    \n",
    "    def create_yolo_structure(self):\n",
    "        \"\"\"åˆ›å»ºYOLOæ•°æ®é›†ç›®å½•ç»“æ„ Create YOLO dataset directory structure\"\"\"\n",
    "        folders = [\n",
    "            'images/train', 'images/val', 'images/test',\n",
    "            'labels/train', 'labels/val', 'labels/test'\n",
    "        ]\n",
    "        \n",
    "        for folder in folders:\n",
    "            (self.output_root / folder).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"âœ… YOLOç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ YOLO directory structure created\")\n",
    "    \n",
    "    def load_annotations(self):\n",
    "        \"\"\"åŠ è½½æ ‡æ³¨æ•°æ® Load annotation data\"\"\"\n",
    "        annotation_files = list(self.annotations_path.glob('*.csv'))\n",
    "        all_annotations = []\n",
    "        \n",
    "        for ann_file in annotation_files:\n",
    "            try:\n",
    "                df = pd.read_csv(ann_file)\n",
    "                df['site'] = ann_file.stem  # æ·»åŠ ç«™ç‚¹ä¿¡æ¯ Add site information\n",
    "                all_annotations.append(df)\n",
    "                print(f\"âœ… åŠ è½½æ ‡æ³¨æ–‡ä»¶ Loaded annotations: {ann_file.name} ({len(df)} æ¡è®°å½• records)\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ æ— æ³•åŠ è½½ Could not load: {ann_file.name} - {e}\")\n",
    "        \n",
    "        if all_annotations:\n",
    "            combined_df = pd.concat(all_annotations, ignore_index=True)\n",
    "            print(f\"ğŸ“Š æ€»æ ‡æ³¨æ•°é‡ Total annotations: {len(combined_df)}\")\n",
    "            return combined_df\n",
    "        else:\n",
    "            print(\"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ ‡æ³¨æ•°æ® No valid annotation data found\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def find_rgb_images(self):\n",
    "        \"\"\"æŸ¥æ‰¾RGBå›¾åƒæ–‡ä»¶ Find RGB image files\"\"\"\n",
    "        rgb_images = []\n",
    "        \n",
    "        # æœç´¢evaluationç›®å½•ä¸‹çš„RGBæ–‡ä»¶å¤¹ Search for RGB folders in evaluation directory\n",
    "        for site_dir in self.data_root.iterdir():\n",
    "            if site_dir.is_dir():\n",
    "                rgb_dir = site_dir / 'RGB'\n",
    "                if rgb_dir.exists():\n",
    "                    # æŸ¥æ‰¾å›¾åƒæ–‡ä»¶ Find image files\n",
    "                    for img_ext in ['*.jpg', '*.jpeg', '*.png', '*.tif', '*.tiff']:\n",
    "                        rgb_images.extend(list(rgb_dir.glob(img_ext)))\n",
    "        \n",
    "        print(f\"ğŸ–¼ï¸  æ‰¾åˆ°RGBå›¾åƒ Found RGB images: {len(rgb_images)}\")\n",
    "        return rgb_images\n",
    "    \n",
    "    def convert_bbox_to_yolo(self, bbox, img_width, img_height):\n",
    "        \"\"\"\n",
    "        å°†è¾¹ç•Œæ¡†åæ ‡è½¬æ¢ä¸ºYOLOæ ¼å¼\n",
    "        Convert bounding box coordinates to YOLO format\n",
    "        \n",
    "        YOLOæ ¼å¼: [class_id, x_center, y_center, width, height] (å½’ä¸€åŒ– normalized)\n",
    "        \"\"\"\n",
    "        # å‡è®¾bboxæ ¼å¼ä¸º [x_min, y_min, x_max, y_max]\n",
    "        # Assume bbox format is [x_min, y_min, x_max, y_max]\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        \n",
    "        # è®¡ç®—ä¸­å¿ƒç‚¹å’Œå®½é«˜ Calculate center point and dimensions\n",
    "        x_center = (x_min + x_max) / 2.0\n",
    "        y_center = (y_min + y_max) / 2.0\n",
    "        width = x_max - x_min\n",
    "        height = y_max - y_min\n",
    "        \n",
    "        # å½’ä¸€åŒ– Normalize\n",
    "        x_center /= img_width\n",
    "        y_center /= img_height\n",
    "        width /= img_width\n",
    "        height /= img_height\n",
    "        \n",
    "        return [0, x_center, y_center, width, height]  # ç±»åˆ«IDä¸º0 (æ ‘æœ¨ tree)\n",
    "    \n",
    "    def process_annotations(self, annotations_df, rgb_images):\n",
    "        \"\"\"\n",
    "        å¤„ç†æ ‡æ³¨æ•°æ®å¹¶è½¬æ¢ä¸ºYOLOæ ¼å¼\n",
    "        Process annotations and convert to YOLO format\n",
    "        \"\"\"\n",
    "        processed_count = 0\n",
    "        \n",
    "        # åˆ›å»ºå›¾åƒåç§°åˆ°è·¯å¾„çš„æ˜ å°„ Create mapping from image names to paths\n",
    "        img_name_to_path = {}\n",
    "        for img_path in rgb_images:\n",
    "            img_name = img_path.stem\n",
    "            img_name_to_path[img_name] = img_path\n",
    "        \n",
    "        print(f\"ğŸ“ å¼€å§‹å¤„ç†æ ‡æ³¨ Starting annotation processing...\")\n",
    "        \n",
    "        for _, row in tqdm(annotations_df.iterrows(), total=len(annotations_df)):\n",
    "            try:\n",
    "                # è·å–å›¾åƒä¿¡æ¯ Get image information\n",
    "                site = row.get('site', '')\n",
    "                \n",
    "                # å°è¯•åŒ¹é…å›¾åƒæ–‡ä»¶ Try to match image file\n",
    "                img_path = None\n",
    "                for img_name, path in img_name_to_path.items():\n",
    "                    if site in str(path) or img_name == site:\n",
    "                        img_path = path\n",
    "                        break\n",
    "                \n",
    "                if img_path is None:\n",
    "                    continue\n",
    "                \n",
    "                # è¯»å–å›¾åƒè·å–å°ºå¯¸ Read image to get dimensions\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    continue\n",
    "                    \n",
    "                img_height, img_width = img.shape[:2]\n",
    "                \n",
    "                # æå–è¾¹ç•Œæ¡†ä¿¡æ¯ Extract bounding box information\n",
    "                # æ ¹æ®å®é™…çš„CSVåˆ—åè°ƒæ•´ Adjust according to actual CSV column names\n",
    "                if 'xmin' in row and 'ymin' in row and 'xmax' in row and 'ymax' in row:\n",
    "                    bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]\n",
    "                elif 'left' in row and 'top' in row and 'right' in row and 'bottom' in row:\n",
    "                    bbox = [row['left'], row['top'], row['right'], row['bottom']]\n",
    "                else:\n",
    "                    # å¦‚æœæ‰¾ä¸åˆ°æ ‡å‡†çš„è¾¹ç•Œæ¡†åˆ—ï¼Œè·³è¿‡\n",
    "                    # Skip if standard bounding box columns are not found\n",
    "                    continue\n",
    "                \n",
    "                # è½¬æ¢ä¸ºYOLOæ ¼å¼ Convert to YOLO format\n",
    "                yolo_bbox = self.convert_bbox_to_yolo(bbox, img_width, img_height)\n",
    "                \n",
    "                # ä¿å­˜å›¾åƒå’Œæ ‡ç­¾ Save image and label\n",
    "                self.save_image_and_label(img_path, img, yolo_bbox, processed_count)\n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"å¤„ç†æ ‡æ³¨æ—¶å‡ºé”™ Error processing annotation: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ… å¤„ç†å®Œæˆ Processing completed: {processed_count} ä¸ªæ ·æœ¬ samples\")\n",
    "        return processed_count\n",
    "    \n",
    "    def save_image_and_label(self, original_img_path, img, yolo_bbox, index):\n",
    "        \"\"\"\n",
    "        ä¿å­˜å›¾åƒå’Œå¯¹åº”çš„YOLOæ ‡ç­¾\n",
    "        Save image and corresponding YOLO label\n",
    "        \"\"\"\n",
    "        # ç¡®å®šæ•°æ®é›†åˆ†å‰² Determine dataset split\n",
    "        if index % 10 < 7:  # 70% è®­ç»ƒé›† training set\n",
    "            split = 'train'\n",
    "        elif index % 10 < 9:  # 20% éªŒè¯é›† validation set\n",
    "            split = 'val'\n",
    "        else:  # 10% æµ‹è¯•é›† test set\n",
    "            split = 'test'\n",
    "        \n",
    "        # ä¿å­˜å›¾åƒ Save image\n",
    "        img_filename = f\"tree_{index:06d}.jpg\"\n",
    "        img_save_path = self.output_root / 'images' / split / img_filename\n",
    "        cv2.imwrite(str(img_save_path), img)\n",
    "        \n",
    "        # ä¿å­˜æ ‡ç­¾ Save label\n",
    "        label_filename = f\"tree_{index:06d}.txt\"\n",
    "        label_save_path = self.output_root / 'labels' / split / label_filename\n",
    "        \n",
    "        with open(label_save_path, 'w') as f:\n",
    "            # YOLOæ ¼å¼: class_id x_center y_center width height\n",
    "            f.write(f\"{yolo_bbox[0]} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f} {yolo_bbox[4]:.6f}\\n\")\n",
    "\n",
    "# æ‰§è¡Œæ•°æ®è½¬æ¢ Execute data conversion\n",
    "print(\"ğŸ”„ å¼€å§‹æ•°æ®æ ¼å¼è½¬æ¢ Starting data format conversion...\")\n",
    "converter = NeonTreeToYOLO('data/raw/evaluation', 'data/processed')\n",
    "\n",
    "# åŠ è½½æ ‡æ³¨æ•°æ® Load annotation data\n",
    "annotations_df = converter.load_annotations()\n",
    "\n",
    "if not annotations_df.empty:\n",
    "    # æŸ¥æ‰¾RGBå›¾åƒ Find RGB images\n",
    "    rgb_images = converter.find_rgb_images()\n",
    "    \n",
    "    if rgb_images:\n",
    "        # å¤„ç†æ ‡æ³¨ Process annotations\n",
    "        processed_samples = converter.process_annotations(annotations_df, rgb_images)\n",
    "        print(f\"âœ… æ•°æ®è½¬æ¢å®Œæˆ Data conversion completed: {processed_samples} ä¸ªæ ·æœ¬ samples\")\n",
    "    else:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°RGBå›¾åƒ No RGB images found\")\n",
    "else:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°æ ‡æ³¨æ•°æ® No annotation data found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. åˆ›å»ºYOLOé…ç½®æ–‡ä»¶ - Create YOLO Configuration Files\n",
    "def create_dataset_yaml():\n",
    "    \"\"\"\n",
    "    åˆ›å»ºYOLOæ•°æ®é›†é…ç½®æ–‡ä»¶\n",
    "    Create YOLO dataset configuration file\n",
    "    \"\"\"\n",
    "    dataset_config = {\n",
    "        'train': 'data/processed/images/train',\n",
    "        'val': 'data/processed/images/val',\n",
    "        'test': 'data/processed/images/test',\n",
    "        'nc': 1,  # ç±»åˆ«æ•°é‡ number of classes\n",
    "        'names': ['tree']  # ç±»åˆ«åç§° class names\n",
    "    }\n",
    "    \n",
    "    # ä¿å­˜é…ç½®æ–‡ä»¶ Save configuration file\n",
    "    with open('data/tree_dataset.yaml', 'w') as f:\n",
    "        yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(\"âœ… YOLOæ•°æ®é›†é…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ YOLO dataset configuration file created\")\n",
    "    print(\"ğŸ“„ é…ç½®æ–‡ä»¶è·¯å¾„ Configuration file path: data/tree_dataset.yaml\")\n",
    "    \n",
    "    return 'data/tree_dataset.yaml'\n",
    "\n",
    "# åˆ›å»ºé…ç½®æ–‡ä»¶ Create configuration file\n",
    "dataset_yaml_path = create_dataset_yaml()\n",
    "\n",
    "# æ˜¾ç¤ºé…ç½®æ–‡ä»¶å†…å®¹ Display configuration file content\n",
    "with open(dataset_yaml_path, 'r') as f:\n",
    "    config_content = f.read()\n",
    "    print(\"\\nğŸ“‹ æ•°æ®é›†é…ç½®å†…å®¹ Dataset Configuration Content:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(config_content)\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. YOLOv5æ¨¡å‹è®­ç»ƒ - YOLOv5 Model Training\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹ Check pre-trained model\n",
    "pretrained_model_path = 'best.pt'\n",
    "if os.path.exists(pretrained_model_path):\n",
    "    print(f\"âœ… æ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ Pre-trained model found: {pretrained_model_path}\")\n",
    "    model = YOLO(pretrained_model_path)\n",
    "else:\n",
    "    print(\"âš ï¸  æœªæ‰¾åˆ°best.ptï¼Œä½¿ç”¨YOLOv5sé¢„è®­ç»ƒæ¨¡å‹ best.pt not found, using YOLOv5s pre-trained model\")\n",
    "    model = YOLO('yolov5s.pt')\n",
    "\n",
    "# è®­ç»ƒå‚æ•°è®¾ç½® Training parameters\n",
    "training_args = {\n",
    "    'data': dataset_yaml_path,        # æ•°æ®é›†é…ç½®æ–‡ä»¶ dataset configuration file\n",
    "    'epochs': 50,                     # è®­ç»ƒè½®æ•° training epochs (å¯æ ¹æ®éœ€è¦è°ƒæ•´ adjust as needed)\n",
    "    'imgsz': 640,                     # è¾“å…¥å›¾åƒå°ºå¯¸ input image size\n",
    "    'batch': 16,                      # æ‰¹æ¬¡å¤§å° batch size (æ ¹æ®GPUå†…å­˜è°ƒæ•´ adjust based on GPU memory)\n",
    "    'optimizer': 'AdamW',             # ä¼˜åŒ–å™¨ optimizer\n",
    "    'lr0': 0.001,                     # åˆå§‹å­¦ä¹ ç‡ initial learning rate\n",
    "    'weight_decay': 0.0005,           # æƒé‡è¡°å‡ weight decay\n",
    "    'warmup_epochs': 3,               # é¢„çƒ­è½®æ•° warmup epochs\n",
    "    'patience': 10,                   # æ—©åœè€å¿ƒå€¼ early stopping patience\n",
    "    'save_period': 10,                # æ¨¡å‹ä¿å­˜é—´éš” model save interval\n",
    "    'device': device,                 # è®¾å¤‡ device\n",
    "    'workers': 2,                     # æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•° data loading workers\n",
    "    'project': 'runs/detect',         # é¡¹ç›®ç›®å½• project directory\n",
    "    'name': 'tree_detection',         # å®éªŒåç§° experiment name\n",
    "    'exist_ok': True,                 # è¦†ç›–å·²å­˜åœ¨çš„å®éªŒ overwrite existing experiment\n",
    "    'pretrained': True,               # æ˜¯å¦ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ whether to use pre-trained weights\n",
    "    'verbose': True                   # è¯¦ç»†è¾“å‡º verbose output\n",
    "}\n",
    "\n",
    "print(\"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹ Starting model training...\")\n",
    "print(f\"è®­ç»ƒå‚æ•° Training parameters: {training_args}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ Start training\n",
    "try:\n",
    "    results = model.train(**training_args)\n",
    "    print(\"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ Model training completed!\")\n",
    "    print(f\"è®­ç»ƒç»“æœä¿å­˜åœ¨ Training results saved in: runs/detect/tree_detection\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ Error during training: {e}\")\n",
    "    print(\"è¯·æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æ­£ç¡®å‡†å¤‡ Please check if dataset is properly prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. æ¨¡å‹æ¨ç†å’Œç»“æœå¯è§†åŒ– - Model Inference and Result Visualization\n",
    "def load_trained_model():\n",
    "    \"\"\"\n",
    "    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "    Load trained model\n",
    "    \"\"\"\n",
    "    # æŸ¥æ‰¾æœ€æ–°çš„è®­ç»ƒç»“æœ Find latest training results\n",
    "    model_paths = [\n",
    "        'runs/detect/tree_detection/weights/best.pt',\n",
    "        'runs/detect/tree_detection/weights/last.pt',\n",
    "        'best.pt',  # åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ original pre-trained model\n",
    "        'yolov5s.pt'  # é»˜è®¤æ¨¡å‹ default model\n",
    "    ]\n",
    "    \n",
    "    for model_path in model_paths:\n",
    "        if os.path.exists(model_path):\n",
    "            print(f\"âœ… åŠ è½½æ¨¡å‹ Loading model: {model_path}\")\n",
    "            return YOLO(model_path)\n",
    "    \n",
    "    print(\"âŒ æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ No available model found\")\n",
    "    return None\n",
    "\n",
    "def visualize_predictions(model, test_images, output_dir='results'):\n",
    "    \"\"\"\n",
    "    å¯è§†åŒ–é¢„æµ‹ç»“æœ\n",
    "    Visualize prediction results\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"ğŸ–¼ï¸  å¼€å§‹æ¨ç† Starting inference on {len(test_images)} images...\")\n",
    "    \n",
    "    for i, img_path in enumerate(test_images[:10]):  # é™åˆ¶ä¸ºå‰10å¼ å›¾åƒ limit to first 10 images\n",
    "        try:\n",
    "            # è¿›è¡Œæ¨ç† Perform inference\n",
    "            results = model(str(img_path))\n",
    "            \n",
    "            # è·å–é¢„æµ‹ç»“æœ Get prediction results\n",
    "            result = results[0]\n",
    "            \n",
    "            # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹æ¡† Draw detection boxes on image\n",
    "            annotated_img = result.plot()\n",
    "            \n",
    "            # ä¿å­˜ç»“æœå›¾åƒ Save result image\n",
    "            output_path = os.path.join(output_dir, f'detection_result_{i+1}.jpg')\n",
    "            cv2.imwrite(output_path, annotated_img)\n",
    "            \n",
    "            # æ˜¾ç¤ºæ£€æµ‹ä¿¡æ¯ Display detection information\n",
    "            boxes = result.boxes\n",
    "            if boxes is not None:\n",
    "                print(f\"å›¾åƒ {i+1} Image {i+1}: æ£€æµ‹åˆ° {len(boxes)} æ£µæ ‘ trees detected\")\n",
    "                for j, box in enumerate(boxes):\n",
    "                    conf = box.conf[0].item()\n",
    "                    print(f\"  æ ‘æœ¨ Tree {j+1}: ç½®ä¿¡åº¦ confidence = {conf:.3f}\")\n",
    "            else:\n",
    "                print(f\"å›¾åƒ {i+1} Image {i+1}: æœªæ£€æµ‹åˆ°æ ‘æœ¨ No trees detected\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"å¤„ç†å›¾åƒ {i+1} æ—¶å‡ºé”™ Error processing image {i+1}: {e}\")\n",
    "    \n",
    "    print(f\"âœ… æ¨ç†å®Œæˆ Inference completed! ç»“æœä¿å­˜åœ¨ Results saved in: {output_dir}\")\n",
    "\n",
    "def display_sample_results(results_dir='results', num_samples=4):\n",
    "    \"\"\"\n",
    "    æ˜¾ç¤ºæ ·æœ¬æ£€æµ‹ç»“æœ\n",
    "    Display sample detection results\n",
    "    \"\"\"\n",
    "    if not os.path.exists(results_dir):\n",
    "        print(\"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨ Results directory does not exist\")\n",
    "        return\n",
    "        \n",
    "    result_images = [f for f in os.listdir(results_dir) if f.endswith('.jpg')]\n",
    "    \n",
    "    if not result_images:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°ç»“æœå›¾åƒ No result images found\")\n",
    "        return\n",
    "    \n",
    "    # åˆ›å»ºå­å›¾ Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('ğŸŒ² æ ‘æœ¨æ£€æµ‹ç»“æœ Tree Detection Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(num_samples, len(result_images))):\n",
    "        img_path = os.path.join(results_dir, result_images[i])\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f'æ£€æµ‹ç»“æœ Detection Result {i+1}', fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # éšè—æœªä½¿ç”¨çš„å­å›¾ Hide unused subplots\n",
    "    for i in range(len(result_images), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œæ¨ç† Load model and perform inference\n",
    "model = load_trained_model()\n",
    "\n",
    "if model is not None:\n",
    "    # è·å–æµ‹è¯•å›¾åƒ Get test images\n",
    "    test_image_dir = Path('data/processed/images/test')\n",
    "    if test_image_dir.exists():\n",
    "        test_images = list(test_image_dir.glob('*.jpg'))\n",
    "        if test_images:\n",
    "            # è¿›è¡Œæ¨ç†å’Œå¯è§†åŒ– Perform inference and visualization\n",
    "            visualize_predictions(model, test_images)\n",
    "            \n",
    "            # æ˜¾ç¤ºæ ·æœ¬ç»“æœ Display sample results\n",
    "            display_sample_results()\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ æµ‹è¯•å›¾åƒç›®å½•ä¸ºç©º Test image directory is empty\")\n",
    "    else:\n",
    "        print(\"âŒ æµ‹è¯•å›¾åƒç›®å½•ä¸å­˜åœ¨ Test image directory does not exist\")\n",
    "        \n",
    "        # ä½¿ç”¨ä¸€äº›ç¤ºä¾‹å›¾åƒè¿›è¡Œæµ‹è¯• Use some sample images for testing\n",
    "        eval_images = list(Path('data/raw/evaluation').rglob('*.jpg'))\n",
    "        if eval_images:\n",
    "            print(f\"ğŸ“¸ ä½¿ç”¨åŸå§‹è¯„ä¼°å›¾åƒè¿›è¡Œæµ‹è¯• Using original evaluation images for testing: {len(eval_images)}\")\n",
    "            visualize_predictions(model, eval_images[:5])  # ä½¿ç”¨å‰5å¼ å›¾åƒ use first 5 images\n",
    "            display_sample_results()\n",
    "else:\n",
    "    print(\"âŒ æ— æ³•åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç† Cannot load model for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. æ¨¡å‹è¯„ä¼°å’Œæ€§èƒ½åˆ†æ - Model Evaluation and Performance Analysis\n",
    "def evaluate_model_performance(model, test_images):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
    "    Evaluate model performance\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š å¼€å§‹æ¨¡å‹æ€§èƒ½è¯„ä¼° Starting model performance evaluation...\")\n",
    "    \n",
    "    total_images = len(test_images)\n",
    "    total_detections = 0\n",
    "    confidence_scores = []\n",
    "    \n",
    "    # ç»Ÿè®¡æ£€æµ‹ç»“æœ Count detection results\n",
    "    for i, img_path in enumerate(test_images):\n",
    "        try:\n",
    "            results = model(str(img_path))\n",
    "            result = results[0]\n",
    "            \n",
    "            if result.boxes is not None:\n",
    "                num_detections = len(result.boxes)\n",
    "                total_detections += num_detections\n",
    "                \n",
    "                # æ”¶é›†ç½®ä¿¡åº¦åˆ†æ•° Collect confidence scores\n",
    "                for box in result.boxes:\n",
    "                    conf = box.conf[0].item()\n",
    "                    confidence_scores.append(conf)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"è¯„ä¼°å›¾åƒ {i+1} æ—¶å‡ºé”™ Error evaluating image {i+1}: {e}\")\n",
    "    \n",
    "    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ Calculate statistics\n",
    "    avg_detections_per_image = total_detections / total_images if total_images > 0 else 0\n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "    max_confidence = np.max(confidence_scores) if confidence_scores else 0\n",
    "    min_confidence = np.min(confidence_scores) if confidence_scores else 0\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ æ¨¡å‹æ€§èƒ½ç»Ÿè®¡ Model Performance Statistics\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"æ€»æµ‹è¯•å›¾åƒæ•°é‡ Total test images: {total_images}\")\n",
    "    print(f\"æ€»æ£€æµ‹æ•°é‡ Total detections: {total_detections}\")\n",
    "    print(f\"å¹³å‡æ¯å¼ å›¾åƒæ£€æµ‹æ•°é‡ Average detections per image: {avg_detections_per_image:.2f}\")\n",
    "    print(f\"å¹³å‡ç½®ä¿¡åº¦ Average confidence: {avg_confidence:.3f}\")\n",
    "    print(f\"æœ€é«˜ç½®ä¿¡åº¦ Maximum confidence: {max_confidence:.3f}\")\n",
    "    print(f\"æœ€ä½ç½®ä¿¡åº¦ Minimum confidence: {min_confidence:.3f}\")\n",
    "    \n",
    "    # ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒå›¾ Plot confidence distribution\n",
    "    if confidence_scores:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(confidence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        plt.title('ğŸ¯ æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ Detection Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('ç½®ä¿¡åº¦ Confidence Score')\n",
    "        plt.ylabel('é¢‘æ¬¡ Frequency')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'total_images': total_images,\n",
    "        'total_detections': total_detections,\n",
    "        'avg_detections_per_image': avg_detections_per_image,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'avg_confidence': avg_confidence\n",
    "    }\n",
    "\n",
    "# å¦‚æœæœ‰æµ‹è¯•å›¾åƒï¼Œè¿›è¡Œæ€§èƒ½è¯„ä¼° If test images exist, perform performance evaluation\n",
    "if model is not None:\n",
    "    test_image_dir = Path('data/processed/images/test')\n",
    "    if test_image_dir.exists():\n",
    "        test_images = list(test_image_dir.glob('*.jpg'))\n",
    "        if test_images:\n",
    "            performance_stats = evaluate_model_performance(model, test_images)\n",
    "        else:\n",
    "            # ä½¿ç”¨åŸå§‹è¯„ä¼°å›¾åƒ Use original evaluation images\n",
    "            eval_images = list(Path('data/raw/evaluation').rglob('*.jpg'))\n",
    "            if eval_images:\n",
    "                performance_stats = evaluate_model_performance(model, eval_images[:20])  # é™åˆ¶ä¸º20å¼ å›¾åƒ\n",
    "    else:\n",
    "        print(\"âš ï¸  è·³è¿‡æ€§èƒ½è¯„ä¼°ï¼Œæœªæ‰¾åˆ°æµ‹è¯•å›¾åƒ Skipping performance evaluation, no test images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ é¡¹ç›®æ€»ç»“ Project Summary\n",
    "\n",
    "### å®Œæˆçš„åŠŸèƒ½ Completed Features\n",
    "1. âœ… **ç¯å¢ƒæ£€æµ‹** - è‡ªåŠ¨æ£€æµ‹GPUå’ŒColabç¯å¢ƒ\n",
    "2. âœ… **æ•°æ®ä¸‹è½½** - è‡ªåŠ¨ä¸‹è½½NeonTreeEvaluationæ•°æ®é›†\n",
    "3. âœ… **æ•°æ®è½¬æ¢** - å°†NeonTreeæ ¼å¼è½¬æ¢ä¸ºYOLOæ ¼å¼\n",
    "4. âœ… **æ¨¡å‹è®­ç»ƒ** - ä½¿ç”¨YOLOv5è¿›è¡Œæ ‘æœ¨æ£€æµ‹è®­ç»ƒ\n",
    "5. âœ… **ç»“æœæ¨ç†** - å¯¹æµ‹è¯•å›¾åƒè¿›è¡Œæ ‘æœ¨æ£€æµ‹\n",
    "6. âœ… **ç»“æœå¯è§†åŒ–** - æ˜¾ç¤ºæ£€æµ‹ç»“æœå’Œæ€§èƒ½ç»Ÿè®¡\n",
    "\n",
    "### ä¸»è¦å‚æ•°è¯´æ˜ Key Parameter Explanations\n",
    "- **è®­ç»ƒè½®æ•° Epochs**: 50è½®ï¼ˆå¯æ ¹æ®æ•ˆæœè°ƒæ•´ï¼‰\n",
    "- **æ‰¹æ¬¡å¤§å° Batch Size**: 16ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼‰\n",
    "- **å›¾åƒå°ºå¯¸ Image Size**: 640x640åƒç´ \n",
    "- **å­¦ä¹ ç‡ Learning Rate**: 0.001ï¼ˆAdamWä¼˜åŒ–å™¨ï¼‰\n",
    "- **æ•°æ®åˆ†å‰² Data Split**: 70%è®­ç»ƒ/20%éªŒè¯/10%æµ‹è¯•\n",
    "\n",
    "### ä½¿ç”¨è¯´æ˜ Usage Instructions\n",
    "1. åœ¨Google Colabä¸­è¿è¡Œæ‰€æœ‰ä»£ç å•å…ƒ\n",
    "2. ç¡®ä¿GPUç¯å¢ƒå¯ç”¨ä»¥åŠ é€Ÿè®­ç»ƒ\n",
    "3. æ ¹æ®éœ€è¦è°ƒæ•´è®­ç»ƒå‚æ•°\n",
    "4. æŸ¥çœ‹resultsç›®å½•ä¸­çš„æ£€æµ‹ç»“æœå›¾åƒ\n",
    "\n",
    "### æ•…éšœæ’é™¤ Troubleshooting\n",
    "- å¦‚æœå†…å­˜ä¸è¶³ï¼Œå‡å°batch_sizeå‚æ•°\n",
    "- å¦‚æœè®­ç»ƒæ—¶é—´è¿‡é•¿ï¼Œå‡å°‘epochsæ•°é‡\n",
    "- å¦‚æœæ£€æµ‹æ•ˆæœä¸ä½³ï¼Œå°è¯•å¢åŠ è®­ç»ƒè½®æ•°\n",
    "- ç¡®ä¿æ•°æ®é›†æ­£ç¡®ä¸‹è½½å’Œè§£å‹\n",
    "\n",
    "### è¿›ä¸€æ­¥æ”¹è¿› Further Improvements\n",
    "- æ•°æ®å¢å¼ºæŠ€æœ¯æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›\n",
    "- è¶…å‚æ•°è°ƒä¼˜ä¼˜åŒ–æ¨¡å‹æ€§èƒ½\n",
    "- å¤šå°ºåº¦è®­ç»ƒæé«˜æ£€æµ‹ç²¾åº¦\n",
    "- æ¨¡å‹é›†æˆæå‡æ•´ä½“æ•ˆæœ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
