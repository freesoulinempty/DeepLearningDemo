{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 树木检测项目 / Tree Detection Project\n",
    "\n",
    "## 项目概述 / Project Overview\n",
    "\n",
    "本项目使用YOLOv5和PyTorch实现基于航空影像的树木检测任务  \n",
    "This project implements tree detection from aerial imagery using YOLOv5 and PyTorch\n",
    "\n",
    "**数据集 / Dataset**: NeonTreeEvaluation Benchmark  \n",
    "**目标 / Goal**: 检测航空正射影像中的树木 / Detect trees in aerial orthoimagery  \n",
    "**平台 / Platform**: Google Colab (自动GPU检测 / Auto GPU detection)\n",
    "\n",
    "## 技术栈 / Tech Stack\n",
    "- **深度学习框架 / Deep Learning**: PyTorch + YOLOv5\n",
    "- **数据处理 / Data Processing**: OpenCV, PIL, pandas\n",
    "- **可视化 / Visualization**: matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 环境检测和基础设置 / Environment Detection and Basic Setup\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"环境检测 / Environment Detection\")\n",
    "\n",
    "# 检查Python版本 / Check Python version\n",
    "print(f\"Python版本 / Python Version: {sys.version}\")\n",
    "\n",
    "# 检查操作系统 / Check OS\n",
    "print(f\"操作系统 / Operating System: {platform.system()}\")\n",
    "\n",
    "# 检查是否在Colab环境 / Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"运行环境 / Environment: Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"运行环境 / Environment: 本地环境 / Local\")\n",
    "\n",
    "# 检查GPU可用性 / Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU可用 / GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA版本 / CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU数量 / GPU Count: {torch.cuda.device_count()}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"GPU不可用，将使用CPU / GPU not available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorch版本 / PyTorch Version: {torch.__version__}\")\n",
    "print(f\"设备 / Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 安装必要依赖 / Install Required Dependencies\n",
    "print(\"安装依赖包 / Installing Dependencies...\")\n",
    "\n",
    "# 安装YOLOv5和相关依赖 / Install YOLOv5 and dependencies\n",
    "!pip install -q ultralytics\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q Pillow\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q pandas\n",
    "!pip install -q tqdm\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q PyYAML\n",
    "\n",
    "# 如果在Colab环境，挂载Google Drive (可选) / Mount Google Drive in Colab (optional)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/drive')  # 取消注释以挂载Drive / Uncomment to mount Drive\n",
    "\n",
    "print(\"依赖安装完成 / Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 导入所需库 / Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置matplotlib中文字体支持 / Set matplotlib Chinese font support\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子 / Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"库导入完成 / Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 数据下载和解压 / Data Download and Extraction\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    下载文件的函数 / Function to download files\n",
    "    \"\"\"\n",
    "    print(f\"开始下载 / Starting download: {filename}\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filename, 'wb') as file, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    print(f\"下载完成 / Download completed: {filename}\")\n",
    "\n",
    "# 创建数据目录 / Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# 数据集URL / Dataset URLs\n",
    "DATASET_URL = \"https://zenodo.org/records/5914554/files/evaluation.zip?download=1\"\n",
    "ANNOTATIONS_URL = \"https://zenodo.org/records/5914554/files/annotations.zip?download=1\"\n",
    "\n",
    "# 下载数据集 / Download datasets\n",
    "if not os.path.exists('data/raw/evaluation.zip'):\n",
    "    download_file(DATASET_URL, 'data/raw/evaluation.zip')\n",
    "else:\n",
    "    print(\"评估数据集已存在 / Evaluation dataset already exists\")\n",
    "\n",
    "if not os.path.exists('data/raw/annotations.zip'):\n",
    "    download_file(ANNOTATIONS_URL, 'data/raw/annotations.zip')\n",
    "else:\n",
    "    print(\"标注数据已存在 / Annotations already exist\")\n",
    "\n",
    "print(\"数据下载完成 / Data download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 解压数据集 / Extract Datasets\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    解压ZIP文件的函数 / Function to extract ZIP files\n",
    "    \"\"\"\n",
    "    print(f\"解压文件 / Extracting: {zip_path}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    print(f\"解压完成 / Extraction completed: {extract_to}\")\n",
    "\n",
    "# 解压评估数据集 / Extract evaluation dataset\n",
    "if not os.path.exists('data/raw/evaluation'):\n",
    "    extract_zip('data/raw/evaluation.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"评估数据集已解压 / Evaluation dataset already extracted\")\n",
    "\n",
    "# 解压标注数据 / Extract annotations\n",
    "if not os.path.exists('data/raw/annotations'):\n",
    "    extract_zip('data/raw/annotations.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"标注数据已解压 / Annotations already extracted\")\n",
    "\n",
    "# 查看数据结构 / Explore data structure\n",
    "print(\"\\n数据结构分析 / Data Structure Analysis\")\n",
    "\n",
    "# 检查评估数据集结构 / Check evaluation dataset structure\n",
    "eval_path = Path('data/raw/evaluation')\n",
    "if eval_path.exists():\n",
    "    print(f\"评估数据集路径 / Evaluation dataset path: {eval_path}\")\n",
    "    subdirs = [d for d in eval_path.iterdir() if d.is_dir()]\n",
    "    print(f\"子目录数量 / Number of subdirectories: {len(subdirs)}\")\n",
    "    for subdir in subdirs[:5]:  # 显示前5个子目录 / Show first 5 subdirectories\n",
    "        print(f\"  - {subdir.name}\")\n",
    "    if len(subdirs) > 5:\n",
    "        print(f\"  ... 还有 {len(subdirs)-5} 个目录 / and {len(subdirs)-5} more directories\")\n",
    "\n",
    "# 检查标注数据结构 / Check annotations structure\n",
    "ann_path = Path('data/raw/annotations')\n",
    "if ann_path.exists():\n",
    "    print(f\"\\n标注数据路径 / Annotations path: {ann_path}\")\n",
    "    ann_files = list(ann_path.glob('*.csv'))\n",
    "    print(f\"CSV标注文件数量 / Number of CSV annotation files: {len(ann_files)}\")\n",
    "    for ann_file in ann_files[:3]:  # 显示前3个标注文件 / Show first 3 annotation files\n",
    "        print(f\"  - {ann_file.name}\")\n",
    "\n",
    "print(\"\\n数据解压和结构分析完成 / Data extraction and structure analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 数据格式转换 / Data Format Conversion (NeonTree -> YOLO) - 使用真实图像\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class NeonTreeToYOLO:\n",
    "    \"\"\"\n",
    "    将NeonTree数据集转换为YOLO格式的类\n",
    "    Class to convert NeonTree dataset to YOLO format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, annotations_root, evaluation_root, output_root):\n",
    "        self.annotations_root = Path(annotations_root)\n",
    "        self.evaluation_root = Path(evaluation_root)\n",
    "        self.output_root = Path(output_root)\n",
    "        \n",
    "        # 创建输出目录 / Create output directories\n",
    "        self.create_yolo_structure()\n",
    "        \n",
    "        # 创建图像文件名映射 / Create image filename mapping\n",
    "        self.create_image_mapping()\n",
    "    \n",
    "    def create_yolo_structure(self):\n",
    "        \"\"\"创建YOLO数据集目录结构 / Create YOLO dataset directory structure\"\"\"\n",
    "        folders = [\n",
    "            'images/train', 'images/val', 'images/test',\n",
    "            'labels/train', 'labels/val', 'labels/test'\n",
    "        ]\n",
    "        \n",
    "        for folder in folders:\n",
    "            (self.output_root / folder).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"YOLO目录结构创建完成 / YOLO directory structure created\")\n",
    "    \n",
    "    def create_image_mapping(self):\n",
    "        \"\"\"\n",
    "        创建图像文件名到路径的映射\n",
    "        Create mapping from image filenames to paths\n",
    "        \"\"\"\n",
    "        self.image_mapping = {}\n",
    "        \n",
    "        # 检查 evaluation/RGB 目录\n",
    "        rgb_dir = self.evaluation_root / 'RGB'\n",
    "        if rgb_dir.exists():\n",
    "            for img_file in rgb_dir.glob('*.tif'):\n",
    "                # 去掉扩展名作为key\n",
    "                base_name = img_file.stem\n",
    "                self.image_mapping[base_name] = img_file\n",
    "                \n",
    "                # 同时添加不带扩展名的映射\n",
    "                if base_name.endswith('.tif'):\n",
    "                    base_name_no_ext = base_name[:-4]\n",
    "                    self.image_mapping[base_name_no_ext] = img_file\n",
    "        \n",
    "        print(f\"创建图像映射 / Created image mapping with {len(self.image_mapping)} entries\")\n",
    "    \n",
    "    def find_matching_image(self, xml_filename):\n",
    "        \"\"\"\n",
    "        根据XML文件名查找对应的RGB图像文件\n",
    "        Find matching RGB image file based on XML filename\n",
    "        \"\"\"\n",
    "        # 从XML文件名中提取基础名称\n",
    "        base_name = xml_filename.replace('.xml', '')\n",
    "        \n",
    "        # 直接匹配\n",
    "        if base_name in self.image_mapping:\n",
    "            return self.image_mapping[base_name]\n",
    "        \n",
    "        # 尝试模糊匹配 - 查找包含base_name的图像文件\n",
    "        for img_name, img_path in self.image_mapping.items():\n",
    "            # 检查是否匹配（去掉年份等变化部分）\n",
    "            if self.files_match(base_name, img_name):\n",
    "                return img_path\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def files_match(self, xml_name, img_name):\n",
    "        \"\"\"\n",
    "        判断XML文件名和图像文件名是否匹配\n",
    "        Check if XML filename matches image filename\n",
    "        \"\"\"\n",
    "        # 移除常见的后缀和前缀\n",
    "        xml_clean = xml_name.lower()\n",
    "        img_clean = img_name.lower()\n",
    "        \n",
    "        # 提取核心站点代码\n",
    "        xml_parts = xml_clean.split('_')\n",
    "        img_parts = img_clean.split('_')\n",
    "        \n",
    "        # 如果站点代码匹配\n",
    "        if len(xml_parts) >= 2 and len(img_parts) >= 2:\n",
    "            xml_site = xml_parts[0]\n",
    "            img_site = img_parts[0]\n",
    "            \n",
    "            if xml_site == img_site:\n",
    "                # 进一步检查编号匹配\n",
    "                if len(xml_parts) >= 3 and len(img_parts) >= 2:\n",
    "                    try:\n",
    "                        xml_num = xml_parts[1]\n",
    "                        img_num = img_parts[1]\n",
    "                        return xml_num == img_num\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # 检查是否包含相同的关键部分\n",
    "        return any(part in img_clean for part in xml_parts if len(part) > 2)\n",
    "    \n",
    "    def parse_xml_annotation(self, xml_file):\n",
    "        \"\"\"\n",
    "        解析XML标注文件\n",
    "        Parse XML annotation file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # 获取图像信息 / Get image information\n",
    "            filename_elem = root.find('filename')\n",
    "            if filename_elem is not None:\n",
    "                filename = filename_elem.text\n",
    "            else:\n",
    "                filename = xml_file.stem + '.tif'  # 默认扩展名\n",
    "            \n",
    "            size = root.find('size')\n",
    "            width = int(size.find('width').text)\n",
    "            height = int(size.find('height').text)\n",
    "            \n",
    "            # 获取所有树木对象 / Get all tree objects\n",
    "            objects = []\n",
    "            for obj in root.findall('object'):\n",
    "                name_elem = obj.find('name')\n",
    "                if name_elem is not None and name_elem.text.lower() == 'tree':\n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    xmin = int(float(bndbox.find('xmin').text))\n",
    "                    ymin = int(float(bndbox.find('ymin').text))\n",
    "                    xmax = int(float(bndbox.find('xmax').text))\n",
    "                    ymax = int(float(bndbox.find('ymax').text))\n",
    "                    \n",
    "                    # 验证边界框有效性 / Validate bounding box\n",
    "                    if xmax > xmin and ymax > ymin:\n",
    "                        objects.append({\n",
    "                            'xmin': xmin,\n",
    "                            'ymin': ymin,\n",
    "                            'xmax': xmax,\n",
    "                            'ymax': ymax\n",
    "                        })\n",
    "            \n",
    "            return {\n",
    "                'filename': filename,\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'objects': objects\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"解析XML文件时出错 / Error parsing XML file {xml_file}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def convert_bbox_to_yolo(self, bbox, img_width, img_height):\n",
    "        \"\"\"\n",
    "        将边界框坐标转换为YOLO格式\n",
    "        Convert bounding box coordinates to YOLO format\n",
    "        \n",
    "        YOLO格式: [class_id, x_center, y_center, width, height] (归一化 / normalized)\n",
    "        \"\"\"\n",
    "        xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']\n",
    "        \n",
    "        # 计算中心点和宽高 / Calculate center point and dimensions\n",
    "        x_center = (xmin + xmax) / 2.0\n",
    "        y_center = (ymin + ymax) / 2.0\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        # 归一化 / Normalize\n",
    "        x_center /= img_width\n",
    "        y_center /= img_height\n",
    "        width /= img_width\n",
    "        height /= img_height\n",
    "        \n",
    "        return [0, x_center, y_center, width, height]  # 类别ID为0 (树木 / tree)\n",
    "    \n",
    "    def process_annotations(self):\n",
    "        \"\"\"\n",
    "        处理所有XML标注文件并转换为YOLO格式\n",
    "        Process all XML annotation files and convert to YOLO format\n",
    "        \"\"\"\n",
    "        xml_files = list(self.annotations_root.glob('*.xml'))\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        print(f\"找到 {len(xml_files)} 个XML标注文件 / Found {len(xml_files)} XML annotation files\")\n",
    "        \n",
    "        for i, xml_file in enumerate(tqdm(xml_files, desc=\"处理标注 / Processing annotations\")):\n",
    "            try:\n",
    "                # 解析XML文件 / Parse XML file\n",
    "                annotation_data = self.parse_xml_annotation(xml_file)\n",
    "                \n",
    "                if annotation_data is None or not annotation_data['objects']:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # 查找对应的RGB图像文件 / Find corresponding RGB image file\n",
    "                img_path = self.find_matching_image(xml_file.name)\n",
    "                \n",
    "                if img_path is None:\n",
    "                    # 打印调试信息，但不中断处理 / Print debug info but don't interrupt\n",
    "                    if processed_count < 5:  # 只打印前5个错误\n",
    "                        print(f\"未找到对应图像 / No matching image for: {xml_file.name}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # 读取真实图像 / Load real image\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    print(f\"无法读取图像 / Cannot read image: {img_path}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # 验证图像尺寸 / Verify image dimensions\n",
    "                actual_height, actual_width = img.shape[:2]\n",
    "                if actual_width != annotation_data['width'] or actual_height != annotation_data['height']:\n",
    "                    # 使用实际图像尺寸 / Use actual image dimensions\n",
    "                    annotation_data['width'] = actual_width\n",
    "                    annotation_data['height'] = actual_height\n",
    "                \n",
    "                # 确定数据集分割 / Determine dataset split\n",
    "                if i % 10 < 7:  # 70% 训练集 / training set\n",
    "                    split = 'train'\n",
    "                elif i % 10 < 9:  # 20% 验证集 / validation set\n",
    "                    split = 'val'\n",
    "                else:  # 10% 测试集 / test set\n",
    "                    split = 'test'\n",
    "                \n",
    "                # 保存图像 / Save image\n",
    "                img_filename = f\"tree_{processed_count:06d}.jpg\"\n",
    "                img_save_path = self.output_root / 'images' / split / img_filename\n",
    "                cv2.imwrite(str(img_save_path), img)\n",
    "                \n",
    "                # 保存YOLO格式标签 / Save YOLO format labels\n",
    "                label_filename = f\"tree_{processed_count:06d}.txt\"\n",
    "                label_save_path = self.output_root / 'labels' / split / label_filename\n",
    "                \n",
    "                with open(label_save_path, 'w') as f:\n",
    "                    for obj in annotation_data['objects']:\n",
    "                        yolo_bbox = self.convert_bbox_to_yolo(\n",
    "                            obj, annotation_data['width'], annotation_data['height']\n",
    "                        )\n",
    "                        # YOLO格式: class_id x_center y_center width height (修复换行符)\n",
    "                        f.write(f\"{yolo_bbox[0]} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f} {yolo_bbox[4]:.6f}\\n\")\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"处理文件 {xml_file.name} 时出错 / Error processing file {xml_file.name}: {e}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "        \n",
    "        print(f\"处理完成 / Processing completed: {processed_count} 个样本 / samples processed, {skipped_count} 个跳过 / skipped\")\n",
    "        return processed_count\n",
    "\n",
    "# 执行数据转换 / Execute data conversion\n",
    "print(\"开始数据格式转换 / Starting data format conversion...\")\n",
    "print(\"使用XML格式的标注数据和真实RGB图像 / Using XML annotations with real RGB images\")\n",
    "\n",
    "# 创建data目录 / Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# 检查annotations和evaluation数据是否存在 / Check if annotations and evaluation data exist\n",
    "annotation_paths = ['data/raw/annotations', 'annotations', '/content/annotations']\n",
    "eval_paths = ['data/raw/evaluation', 'evaluation', '/content/evaluation']\n",
    "\n",
    "annotations_root = None\n",
    "eval_root = None\n",
    "\n",
    "# 查找annotations目录 / Find annotations directory\n",
    "for path in annotation_paths:\n",
    "    if Path(path).exists():\n",
    "        annotations_root = Path(path)\n",
    "        xml_count = len(list(annotations_root.glob('*.xml')))\n",
    "        if xml_count > 0:\n",
    "            print(f\"找到annotations数据集 / Found annotations dataset at: {annotations_root} ({xml_count} XML files)\")\n",
    "            break\n",
    "\n",
    "if annotations_root is None:\n",
    "    print(\"未找到annotations数据集或XML文件 / annotations dataset or XML files not found\")\n",
    "    print(\"请确保annotations目录存在且包含XML文件 / Please ensure annotations directory exists with XML files\")\n",
    "\n",
    "# 查找evaluation目录 / Find evaluation directory  \n",
    "for path in eval_paths:\n",
    "    if Path(path).exists():\n",
    "        eval_root = Path(path)\n",
    "        # 检查RGB子目录\n",
    "        rgb_dir = eval_root / 'RGB'\n",
    "        if rgb_dir.exists():\n",
    "            img_count = len(list(rgb_dir.glob('*.tif')))\n",
    "            print(f\"找到evaluation数据集 / Found evaluation dataset at: {eval_root} ({img_count} images)\")\n",
    "            break\n",
    "\n",
    "if eval_root is None:\n",
    "    print(\"未找到evaluation数据集 / evaluation dataset not found, please ensure evaluation.zip is downloaded and extracted\")\n",
    "\n",
    "# 只有当两个数据集都找到时才进行转换 / Only proceed if both datasets are found\n",
    "if annotations_root is not None and eval_root is not None:\n",
    "    # 初始化转换器 / Initialize converter\n",
    "    converter = NeonTreeToYOLO(annotations_root, eval_root, 'data/processed')\n",
    "    \n",
    "    # 处理标注数据 / Process annotation data\n",
    "    processed_samples = converter.process_annotations()\n",
    "    \n",
    "    if processed_samples > 0:\n",
    "        print(f\"数据转换完成 / Data conversion completed: {processed_samples} 个样本 / samples\")\n",
    "        \n",
    "        # 显示数据集统计 / Show dataset statistics\n",
    "        train_images = len(list(Path('data/processed/images/train').glob('*.jpg')))\n",
    "        val_images = len(list(Path('data/processed/images/val').glob('*.jpg')))\n",
    "        test_images = len(list(Path('data/processed/images/test').glob('*.jpg')))\n",
    "        \n",
    "        print(f\"数据集统计 / Dataset Statistics:\")\n",
    "        print(f\"   训练集 / Training: {train_images} 张图像 / images\")\n",
    "        print(f\"   验证集 / Validation: {val_images} 张图像 / images\")\n",
    "        print(f\"   测试集 / Testing: {test_images} 张图像 / images\")\n",
    "        print(f\"   总计 / Total: {train_images + val_images + test_images} 张图像 / images\")\n",
    "    else:\n",
    "        print(\"未能处理任何样本 / No samples were processed\")\n",
    "        print(\"请检查XML文件和图像文件的匹配关系 / Please check XML and image file matching\")\n",
    "else:\n",
    "    print(\"缺少必要的数据集，无法进行转换 / Missing required datasets, cannot proceed with conversion\")\n",
    "    print(\"请确保以下目录存在 / Please ensure the following directories exist:\")\n",
    "    print(\"  - annotations 目录（包含XML文件） / annotations directory (with XML files)\")\n",
    "    print(\"  - evaluation/RGB 目录（包含.tif图像文件） / evaluation/RGB directory (with .tif image files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 创建YOLO配置文件 / Create YOLO Configuration Files (修复路径问题)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "def create_dataset_yaml():\n",
    "    \"\"\"\n",
    "    创建YOLO数据集配置文件\n",
    "    Create YOLO dataset configuration file\n",
    "    \"\"\"\n",
    "    # 获取当前工作目录 / Get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # 构建绝对路径 / Build absolute paths\n",
    "    processed_dir = os.path.join(current_dir, 'data/processed')\n",
    "    train_path = os.path.join(processed_dir, 'images/train')\n",
    "    val_path = os.path.join(processed_dir, 'images/val')\n",
    "    test_path = os.path.join(processed_dir, 'images/test')\n",
    "    \n",
    "    # 检查路径是否存在 / Check if paths exist\n",
    "    paths_info = {\n",
    "        'train': train_path,\n",
    "        'val': val_path,\n",
    "        'test': test_path\n",
    "    }\n",
    "    \n",
    "    print(\"检查数据集路径 / Checking dataset paths:\")\n",
    "    existing_paths = {}\n",
    "    \n",
    "    for split, path in paths_info.items():\n",
    "        if os.path.exists(path):\n",
    "            img_count = len([f for f in os.listdir(path) if f.endswith('.jpg')])\n",
    "            print(f\"{split}: {path} ({img_count} 张图像 / images)\")\n",
    "            existing_paths[split] = path\n",
    "        else:\n",
    "            print(f\"{split}: {path} (路径不存在 / path does not exist)\")\n",
    "    \n",
    "    # 确保至少有训练集存在 / Ensure at least training set exists\n",
    "    if 'train' not in existing_paths:\n",
    "        print(\"错误：未找到训练集 / Error: Training set not found\")\n",
    "        return None\n",
    "    \n",
    "    # 创建数据集配置 / Create dataset configuration\n",
    "    dataset_config = {\n",
    "        'path': processed_dir,  # 数据集根目录 / dataset root dir\n",
    "        'train': 'images/train',  # 相对于path的训练图像路径 / train images relative to path\n",
    "        'val': 'images/val' if 'val' in existing_paths else 'images/train',  # 验证集，如果不存在则使用训练集\n",
    "        'test': 'images/test' if 'test' in existing_paths else 'images/train',  # 测试集，如果不存在则使用训练集\n",
    "        'nc': 1,                  # 类别数量 / number of classes\n",
    "        'names': ['tree']         # 类别名称 / class names\n",
    "    }\n",
    "    \n",
    "    # 特别处理验证集路径\n",
    "    if 'val' not in existing_paths:\n",
    "        print(\"验证集不存在，使用训练集作为验证集 / Validation set not found, using training set as validation\")\n",
    "        dataset_config['val'] = 'images/train'\n",
    "    \n",
    "    if 'test' not in existing_paths:\n",
    "        print(\"测试集不存在，使用训练集作为测试集 / Test set not found, using training set as test\")\n",
    "        dataset_config['test'] = 'images/train'\n",
    "    \n",
    "    # 确保配置目录存在\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    # 保存配置文件 / Save configuration file\n",
    "    config_path = 'data/tree_dataset.yaml'\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"YOLO数据集配置文件创建完成 / YOLO dataset configuration file created: {config_path}\")\n",
    "    \n",
    "    # 验证配置文件内容\n",
    "    print(\"\\n验证配置文件内容 / Verifying configuration file:\")\n",
    "    for key, value in dataset_config.items():\n",
    "        if key in ['train', 'val', 'test']:\n",
    "            full_path = os.path.join(processed_dir, value)\n",
    "            exists = os.path.exists(full_path)\n",
    "            print(f\"   {key}: {value} -> {full_path} ({'存在' if exists else '不存在'} / {'exists' if exists else 'missing'})\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    return config_path\n",
    "\n",
    "def fix_data_structure():\n",
    "    \"\"\"\n",
    "    修复数据结构问题\n",
    "    Fix data structure issues\n",
    "    \"\"\"\n",
    "    print(\"\\n检查并修复数据结构 / Checking and fixing data structure...\")\n",
    "    \n",
    "    processed_dir = Path('data/processed')\n",
    "    \n",
    "    if not processed_dir.exists():\n",
    "        print(\"data/processed 目录不存在 / data/processed directory does not exist\")\n",
    "        return False\n",
    "    \n",
    "    # 检查必要的目录结构\n",
    "    required_dirs = [\n",
    "        'images/train', 'images/val', 'images/test',\n",
    "        'labels/train', 'labels/val', 'labels/test'\n",
    "    ]\n",
    "    \n",
    "    missing_dirs = []\n",
    "    for dir_path in required_dirs:\n",
    "        full_path = processed_dir / dir_path\n",
    "        if not full_path.exists():\n",
    "            missing_dirs.append(dir_path)\n",
    "    \n",
    "    if missing_dirs:\n",
    "        print(f\"缺少目录 / Missing directories: {missing_dirs}\")\n",
    "        \n",
    "        # 如果只是验证集和测试集缺失，创建它们\n",
    "        for missing_dir in missing_dirs:\n",
    "            if 'val' in missing_dir or 'test' in missing_dir:\n",
    "                (processed_dir / missing_dir).mkdir(parents=True, exist_ok=True)\n",
    "                print(f\"创建目录 / Created directory: {missing_dir}\")\n",
    "    \n",
    "    # 检查训练集是否有数据\n",
    "    train_images = list((processed_dir / 'images/train').glob('*.jpg'))\n",
    "    train_labels = list((processed_dir / 'labels/train').glob('*.txt'))\n",
    "    \n",
    "    print(f\"训练集统计 / Training set statistics:\")\n",
    "    print(f\"   图像文件 / Image files: {len(train_images)}\")\n",
    "    print(f\"   标签文件 / Label files: {len(train_labels)}\")\n",
    "    \n",
    "    if len(train_images) == 0:\n",
    "        print(\"训练集为空，请先运行数据转换 / Training set is empty, please run data conversion first\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# 修复数据结构\n",
    "data_structure_ok = fix_data_structure()\n",
    "\n",
    "if data_structure_ok:\n",
    "    # 创建配置文件 / Create configuration file\n",
    "    dataset_yaml_path = create_dataset_yaml()\n",
    "    \n",
    "    if dataset_yaml_path:\n",
    "        # 显示配置文件内容 / Display configuration file content\n",
    "        with open(dataset_yaml_path, 'r') as f:\n",
    "            config_content = f.read()\n",
    "            print(\"\\n数据集配置内容 / Dataset Configuration Content:\")\n",
    "            print(\"=\" * 40)\n",
    "            print(config_content)\n",
    "            print(\"=\" * 40)\n",
    "        \n",
    "        # 最终验证\n",
    "        print(\"\\n配置文件创建并验证完成 / Configuration file created and verified\")\n",
    "    else:\n",
    "        print(\"配置文件创建失败 / Failed to create configuration file\")\n",
    "else:\n",
    "    print(\"数据结构修复失败，请检查数据转换步骤 / Data structure fix failed, please check data conversion step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. YOLOv5专业树木检测训练 / Professional YOLOv5 Tree Detection Training (修复参数)\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"YOLOv5专业树木检测训练 / YOLOv5 Professional Tree Detection Training\")\n",
    "print(\"基于2024年最新YOLOv5最佳实践 / Based on 2024 latest YOLOv5 best practices\")\n",
    "print(\"完全放弃best.pt，使用官方预训练模型从零开始 / Abandon best.pt, use official pretrained models from scratch\")\n",
    "print(\"修复训练命令参数格式 / Fix training command parameter format\")\n",
    "\n",
    "def memory_cleanup():\n",
    "    \"\"\"内存清理 / Memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def create_professional_dataset_config():\n",
    "    \"\"\"创建专业的数据集配置 / Create professional dataset configuration\"\"\"\n",
    "    print(\"创建专业YOLOv5数据集配置 / Creating professional YOLOv5 dataset configuration...\")\n",
    "    \n",
    "    # 检查数据集路径\n",
    "    dataset_path = os.path.join(os.getcwd(), 'data/processed')\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"数据集路径不存在 / Dataset path does not exist: {dataset_path}\")\n",
    "        return None\n",
    "    \n",
    "    # 专业配置 - 遵循YOLOv5官方标准 / Professional config - Follow YOLOv5 official standards\n",
    "    professional_config = {\n",
    "        'path': dataset_path,\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/train',  # 使用训练集作为验证集（小数据集最佳实践） / Use training set as validation set (best practice for small datasets)\n",
    "        'test': 'images/test',\n",
    "        'nc': 1,\n",
    "        'names': ['tree']\n",
    "    }\n",
    "    \n",
    "    # 检查训练数据\n",
    "    train_path = os.path.join(dataset_path, 'images/train')\n",
    "    \n",
    "    if os.path.exists(train_path):\n",
    "        train_count = len([f for f in os.listdir(train_path) if f.endswith('.jpg')])\n",
    "        print(f\"   训练集图像 / Training images: {train_count}\")\n",
    "        \n",
    "        if train_count == 0:\n",
    "            print(\"训练数据为空 / Training data is empty\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"训练数据目录不存在 / Training data directory does not exist\")\n",
    "        return None\n",
    "    \n",
    "    # 保存专业配置\n",
    "    config_path = 'tree_detection_professional.yaml'\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(professional_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"专业配置文件 / Professional config file: {config_path}\")\n",
    "    return config_path\n",
    "\n",
    "# 执行内存清理\n",
    "memory_cleanup()\n",
    "\n",
    "# 确保YOLOv5环境\n",
    "if not os.path.exists('yolov5'):\n",
    "    print(\"克隆官方YOLOv5仓库 / Cloning official YOLOv5 repository...\")\n",
    "    !git clone https://github.com/ultralytics/yolov5.git\n",
    "    print(\"YOLOv5仓库克隆完成 / YOLOv5 repository cloning completed\")\n",
    "\n",
    "# 安装/更新依赖\n",
    "print(\"检查YOLOv5依赖 / Checking YOLOv5 dependencies...\")\n",
    "try:\n",
    "    os.chdir('yolov5')\n",
    "    !pip install -r requirements.txt --quiet\n",
    "    os.chdir('..')\n",
    "    print(\"YOLOv5依赖检查完成 / YOLOv5 dependencies check completed\")\n",
    "except Exception as e:\n",
    "    print(f\"依赖安装警告 / Dependency installation warning: {e}\")\n",
    "\n",
    "# 创建专业数据集配置\n",
    "dataset_yaml = create_professional_dataset_config()\n",
    "\n",
    "if dataset_yaml is None:\n",
    "    print(\"无法创建数据集配置，训练终止 / Unable to create dataset configuration, training terminated\")\n",
    "else:\n",
    "    print(\"\\n开始YOLOv5专业训练 / Starting YOLOv5 professional training...\")\n",
    "    \n",
    "    # 根据硬件配置设置训练参数 / Set training parameters based on hardware configuration\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        device_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"GPU: {device_name} ({gpu_memory:.1f} GB)\")\n",
    "        \n",
    "        # 根据GPU内存优化参数 / Optimize parameters based on GPU memory\n",
    "        if gpu_memory >= 20:  # A100, RTX 4090等\n",
    "            model_size = 'yolov5l'\n",
    "            batch_size = 24\n",
    "            img_size = 640\n",
    "            epochs = 200\n",
    "            workers = 6\n",
    "        elif gpu_memory >= 15:  # L4, RTX 3090等\n",
    "            model_size = 'yolov5m'\n",
    "            batch_size = 20\n",
    "            img_size = 640\n",
    "            epochs = 150\n",
    "            workers = 4\n",
    "        elif gpu_memory >= 8:   # RTX 3070, V100等\n",
    "            model_size = 'yolov5s'\n",
    "            batch_size = 16\n",
    "            img_size = 640\n",
    "            epochs = 120\n",
    "            workers = 2\n",
    "        else:                   # GTX 1080, RTX 2070等\n",
    "            model_size = 'yolov5s'\n",
    "            batch_size = 8\n",
    "            img_size = 512\n",
    "            epochs = 100\n",
    "            workers = 2\n",
    "            \n",
    "        device_param = '0'\n",
    "    else:\n",
    "        print(\"使用CPU训练 (不推荐) / Using CPU training (not recommended)\")\n",
    "        model_size = 'yolov5s'\n",
    "        batch_size = 4\n",
    "        img_size = 512\n",
    "        epochs = 50\n",
    "        workers = 1\n",
    "        device_param = 'cpu'\n",
    "    \n",
    "    print(f\"\\n专业训练配置 / Professional training configuration:\")\n",
    "    print(f\"   模型 / Model: {model_size}.pt (官方预训练 / official pretrained)\")\n",
    "    print(f\"   图像尺寸 / Image size: {img_size}\")\n",
    "    print(f\"   批次大小 / Batch size: {batch_size}\")\n",
    "    print(f\"   训练轮数 / Epochs: {epochs}\")\n",
    "    print(f\"   数据加载线程 / Data loading workers: {workers}\")\n",
    "    print(f\"   设备 / Device: {device_param}\")\n",
    "    \n",
    "    # 构建修复的专业训练命令 / Build fixed professional training command\n",
    "    professional_command = [\n",
    "        sys.executable, 'yolov5/train.py',\n",
    "        '--data', dataset_yaml,\n",
    "        '--weights', f'{model_size}.pt',        # 官方预训练权重 / official pretrained weights\n",
    "        '--epochs', str(epochs),\n",
    "        '--batch-size', str(batch_size),\n",
    "        '--imgsz', str(img_size),\n",
    "        '--device', device_param,\n",
    "        '--workers', str(workers),\n",
    "        '--project', 'runs/train',\n",
    "        '--name', 'tree_detection_professional',\n",
    "        '--optimizer', 'SGD',                   # 推荐的优化器 / recommended optimizer\n",
    "        '--patience', '30',                     # 早停耐心值 / early stopping patience\n",
    "        '--save-period', '25',                  # 定期保存 / periodic saving\n",
    "        '--cache',                              # 缓存图像 / cache images\n",
    "        '--exist-ok'\n",
    "    ]\n",
    "    \n",
    "    # 高端GPU的额外优化 / Additional optimization for high-end GPUs\n",
    "    if torch.cuda.is_available() and gpu_memory >= 8:\n",
    "        professional_command.append('--multi-scale')  # 多尺度训练 / multi-scale training\n",
    "        if gpu_memory >= 15:\n",
    "            professional_command.append('--amp')       # 混合精度训练 / mixed precision training\n",
    "    \n",
    "    print(f\"\\n修复后的训练命令 / Fixed training command:\")\n",
    "    cmd_str = ' '.join(professional_command)\n",
    "    print(f\"   {cmd_str}\")\n",
    "    \n",
    "    print(f\"\\n开始专业训练 / Starting professional training...\")\n",
    "    print(f\"策略 / Strategy: 官方预训练模型 + 简化稳定参数 / Official pretrained model + simplified stable parameters\")\n",
    "    print(f\"目标 / Goal: 高质量树木检测模型 / High-quality tree detection model\")\n",
    "    \n",
    "    # 执行专业训练 / Execute professional training\n",
    "    try:\n",
    "        memory_cleanup()  # 训练前清理 / Pre-training cleanup\n",
    "        \n",
    "        print(\"训练进行中，请耐心等待 / Training in progress, please wait patiently...\")\n",
    "        result = subprocess.run(\n",
    "            professional_command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=7200  # 2小时超时 / 2 hour timeout\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"专业训练成功完成 / Professional training successfully completed!\")\n",
    "            \n",
    "            # 检查训练结果 / Check training results\n",
    "            model_dir = 'runs/train/tree_detection_professional/weights'\n",
    "            if os.path.exists(model_dir):\n",
    "                best_model = os.path.join(model_dir, 'best.pt')\n",
    "                last_model = os.path.join(model_dir, 'last.pt')\n",
    "                \n",
    "                if os.path.exists(best_model):\n",
    "                    model_size_mb = os.path.getsize(best_model) / (1024 * 1024)\n",
    "                    print(f\"最佳模型 / Best model: {best_model} ({model_size_mb:.1f}MB)\")\n",
    "                    final_model_path = best_model\n",
    "                elif os.path.exists(last_model):\n",
    "                    model_size_mb = os.path.getsize(last_model) / (1024 * 1024)\n",
    "                    print(f\"最新模型 / Latest model: {last_model} ({model_size_mb:.1f}MB)\")\n",
    "                    final_model_path = last_model\n",
    "                else:\n",
    "                    final_model_path = None\n",
    "                    print(\"未找到训练模型 / No training model found\")\n",
    "                \n",
    "                # 显示训练总结 / Show training summary\n",
    "                print(\"\\n训练输出总结 / Training output summary:\")\n",
    "                output_lines = result.stdout.split('\\n')\n",
    "                key_lines = []\n",
    "                for line in output_lines:\n",
    "                    if any(keyword in line.lower() for keyword in \n",
    "                          ['results', 'best', 'map', 'precision', 'recall', 'fitness', 'epoch']):\n",
    "                        key_lines.append(line)\n",
    "                \n",
    "                # 显示最后15行关键信息 / Show last 15 lines of key information\n",
    "                for line in key_lines[-15:]:\n",
    "                    if line.strip():\n",
    "                        print(f\"   {line}\")\n",
    "                        \n",
    "            else:\n",
    "                print(\"训练结果目录不存在 / Training results directory does not exist\")\n",
    "                final_model_path = None\n",
    "                \n",
    "        else:\n",
    "            print(f\"训练失败，返回码 / Training failed, return code: {result.returncode}\")\n",
    "            print(\"\\n错误输出 / Error output:\")\n",
    "            if result.stderr:\n",
    "                print(result.stderr[:1500])\n",
    "            \n",
    "            print(\"\\n标准输出片段 / Standard output fragment:\")\n",
    "            if result.stdout:\n",
    "                stdout_lines = result.stdout.split('\\n')\n",
    "                for line in stdout_lines[-20:]:\n",
    "                    if line.strip():\n",
    "                        print(f\"   {line}\")\n",
    "            \n",
    "            final_model_path = None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"训练超时，检查部分结果 / Training timeout, checking partial results...\")\n",
    "        model_dir = 'runs/train/tree_detection_professional/weights'\n",
    "        if os.path.exists(os.path.join(model_dir, 'last.pt')):\n",
    "            final_model_path = os.path.join(model_dir, 'last.pt')\n",
    "            print(f\"找到部分训练模型 / Found partial training model: {final_model_path}\")\n",
    "        else:\n",
    "            final_model_path = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"训练异常 / Training exception: {e}\")\n",
    "        final_model_path = None\n",
    "    \n",
    "    # 最终清理和总结 / Final cleanup and summary\n",
    "    memory_cleanup()\n",
    "    \n",
    "    print(\"\\nYOLOv5专业训练完成 / YOLOv5 professional training completed\")\n",
    "    if final_model_path:\n",
    "        print(f\"专业训练模型 / Professional training model: {final_model_path}\")\n",
    "        print(\"基于官方预训练模型和稳定参数 / Based on official pretrained model and stable parameters\")\n",
    "        print(\"使用简化配置确保兼容性 / Using simplified configuration to ensure compatibility\")\n",
    "        print(\"运行Cell 9进行模型推理测试 / Run Cell 9 for model inference testing\")\n",
    "    else:\n",
    "        print(\"训练未成功完成 / Training not successfully completed\")\n",
    "        print(\"可能的解决方案 / Possible solutions:\")\n",
    "        print(\"   1. 检查数据集格式和路径 / Check dataset format and paths\")\n",
    "        print(\"   2. 确认GPU内存充足 / Confirm sufficient GPU memory\")\n",
    "        print(\"   3. 检查YOLOv5环境安装 / Check YOLOv5 environment installation\")\n",
    "        print(\"   4. 尝试降低batch_size参数 / Try reducing batch_size parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 专业模型推理测试 / Professional Model Inference Testing\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def comprehensive_cleanup():\n",
    "    \"\"\"综合内存清理 / Comprehensive memory cleanup\"\"\"\n",
    "    try:\n",
    "        print(\"执行内存清理 / Executing memory cleanup...\")\n",
    "        \n",
    "        # Python垃圾回收 / Python garbage collection\n",
    "        collected = gc.collect()\n",
    "        print(f\"   垃圾回收释放对象 / Garbage collection freed objects: {collected}\")\n",
    "        \n",
    "        # CUDA缓存清理 / CUDA cache cleanup\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "            print(f\"   CUDA内存 / CUDA memory: 已分配 / allocated {allocated:.1f}GB, 已保留 / reserved {reserved:.1f}GB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"清理过程警告 / Cleanup process warning: {e}\")\n",
    "        return False\n",
    "\n",
    "def professional_inference(model_path, test_images, confidence_threshold=0.25):\n",
    "    \"\"\"专业模型推理 / Professional model inference\"\"\"\n",
    "    print(f\"专业模型推理测试 / Professional model inference testing: {model_path}\")\n",
    "    print(f\"测试图像数量 / Test image count: {len(test_images)}\")\n",
    "    print(f\"置信度阈值 / Confidence threshold: {confidence_threshold}\")\n",
    "    \n",
    "    # 创建输出目录 / Create output directory\n",
    "    output_dir = 'professional_inference_results'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 构建推理命令 / Build inference command\n",
    "    inference_command = [\n",
    "        sys.executable, 'yolov5/detect.py',\n",
    "        '--weights', model_path,\n",
    "        '--source', str(test_images[0].parent),  # 使用图像目录 / Use image directory\n",
    "        '--project', output_dir,\n",
    "        '--name', 'detect',\n",
    "        '--img', '640',\n",
    "        '--conf', str(confidence_threshold),\n",
    "        '--iou', '0.45',\n",
    "        '--max-det', '1000',\n",
    "        '--save-txt',\n",
    "        '--save-conf',\n",
    "        '--exist-ok'\n",
    "    ]\n",
    "    \n",
    "    # 添加设备参数 / Add device parameter\n",
    "    if torch.cuda.is_available():\n",
    "        inference_command.extend(['--device', '0'])\n",
    "    else:\n",
    "        inference_command.extend(['--device', 'cpu'])\n",
    "    \n",
    "    print(f\"\\n执行推理命令 / Executing inference command:\")\n",
    "    print(f\"   {' '.join(inference_command)}\")\n",
    "    \n",
    "    # 执行推理 / Execute inference\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            inference_command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300  # 5分钟超时 / 5 minute timeout\n",
    "        )\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"推理成功完成 / Inference successfully completed (耗时 / time: {inference_time:.1f}s)\")\n",
    "            \n",
    "            # 分析推理结果 / Analyze inference results\n",
    "            results = analyze_inference_results(output_dir)\n",
    "            return results\n",
    "            \n",
    "        else:\n",
    "            print(f\"推理失败 / Inference failed: {result.stderr[:500]}\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"推理超时 / Inference timeout\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"推理异常 / Inference exception: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_inference_results(output_dir):\n",
    "    \"\"\"分析推理结果 / Analyze inference results\"\"\"\n",
    "    print(\"\\n分析推理结果 / Analyzing inference results...\")\n",
    "    \n",
    "    results_dir = Path(output_dir) / 'detect'\n",
    "    if not results_dir.exists():\n",
    "        print(\"推理结果目录不存在 / Inference results directory does not exist\")\n",
    "        return None\n",
    "    \n",
    "    # 统计检测结果 / Count detection results\n",
    "    total_detections = 0\n",
    "    confidence_scores = []\n",
    "    image_counts = {}\n",
    "    \n",
    "    # 分析标签文件 / Analyze label files\n",
    "    label_dir = results_dir / 'labels'\n",
    "    if label_dir.exists():\n",
    "        label_files = list(label_dir.glob('*.txt'))\n",
    "        print(f\"   处理标签文件 / Processing label files: {len(label_files)}\")\n",
    "        \n",
    "        for label_file in label_files:\n",
    "            image_name = label_file.stem\n",
    "            detections_in_image = 0\n",
    "            \n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 6:\n",
    "                            conf = float(parts[5])\n",
    "                            confidence_scores.append(conf)\n",
    "                            total_detections += 1\n",
    "                            detections_in_image += 1\n",
    "            \n",
    "            image_counts[image_name] = detections_in_image\n",
    "    \n",
    "    # 检查输出图像 / Check output images\n",
    "    output_images = list(results_dir.glob('*.jpg')) + list(results_dir.glob('*.png'))\n",
    "    \n",
    "    # 计算统计信息 / Calculate statistics\n",
    "    if confidence_scores:\n",
    "        avg_confidence = np.mean(confidence_scores)\n",
    "        max_confidence = np.max(confidence_scores)\n",
    "        min_confidence = np.min(confidence_scores)\n",
    "        std_confidence = np.std(confidence_scores)\n",
    "        \n",
    "        # 置信度分布统计 / Confidence distribution statistics\n",
    "        high_conf_count = sum(1 for c in confidence_scores if c > 0.7)\n",
    "        medium_conf_count = sum(1 for c in confidence_scores if 0.4 <= c <= 0.7)\n",
    "        low_conf_count = sum(1 for c in confidence_scores if c < 0.4)\n",
    "        \n",
    "        print(f\"\\n检测统计 / Detection statistics:\")\n",
    "        print(f\"   总检测数量 / Total detections: {total_detections}\")\n",
    "        print(f\"   平均置信度 / Average confidence: {avg_confidence:.3f}\")\n",
    "        print(f\"   置信度范围 / Confidence range: {min_confidence:.3f} - {max_confidence:.3f}\")\n",
    "        print(f\"   置信度标准差 / Confidence std: {std_confidence:.3f}\")\n",
    "        print(f\"   高置信度检测(>0.7) / High confidence detections (>0.7): {high_conf_count}\")\n",
    "        print(f\"   中等置信度检测(0.4-0.7) / Medium confidence detections (0.4-0.7): {medium_conf_count}\")\n",
    "        print(f\"   低置信度检测(<0.4) / Low confidence detections (<0.4): {low_conf_count}\")\n",
    "        \n",
    "        # 每张图像检测统计 / Per-image detection statistics\n",
    "        if image_counts:\n",
    "            avg_detections_per_image = np.mean(list(image_counts.values()))\n",
    "            max_detections = max(image_counts.values())\n",
    "            print(f\"\\n图像检测统计 / Image detection statistics:\")\n",
    "            print(f\"   处理图像数量 / Processed images: {len(image_counts)}\")\n",
    "            print(f\"   平均每图检测数 / Average detections per image: {avg_detections_per_image:.1f}\")\n",
    "            print(f\"   最大单图检测数 / Maximum detections in single image: {max_detections}\")\n",
    "            \n",
    "            # 显示检测最多的图像 / Show images with most detections\n",
    "            top_images = sorted(image_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"   检测数最多的图像 / Images with most detections:\")\n",
    "            for img_name, count in top_images:\n",
    "                print(f\"     {img_name}: {count} 个检测 / detections\")\n",
    "        \n",
    "        # 输出图像信息 / Output image information\n",
    "        if output_images:\n",
    "            print(f\"\\n输出文件 / Output files:\")\n",
    "            print(f\"   检测结果图像 / Detection result images: {len(output_images)}\")\n",
    "            for img_path in output_images[:5]:  # 显示前5个 / Show first 5\n",
    "                print(f\"     {img_path.name}\")\n",
    "            if len(output_images) > 5:\n",
    "                print(f\"     ... 还有 {len(output_images)-5} 个文件 / and {len(output_images)-5} more files\")\n",
    "        \n",
    "        return {\n",
    "            'total_detections': total_detections,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'max_confidence': max_confidence,\n",
    "            'min_confidence': min_confidence,\n",
    "            'std_confidence': std_confidence,\n",
    "            'high_conf_count': high_conf_count,\n",
    "            'medium_conf_count': medium_conf_count,\n",
    "            'low_conf_count': low_conf_count,\n",
    "            'confidence_scores': confidence_scores,\n",
    "            'image_counts': image_counts,\n",
    "            'output_images': output_images,\n",
    "            'results_dir': str(results_dir)\n",
    "        }\n",
    "    else:\n",
    "        print(\"未检测到任何对象 / No objects detected\")\n",
    "        return {\n",
    "            'total_detections': 0,\n",
    "            'avg_confidence': 0,\n",
    "            'results_dir': str(results_dir)\n",
    "        }\n",
    "\n",
    "def visualize_results(results):\n",
    "    \"\"\"可视化推理结果 / Visualize inference results\"\"\"\n",
    "    if not results or results['total_detections'] == 0:\n",
    "        print(\"无检测结果可视化 / No detection results to visualize\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n创建结果可视化图表 / Creating result visualization charts...\")\n",
    "    \n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 置信度分布直方图 / Confidence distribution histogram\n",
    "        confidences = results['confidence_scores']\n",
    "        axes[0].hist(confidences, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0].set_title('检测置信度分布 / Detection Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('置信度 / Confidence')\n",
    "        axes[0].set_ylabel('频次 / Frequency')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 添加统计线 / Add statistics line\n",
    "        avg_conf = results['avg_confidence']\n",
    "        axes[0].axvline(avg_conf, color='red', linestyle='--', linewidth=2,\n",
    "                       label=f'平均置信度 / Average: {avg_conf:.3f}')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # 置信度等级饼图 / Confidence level pie chart\n",
    "        high_count = results['high_conf_count']\n",
    "        medium_count = results['medium_conf_count'] \n",
    "        low_count = results['low_conf_count']\n",
    "        \n",
    "        labels = ['高置信度(>0.7)\\nHigh (>0.7)', '中等置信度(0.4-0.7)\\nMedium (0.4-0.7)', '低置信度(<0.4)\\nLow (<0.4)']\n",
    "        sizes = [high_count, medium_count, low_count]\n",
    "        colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "        \n",
    "        # 过滤掉为0的项 / Filter out zero items\n",
    "        filtered_data = [(label, size, color) for label, size, color in zip(labels, sizes, colors) if size > 0]\n",
    "        if filtered_data:\n",
    "            labels, sizes, colors = zip(*filtered_data)\n",
    "            axes[1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "            axes[1].set_title('置信度等级分布 / Confidence Level Distribution', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, '无检测数据 / No detection data', ha='center', va='center', \n",
    "                        transform=axes[1].transAxes, fontsize=16)\n",
    "            axes[1].set_title('置信度等级分布 / Confidence Level Distribution', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"可视化图表创建完成 / Visualization charts created successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"可视化创建失败 / Visualization creation failed: {e}\")\n",
    "\n",
    "def show_sample_results(results):\n",
    "    \"\"\"显示示例检测结果 / Show sample detection results\"\"\"\n",
    "    if not results or not results.get('output_images'):\n",
    "        print(\"无示例图像可显示 / No sample images to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n显示检测结果示例 / Displaying detection result samples...\")\n",
    "    \n",
    "    try:\n",
    "        output_images = results['output_images']\n",
    "        sample_count = min(3, len(output_images))\n",
    "        \n",
    "        if sample_count > 0:\n",
    "            fig, axes = plt.subplots(1, sample_count, figsize=(5*sample_count, 5))\n",
    "            if sample_count == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i in range(sample_count):\n",
    "                img_path = output_images[i]\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is not None:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    axes[i].imshow(img_rgb)\n",
    "                    axes[i].set_title(f'检测结果 / Detection result: {img_path.name}', fontsize=12)\n",
    "                    axes[i].axis('off')\n",
    "                else:\n",
    "                    axes[i].text(0.5, 0.5, '图像加载失败 / Image loading failed', ha='center', va='center',\n",
    "                               transform=axes[i].transAxes)\n",
    "                    axes[i].set_title(f'错误 / Error: {img_path.name}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            print(f\"显示了 {sample_count} 个检测结果示例 / Displayed {sample_count} detection result samples\")\n",
    "        else:\n",
    "            print(\"没有可显示的检测结果图像 / No detection result images to display\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"示例显示失败 / Sample display failed: {e}\")\n",
    "\n",
    "# 主执行流程 / Main execution flow\n",
    "print(\"启动专业模型推理测试 / Starting professional model inference testing...\")\n",
    "\n",
    "# 清理内存 / Memory cleanup\n",
    "comprehensive_cleanup()\n",
    "\n",
    "# 查找训练好的模型 / Find trained models\n",
    "model_candidates = [\n",
    "    'runs/train/tree_detection_professional/weights/best.pt',\n",
    "    'runs/train/tree_detection_professional/weights/last.pt',\n",
    "    'runs/train/tree_detection_modern/weights/best.pt',\n",
    "    'runs/train/tree_detection_basic/weights/best.pt'\n",
    "]\n",
    "\n",
    "available_model = None\n",
    "for model_path in model_candidates:\n",
    "    if os.path.exists(model_path):\n",
    "        model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "        print(f\"找到训练模型 / Found training model: {model_path} ({model_size:.1f}MB)\")\n",
    "        available_model = model_path\n",
    "        break\n",
    "\n",
    "if not available_model:\n",
    "    print(\"未找到训练好的模型 / No trained model found\")\n",
    "    print(\"请先运行Cell 8完成模型训练 / Please run Cell 8 first to complete model training\")\n",
    "else:\n",
    "    # 查找测试图像 / Find test images\n",
    "    test_dirs = [\n",
    "        'data/processed/images/test',\n",
    "        'data/processed/images/val', \n",
    "        'data/processed/images/train'\n",
    "    ]\n",
    "    \n",
    "    test_images = []\n",
    "    for test_dir in test_dirs:\n",
    "        if os.path.exists(test_dir):\n",
    "            images = list(Path(test_dir).glob('*.jpg'))\n",
    "            test_images.extend(images)\n",
    "            if len(test_images) >= 10:  # 限制测试图像数量 / Limit test image count\n",
    "                test_images = test_images[:10]\n",
    "                break\n",
    "    \n",
    "    if not test_images:\n",
    "        print(\"未找到测试图像 / No test images found\")\n",
    "    else:\n",
    "        print(f\"找到测试图像 / Found test images: {len(test_images)} 张 / images\")\n",
    "        \n",
    "        # 执行专业推理测试 / Execute professional inference testing\n",
    "        results = professional_inference(available_model, test_images)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"\\n推理测试完成 / Inference testing completed!\")\n",
    "            \n",
    "            # 可视化结果 / Visualize results\n",
    "            visualize_results(results)\n",
    "            \n",
    "            # 显示示例结果 / Show sample results\n",
    "            show_sample_results(results)\n",
    "            \n",
    "            # 总结报告 / Summary report\n",
    "            print(f\"\\n推理测试总结报告 / Inference testing summary report:\")\n",
    "            print(f\"   模型 / Model: {available_model}\")\n",
    "            print(f\"   测试图像 / Test images: {len(test_images)}\")\n",
    "            print(f\"   总检测数 / Total detections: {results['total_detections']}\")\n",
    "            if results['total_detections'] > 0:\n",
    "                print(f\"   平均置信度 / Average confidence: {results['avg_confidence']:.3f}\")\n",
    "                print(f\"   高质量检测(>0.7) / High quality detections (>0.7): {results['high_conf_count']}\")\n",
    "            print(f\"   结果目录 / Results directory: {results['results_dir']}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"推理测试失败 / Inference testing failed\")\n",
    "\n",
    "# 最终清理 / Final cleanup\n",
    "comprehensive_cleanup()\n",
    "print(\"专业模型推理测试完成 / Professional model inference testing completed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
