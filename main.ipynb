{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌲 树木检测项目 - Tree Detection Project\n",
    "\n",
    "## 项目概述 Project Overview\n",
    "\n",
    "本项目使用YOLOv5和PyTorch实现基于航空影像的树木检测任务  \n",
    "This project implements tree detection from aerial imagery using YOLOv5 and PyTorch\n",
    "\n",
    "**数据集 Dataset**: NeonTreeEvaluation Benchmark  \n",
    "**目标 Goal**: 检测航空正射影像中的树木 - Detect trees in aerial orthoimagery  \n",
    "**平台 Platform**: Google Colab (自动GPU检测 - Auto GPU detection)\n",
    "\n",
    "## 技术栈 Tech Stack\n",
    "- **深度学习框架 Deep Learning**: PyTorch + YOLOv5\n",
    "- **数据处理 Data Processing**: OpenCV, PIL, pandas\n",
    "- **可视化 Visualization**: matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 1. 环境检测和基础设置 - Environment Detection and Basic Setup\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"🚀 环境检测 Environment Detection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 检查Python版本 Check Python version\n",
    "print(f\"Python版本 Python Version: {sys.version}\")\n",
    "\n",
    "# 检查操作系统 Check OS\n",
    "print(f\"操作系统 Operating System: {platform.system()}\")\n",
    "\n",
    "# 检查是否在Colab环境 Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"✅ 运行环境: Google Colab - Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"❌ 运行环境: 本地环境 - Running locally\")\n",
    "\n",
    "# 检查GPU可用性 Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✅ GPU可用 GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA版本 CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   GPU数量 GPU Count: {torch.cuda.device_count()}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"⚠️  GPU不可用，将使用CPU - GPU not available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorch版本 PyTorch Version: {torch.__version__}\")\n",
    "print(f\"设备 Device: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 安装必要依赖 - Install Required Dependencies\n",
    "print(\"📦 安装依赖包 Installing Dependencies...\")\n",
    "\n",
    "# 安装YOLOv5和相关依赖 Install YOLOv5 and dependencies\n",
    "!pip install -q ultralytics\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q Pillow\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q pandas\n",
    "!pip install -q tqdm\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q PyYAML\n",
    "\n",
    "# 如果在Colab环境，挂载Google Drive (可选)\n",
    "# Mount Google Drive in Colab (optional)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/drive')  # 取消注释以挂载Drive - Uncomment to mount Drive\n",
    "\n",
    "print(\"✅ 依赖安装完成 Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 导入所需库 - Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置matplotlib中文字体支持 Set matplotlib Chinese font support\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子 Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"✅ 库导入完成 Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 数据下载和解压 - Data Download and Extraction\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    下载文件的函数 Function to download files\n",
    "    \"\"\"\n",
    "    print(f\"🔄 开始下载 Starting download: {filename}\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filename, 'wb') as file, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    print(f\"✅ 下载完成 Download completed: {filename}\")\n",
    "\n",
    "# 创建数据目录 Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# 数据集URL Dataset URLs\n",
    "DATASET_URL = \"https://zenodo.org/records/5914554/files/evaluation.zip?download=1\"\n",
    "ANNOTATIONS_URL = \"https://zenodo.org/records/5914554/files/annotations.zip?download=1\"\n",
    "\n",
    "# 下载数据集 Download datasets\n",
    "if not os.path.exists('data/raw/evaluation.zip'):\n",
    "    download_file(DATASET_URL, 'data/raw/evaluation.zip')\n",
    "else:\n",
    "    print(\"✅ 评估数据集已存在 Evaluation dataset already exists\")\n",
    "\n",
    "if not os.path.exists('data/raw/annotations.zip'):\n",
    "    download_file(ANNOTATIONS_URL, 'data/raw/annotations.zip')\n",
    "else:\n",
    "    print(\"✅ 标注数据已存在 Annotations already exist\")\n",
    "\n",
    "print(\"📁 数据下载完成 Data download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 解压数据集 - Extract Datasets\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    解压ZIP文件的函数 Function to extract ZIP files\n",
    "    \"\"\"\n",
    "    print(f\"📦 解压文件 Extracting: {zip_path}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    print(f\"✅ 解压完成 Extraction completed: {extract_to}\")\n",
    "\n",
    "# 解压评估数据集 Extract evaluation dataset\n",
    "if not os.path.exists('data/raw/evaluation'):\n",
    "    extract_zip('data/raw/evaluation.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"✅ 评估数据集已解压 Evaluation dataset already extracted\")\n",
    "\n",
    "# 解压标注数据 Extract annotations\n",
    "if not os.path.exists('data/raw/annotations'):\n",
    "    extract_zip('data/raw/annotations.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"✅ 标注数据已解压 Annotations already extracted\")\n",
    "\n",
    "# 查看数据结构 Explore data structure\n",
    "print(\"\\n📊 数据结构分析 Data Structure Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 检查评估数据集结构 Check evaluation dataset structure\n",
    "eval_path = Path('data/raw/evaluation')\n",
    "if eval_path.exists():\n",
    "    print(f\"评估数据集路径 Evaluation dataset path: {eval_path}\")\n",
    "    subdirs = [d for d in eval_path.iterdir() if d.is_dir()]\n",
    "    print(f\"子目录数量 Number of subdirectories: {len(subdirs)}\")\n",
    "    for subdir in subdirs[:5]:  # 显示前5个子目录 Show first 5 subdirectories\n",
    "        print(f\"  - {subdir.name}\")\n",
    "    if len(subdirs) > 5:\n",
    "        print(f\"  ... 还有 {len(subdirs)-5} 个目录 and {len(subdirs)-5} more directories\")\n",
    "\n",
    "# 检查标注数据结构 Check annotations structure\n",
    "ann_path = Path('data/raw/annotations')\n",
    "if ann_path.exists():\n",
    "    print(f\"\\n标注数据路径 Annotations path: {ann_path}\")\n",
    "    ann_files = list(ann_path.glob('*.csv'))\n",
    "    print(f\"CSV标注文件数量 Number of CSV annotation files: {len(ann_files)}\")\n",
    "    for ann_file in ann_files[:3]:  # 显示前3个标注文件 Show first 3 annotation files\n",
    "        print(f\"  - {ann_file.name}\")\n",
    "\n",
    "print(\"\\n✅ 数据解压和结构分析完成 Data extraction and structure analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 数据格式转换 - Data Format Conversion (NeonTree -> YOLO) - 使用真实图像\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class NeonTreeToYOLO:\n",
    "    \"\"\"\n",
    "    将NeonTree数据集转换为YOLO格式的类\n",
    "    Class to convert NeonTree dataset to YOLO format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, annotations_root, evaluation_root, output_root):\n",
    "        self.annotations_root = Path(annotations_root)\n",
    "        self.evaluation_root = Path(evaluation_root)\n",
    "        self.output_root = Path(output_root)\n",
    "        \n",
    "        # 创建输出目录 Create output directories\n",
    "        self.create_yolo_structure()\n",
    "        \n",
    "        # 创建图像文件名映射 Create image filename mapping\n",
    "        self.create_image_mapping()\n",
    "    \n",
    "    def create_yolo_structure(self):\n",
    "        \"\"\"创建YOLO数据集目录结构 Create YOLO dataset directory structure\"\"\"\n",
    "        folders = [\n",
    "            'images/train', 'images/val', 'images/test',\n",
    "            'labels/train', 'labels/val', 'labels/test'\n",
    "        ]\n",
    "        \n",
    "        for folder in folders:\n",
    "            (self.output_root / folder).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"✅ YOLO目录结构创建完成 YOLO directory structure created\")\n",
    "    \n",
    "    def create_image_mapping(self):\n",
    "        \"\"\"\n",
    "        创建图像文件名到路径的映射\n",
    "        Create mapping from image filenames to paths\n",
    "        \"\"\"\n",
    "        self.image_mapping = {}\n",
    "        \n",
    "        # 检查 evaluation/RGB 目录\n",
    "        rgb_dir = self.evaluation_root / 'RGB'\n",
    "        if rgb_dir.exists():\n",
    "            for img_file in rgb_dir.glob('*.tif'):\n",
    "                # 去掉扩展名作为key\n",
    "                base_name = img_file.stem\n",
    "                self.image_mapping[base_name] = img_file\n",
    "                \n",
    "                # 同时添加不带扩展名的映射\n",
    "                if base_name.endswith('.tif'):\n",
    "                    base_name_no_ext = base_name[:-4]\n",
    "                    self.image_mapping[base_name_no_ext] = img_file\n",
    "        \n",
    "        print(f\"📋 创建图像映射 Created image mapping with {len(self.image_mapping)} entries\")\n",
    "    \n",
    "    def find_matching_image(self, xml_filename):\n",
    "        \"\"\"\n",
    "        根据XML文件名查找对应的RGB图像文件\n",
    "        Find matching RGB image file based on XML filename\n",
    "        \"\"\"\n",
    "        # 从XML文件名中提取基础名称\n",
    "        base_name = xml_filename.replace('.xml', '')\n",
    "        \n",
    "        # 直接匹配\n",
    "        if base_name in self.image_mapping:\n",
    "            return self.image_mapping[base_name]\n",
    "        \n",
    "        # 尝试模糊匹配 - 查找包含base_name的图像文件\n",
    "        for img_name, img_path in self.image_mapping.items():\n",
    "            # 检查是否匹配（去掉年份等变化部分）\n",
    "            if self.files_match(base_name, img_name):\n",
    "                return img_path\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def files_match(self, xml_name, img_name):\n",
    "        \"\"\"\n",
    "        判断XML文件名和图像文件名是否匹配\n",
    "        Check if XML filename matches image filename\n",
    "        \"\"\"\n",
    "        # 移除常见的后缀和前缀\n",
    "        xml_clean = xml_name.lower()\n",
    "        img_clean = img_name.lower()\n",
    "        \n",
    "        # 提取核心站点代码\n",
    "        xml_parts = xml_clean.split('_')\n",
    "        img_parts = img_clean.split('_')\n",
    "        \n",
    "        # 如果站点代码匹配\n",
    "        if len(xml_parts) >= 2 and len(img_parts) >= 2:\n",
    "            xml_site = xml_parts[0]\n",
    "            img_site = img_parts[0]\n",
    "            \n",
    "            if xml_site == img_site:\n",
    "                # 进一步检查编号匹配\n",
    "                if len(xml_parts) >= 3 and len(img_parts) >= 2:\n",
    "                    try:\n",
    "                        xml_num = xml_parts[1]\n",
    "                        img_num = img_parts[1]\n",
    "                        return xml_num == img_num\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # 检查是否包含相同的关键部分\n",
    "        return any(part in img_clean for part in xml_parts if len(part) > 2)\n",
    "    \n",
    "    def parse_xml_annotation(self, xml_file):\n",
    "        \"\"\"\n",
    "        解析XML标注文件\n",
    "        Parse XML annotation file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # 获取图像信息 Get image information\n",
    "            filename_elem = root.find('filename')\n",
    "            if filename_elem is not None:\n",
    "                filename = filename_elem.text\n",
    "            else:\n",
    "                filename = xml_file.stem + '.tif'  # 默认扩展名\n",
    "            \n",
    "            size = root.find('size')\n",
    "            width = int(size.find('width').text)\n",
    "            height = int(size.find('height').text)\n",
    "            \n",
    "            # 获取所有树木对象 Get all tree objects\n",
    "            objects = []\n",
    "            for obj in root.findall('object'):\n",
    "                name_elem = obj.find('name')\n",
    "                if name_elem is not None and name_elem.text.lower() == 'tree':\n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    xmin = int(float(bndbox.find('xmin').text))\n",
    "                    ymin = int(float(bndbox.find('ymin').text))\n",
    "                    xmax = int(float(bndbox.find('xmax').text))\n",
    "                    ymax = int(float(bndbox.find('ymax').text))\n",
    "                    \n",
    "                    # 验证边界框有效性 Validate bounding box\n",
    "                    if xmax > xmin and ymax > ymin:\n",
    "                        objects.append({\n",
    "                            'xmin': xmin,\n",
    "                            'ymin': ymin,\n",
    "                            'xmax': xmax,\n",
    "                            'ymax': ymax\n",
    "                        })\n",
    "            \n",
    "            return {\n",
    "                'filename': filename,\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'objects': objects\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"解析XML文件时出错 Error parsing XML file {xml_file}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def convert_bbox_to_yolo(self, bbox, img_width, img_height):\n",
    "        \"\"\"\n",
    "        将边界框坐标转换为YOLO格式\n",
    "        Convert bounding box coordinates to YOLO format\n",
    "        \n",
    "        YOLO格式: [class_id, x_center, y_center, width, height] (归一化 normalized)\n",
    "        \"\"\"\n",
    "        xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']\n",
    "        \n",
    "        # 计算中心点和宽高 Calculate center point and dimensions\n",
    "        x_center = (xmin + xmax) / 2.0\n",
    "        y_center = (ymin + ymax) / 2.0\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        # 归一化 Normalize\n",
    "        x_center /= img_width\n",
    "        y_center /= img_height\n",
    "        width /= img_width\n",
    "        height /= img_height\n",
    "        \n",
    "        return [0, x_center, y_center, width, height]  # 类别ID为0 (树木 tree)\n",
    "    \n",
    "    def process_annotations(self):\n",
    "        \"\"\"\n",
    "        处理所有XML标注文件并转换为YOLO格式\n",
    "        Process all XML annotation files and convert to YOLO format\n",
    "        \"\"\"\n",
    "        xml_files = list(self.annotations_root.glob('*.xml'))\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        print(f\"📝 找到 {len(xml_files)} 个XML标注文件 Found {len(xml_files)} XML annotation files\")\n",
    "        \n",
    "        for i, xml_file in enumerate(tqdm(xml_files, desc=\"处理标注 Processing annotations\")):\n",
    "            try:\n",
    "                # 解析XML文件 Parse XML file\n",
    "                annotation_data = self.parse_xml_annotation(xml_file)\n",
    "                \n",
    "                if annotation_data is None or not annotation_data['objects']:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # 查找对应的RGB图像文件 Find corresponding RGB image file\n",
    "                img_path = self.find_matching_image(xml_file.name)\n",
    "                \n",
    "                if img_path is None:\n",
    "                    # 打印调试信息，但不中断处理 Print debug info but don't interrupt\n",
    "                    if processed_count < 5:  # 只打印前5个错误\n",
    "                        print(f\"⚠️  未找到对应图像 No matching image for: {xml_file.name}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # 读取真实图像 Load real image\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    print(f\"⚠️  无法读取图像 Cannot read image: {img_path}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # 验证图像尺寸 Verify image dimensions\n",
    "                actual_height, actual_width = img.shape[:2]\n",
    "                if actual_width != annotation_data['width'] or actual_height != annotation_data['height']:\n",
    "                    # 使用实际图像尺寸 Use actual image dimensions\n",
    "                    annotation_data['width'] = actual_width\n",
    "                    annotation_data['height'] = actual_height\n",
    "                \n",
    "                # 确定数据集分割 Determine dataset split\n",
    "                if i % 10 < 7:  # 70% 训练集 training set\n",
    "                    split = 'train'\n",
    "                elif i % 10 < 9:  # 20% 验证集 validation set\n",
    "                    split = 'val'\n",
    "                else:  # 10% 测试集 test set\n",
    "                    split = 'test'\n",
    "                \n",
    "                # 保存图像 Save image\n",
    "                img_filename = f\"tree_{processed_count:06d}.jpg\"\n",
    "                img_save_path = self.output_root / 'images' / split / img_filename\n",
    "                cv2.imwrite(str(img_save_path), img)\n",
    "                \n",
    "                # 保存YOLO格式标签 Save YOLO format labels\n",
    "                label_filename = f\"tree_{processed_count:06d}.txt\"\n",
    "                label_save_path = self.output_root / 'labels' / split / label_filename\n",
    "                \n",
    "                with open(label_save_path, 'w') as f:\n",
    "                    for obj in annotation_data['objects']:\n",
    "                        yolo_bbox = self.convert_bbox_to_yolo(\n",
    "                            obj, annotation_data['width'], annotation_data['height']\n",
    "                        )\n",
    "                        # YOLO格式: class_id x_center y_center width height (修复换行符)\n",
    "                        f.write(f\"{yolo_bbox[0]} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f} {yolo_bbox[4]:.6f}\\n\")\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"处理文件 {xml_file.name} 时出错 Error processing file {xml_file.name}: {e}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "        \n",
    "        print(f\"✅ 处理完成 Processing completed: {processed_count} 个样本 samples processed, {skipped_count} 个跳过 skipped\")\n",
    "        return processed_count\n",
    "\n",
    "# 执行数据转换 Execute data conversion\n",
    "print(\"🔄 开始数据格式转换 Starting data format conversion...\")\n",
    "print(\"📋 使用XML格式的标注数据和真实RGB图像 Using XML annotations with real RGB images\")\n",
    "\n",
    "# 创建data目录 Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# 检查annotations和evaluation数据是否存在 Check if annotations and evaluation data exist\n",
    "annotation_paths = ['data/raw/annotations', 'annotations', '/content/annotations']\n",
    "eval_paths = ['data/raw/evaluation', 'evaluation', '/content/evaluation']\n",
    "\n",
    "annotations_root = None\n",
    "eval_root = None\n",
    "\n",
    "# 查找annotations目录 Find annotations directory\n",
    "for path in annotation_paths:\n",
    "    if Path(path).exists():\n",
    "        annotations_root = Path(path)\n",
    "        xml_count = len(list(annotations_root.glob('*.xml')))\n",
    "        if xml_count > 0:\n",
    "            print(f\"✅ 找到annotations数据集 Found annotations dataset at: {annotations_root} ({xml_count} XML files)\")\n",
    "            break\n",
    "\n",
    "if annotations_root is None:\n",
    "    print(\"❌ 未找到annotations数据集或XML文件\")\n",
    "    print(\"📁 请确保annotations目录存在且包含XML文件\")\n",
    "\n",
    "# 查找evaluation目录 Find evaluation directory  \n",
    "for path in eval_paths:\n",
    "    if Path(path).exists():\n",
    "        eval_root = Path(path)\n",
    "        # 检查RGB子目录\n",
    "        rgb_dir = eval_root / 'RGB'\n",
    "        if rgb_dir.exists():\n",
    "            img_count = len(list(rgb_dir.glob('*.tif')))\n",
    "            print(f\"✅ 找到evaluation数据集 Found evaluation dataset at: {eval_root} ({img_count} images)\")\n",
    "            break\n",
    "\n",
    "if eval_root is None:\n",
    "    print(\"❌ 未找到evaluation数据集，请确保已下载并解压evaluation.zip\")\n",
    "\n",
    "# 只有当两个数据集都找到时才进行转换 Only proceed if both datasets are found\n",
    "if annotations_root is not None and eval_root is not None:\n",
    "    # 初始化转换器 Initialize converter\n",
    "    converter = NeonTreeToYOLO(annotations_root, eval_root, 'data/processed')\n",
    "    \n",
    "    # 处理标注数据 Process annotation data\n",
    "    processed_samples = converter.process_annotations()\n",
    "    \n",
    "    if processed_samples > 0:\n",
    "        print(f\"✅ 数据转换完成 Data conversion completed: {processed_samples} 个样本 samples\")\n",
    "        \n",
    "        # 显示数据集统计 Show dataset statistics\n",
    "        train_images = len(list(Path('data/processed/images/train').glob('*.jpg')))\n",
    "        val_images = len(list(Path('data/processed/images/val').glob('*.jpg')))\n",
    "        test_images = len(list(Path('data/processed/images/test').glob('*.jpg')))\n",
    "        \n",
    "        print(f\"📊 数据集统计 Dataset Statistics:\")\n",
    "        print(f\"   训练集 Training: {train_images} 张图像\")\n",
    "        print(f\"   验证集 Validation: {val_images} 张图像\")\n",
    "        print(f\"   测试集 Testing: {test_images} 张图像\")\n",
    "        print(f\"   总计 Total: {train_images + val_images + test_images} 张图像\")\n",
    "    else:\n",
    "        print(\"❌ 未能处理任何样本 No samples were processed\")\n",
    "        print(\"请检查XML文件和图像文件的匹配关系\")\n",
    "else:\n",
    "    print(\"❌ 缺少必要的数据集，无法进行转换\")\n",
    "    print(\"请确保以下目录存在：\")\n",
    "    print(\"  - annotations 目录（包含XML文件）\")\n",
    "    print(\"  - evaluation/RGB 目录（包含.tif图像文件）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 创建YOLO配置文件 - Create YOLO Configuration Files (修复路径问题)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "def create_dataset_yaml():\n",
    "    \"\"\"\n",
    "    创建YOLO数据集配置文件\n",
    "    Create YOLO dataset configuration file\n",
    "    \"\"\"\n",
    "    # 获取当前工作目录 Get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # 构建绝对路径 Build absolute paths\n",
    "    processed_dir = os.path.join(current_dir, 'data/processed')\n",
    "    train_path = os.path.join(processed_dir, 'images/train')\n",
    "    val_path = os.path.join(processed_dir, 'images/val')\n",
    "    test_path = os.path.join(processed_dir, 'images/test')\n",
    "    \n",
    "    # 检查路径是否存在 Check if paths exist\n",
    "    paths_info = {\n",
    "        'train': train_path,\n",
    "        'val': val_path,\n",
    "        'test': test_path\n",
    "    }\n",
    "    \n",
    "    print(\"📋 检查数据集路径 Checking dataset paths:\")\n",
    "    existing_paths = {}\n",
    "    \n",
    "    for split, path in paths_info.items():\n",
    "        if os.path.exists(path):\n",
    "            img_count = len([f for f in os.listdir(path) if f.endswith('.jpg')])\n",
    "            print(f\"✅ {split}: {path} ({img_count} 张图像 images)\")\n",
    "            existing_paths[split] = path\n",
    "        else:\n",
    "            print(f\"❌ {split}: {path} (路径不存在 path does not exist)\")\n",
    "    \n",
    "    # 确保至少有训练集存在 Ensure at least training set exists\n",
    "    if 'train' not in existing_paths:\n",
    "        print(\"❌ 错误：未找到训练集 Error: Training set not found\")\n",
    "        return None\n",
    "    \n",
    "    # 创建数据集配置 Create dataset configuration\n",
    "    dataset_config = {\n",
    "        'path': processed_dir,  # 数据集根目录 dataset root dir\n",
    "        'train': 'images/train',  # 相对于path的训练图像路径 train images relative to path\n",
    "        'val': 'images/val' if 'val' in existing_paths else 'images/train',  # 验证集，如果不存在则使用训练集\n",
    "        'test': 'images/test' if 'test' in existing_paths else 'images/train',  # 测试集，如果不存在则使用训练集\n",
    "        'nc': 1,                  # 类别数量 number of classes\n",
    "        'names': ['tree']         # 类别名称 class names\n",
    "    }\n",
    "    \n",
    "    # 特别处理验证集路径\n",
    "    if 'val' not in existing_paths:\n",
    "        print(\"⚠️  验证集不存在，使用训练集作为验证集 Validation set not found, using training set as validation\")\n",
    "        dataset_config['val'] = 'images/train'\n",
    "    \n",
    "    if 'test' not in existing_paths:\n",
    "        print(\"⚠️  测试集不存在，使用训练集作为测试集 Test set not found, using training set as test\")\n",
    "        dataset_config['test'] = 'images/train'\n",
    "    \n",
    "    # 确保配置目录存在\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    # 保存配置文件 Save configuration file\n",
    "    config_path = 'data/tree_dataset.yaml'\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"✅ YOLO数据集配置文件创建完成 YOLO dataset configuration file created: {config_path}\")\n",
    "    \n",
    "    # 验证配置文件内容\n",
    "    print(\"\\n🔍 验证配置文件内容 Verifying configuration file:\")\n",
    "    for key, value in dataset_config.items():\n",
    "        if key in ['train', 'val', 'test']:\n",
    "            full_path = os.path.join(processed_dir, value)\n",
    "            exists = os.path.exists(full_path)\n",
    "            print(f\"   {key}: {value} -> {full_path} ({'✅' if exists else '❌'})\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    return config_path\n",
    "\n",
    "def fix_data_structure():\n",
    "    \"\"\"\n",
    "    修复数据结构问题\n",
    "    Fix data structure issues\n",
    "    \"\"\"\n",
    "    print(\"\\n🔧 检查并修复数据结构 Checking and fixing data structure...\")\n",
    "    \n",
    "    processed_dir = Path('data/processed')\n",
    "    \n",
    "    if not processed_dir.exists():\n",
    "        print(\"❌ data/processed 目录不存在 data/processed directory does not exist\")\n",
    "        return False\n",
    "    \n",
    "    # 检查必要的目录结构\n",
    "    required_dirs = [\n",
    "        'images/train', 'images/val', 'images/test',\n",
    "        'labels/train', 'labels/val', 'labels/test'\n",
    "    ]\n",
    "    \n",
    "    missing_dirs = []\n",
    "    for dir_path in required_dirs:\n",
    "        full_path = processed_dir / dir_path\n",
    "        if not full_path.exists():\n",
    "            missing_dirs.append(dir_path)\n",
    "    \n",
    "    if missing_dirs:\n",
    "        print(f\"⚠️  缺少目录 Missing directories: {missing_dirs}\")\n",
    "        \n",
    "        # 如果只是验证集和测试集缺失，创建它们\n",
    "        for missing_dir in missing_dirs:\n",
    "            if 'val' in missing_dir or 'test' in missing_dir:\n",
    "                (processed_dir / missing_dir).mkdir(parents=True, exist_ok=True)\n",
    "                print(f\"✅ 创建目录 Created directory: {missing_dir}\")\n",
    "    \n",
    "    # 检查训练集是否有数据\n",
    "    train_images = list((processed_dir / 'images/train').glob('*.jpg'))\n",
    "    train_labels = list((processed_dir / 'labels/train').glob('*.txt'))\n",
    "    \n",
    "    print(f\"📊 训练集统计 Training set statistics:\")\n",
    "    print(f\"   图像文件 Image files: {len(train_images)}\")\n",
    "    print(f\"   标签文件 Label files: {len(train_labels)}\")\n",
    "    \n",
    "    if len(train_images) == 0:\n",
    "        print(\"❌ 训练集为空，请先运行数据转换 Training set is empty, please run data conversion first\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# 修复数据结构\n",
    "data_structure_ok = fix_data_structure()\n",
    "\n",
    "if data_structure_ok:\n",
    "    # 创建配置文件 Create configuration file\n",
    "    dataset_yaml_path = create_dataset_yaml()\n",
    "    \n",
    "    if dataset_yaml_path:\n",
    "        # 显示配置文件内容 Display configuration file content\n",
    "        with open(dataset_yaml_path, 'r') as f:\n",
    "            config_content = f.read()\n",
    "            print(\"\\n📋 数据集配置内容 Dataset Configuration Content:\")\n",
    "            print(\"=\" * 40)\n",
    "            print(config_content)\n",
    "            print(\"=\" * 40)\n",
    "        \n",
    "        # 最终验证\n",
    "        print(\"\\n✅ 配置文件创建并验证完成 Configuration file created and verified\")\n",
    "    else:\n",
    "        print(\"❌ 配置文件创建失败 Failed to create configuration file\")\n",
    "else:\n",
    "    print(\"❌ 数据结构修复失败，请检查数据转换步骤 Data structure fix failed, please check data conversion step\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 8. YOLOv5专业树木检测训练 - Professional YOLOv5 Tree Detection Training (修复参数)\n\nimport subprocess\nimport sys\nimport yaml\nimport os\nimport torch\nimport gc\nfrom pathlib import Path\n\nprint(\"🌲 YOLOv5专业树木检测训练\")\nprint(\"💡 基于2024年最新YOLOv5最佳实践\")\nprint(\"🚀 完全放弃best.pt，使用官方预训练模型从零开始\")\nprint(\"🔧 修复训练命令参数格式\")\nprint(\"=\" * 70)\n\ndef memory_cleanup():\n    \"\"\"内存清理\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\ndef create_professional_dataset_config():\n    \"\"\"创建专业的数据集配置\"\"\"\n    print(\"📝 创建专业YOLOv5数据集配置...\")\n    \n    # 检查数据集路径\n    dataset_path = os.path.join(os.getcwd(), 'data/processed')\n    if not os.path.exists(dataset_path):\n        print(f\"❌ 数据集路径不存在: {dataset_path}\")\n        return None\n    \n    # 专业配置 - 遵循YOLOv5官方标准\n    professional_config = {\n        'path': dataset_path,\n        'train': 'images/train',\n        'val': 'images/train',  # 使用训练集作为验证集（小数据集最佳实践）\n        'test': 'images/test',\n        'nc': 1,\n        'names': ['tree']\n    }\n    \n    # 检查训练数据\n    train_path = os.path.join(dataset_path, 'images/train')\n    \n    if os.path.exists(train_path):\n        train_count = len([f for f in os.listdir(train_path) if f.endswith('.jpg')])\n        print(f\"   训练集图像: {train_count}\")\n        \n        if train_count == 0:\n            print(\"❌ 训练数据为空\")\n            return None\n    else:\n        print(\"❌ 训练数据目录不存在\")\n        return None\n    \n    # 保存专业配置\n    config_path = 'tree_detection_professional.yaml'\n    with open(config_path, 'w') as f:\n        yaml.dump(professional_config, f, default_flow_style=False)\n    \n    print(f\"✅ 专业配置文件: {config_path}\")\n    return config_path\n\n# 执行内存清理\nmemory_cleanup()\n\n# 确保YOLOv5环境\nif not os.path.exists('yolov5'):\n    print(\"📦 克隆官方YOLOv5仓库...\")\n    !git clone https://github.com/ultralytics/yolov5.git\n    print(\"✅ YOLOv5仓库克隆完成\")\n\n# 安装/更新依赖\nprint(\"📦 检查YOLOv5依赖...\")\ntry:\n    os.chdir('yolov5')\n    !pip install -r requirements.txt --quiet\n    os.chdir('..')\n    print(\"✅ YOLOv5依赖检查完成\")\nexcept Exception as e:\n    print(f\"⚠️ 依赖安装警告: {e}\")\n\n# 创建专业数据集配置\ndataset_yaml = create_professional_dataset_config()\n\nif dataset_yaml is None:\n    print(\"❌ 无法创建数据集配置，训练终止\")\nelse:\n    print(\"\\n🚀 开始YOLOv5专业训练...\")\n    \n    # 根据硬件配置设置训练参数\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        device_name = torch.cuda.get_device_name(0)\n        print(f\"🎮 GPU: {device_name} ({gpu_memory:.1f} GB)\")\n        \n        # 根据GPU内存优化参数\n        if gpu_memory >= 20:  # A100, RTX 4090等\n            model_size = 'yolov5l'\n            batch_size = 24\n            img_size = 640\n            epochs = 200\n            workers = 6\n        elif gpu_memory >= 15:  # L4, RTX 3090等\n            model_size = 'yolov5m'\n            batch_size = 20\n            img_size = 640\n            epochs = 150\n            workers = 4\n        elif gpu_memory >= 8:   # RTX 3070, V100等\n            model_size = 'yolov5s'\n            batch_size = 16\n            img_size = 640\n            epochs = 120\n            workers = 2\n        else:                   # GTX 1080, RTX 2070等\n            model_size = 'yolov5s'\n            batch_size = 8\n            img_size = 512\n            epochs = 100\n            workers = 2\n            \n        device_param = '0'\n    else:\n        print(\"💻 使用CPU训练 (不推荐)\")\n        model_size = 'yolov5s'\n        batch_size = 4\n        img_size = 512\n        epochs = 50\n        workers = 1\n        device_param = 'cpu'\n    \n    print(f\"\\n⚙️  专业训练配置:\")\n    print(f\"   模型: {model_size}.pt (官方预训练)\")\n    print(f\"   图像尺寸: {img_size}\")\n    print(f\"   批次大小: {batch_size}\")\n    print(f\"   训练轮数: {epochs}\")\n    print(f\"   数据加载线程: {workers}\")\n    print(f\"   设备: {device_param}\")\n    \n    # 构建修复的专业训练命令\n    professional_command = [\n        sys.executable, 'yolov5/train.py',\n        '--data', dataset_yaml,\n        '--weights', f'{model_size}.pt',        # 官方预训练权重\n        '--epochs', str(epochs),\n        '--batch-size', str(batch_size),\n        '--imgsz', str(img_size),\n        '--device', device_param,\n        '--workers', str(workers),\n        '--project', 'runs/train',\n        '--name', 'tree_detection_professional',\n        '--optimizer', 'SGD',                   # 推荐的优化器\n        '--patience', '30',                     # 早停耐心值\n        '--save-period', '25',                  # 定期保存\n        '--cache',                              # 缓存图像\n        '--exist-ok'\n    ]\n    \n    # 高端GPU的额外优化\n    if torch.cuda.is_available() and gpu_memory >= 8:\n        professional_command.append('--multi-scale')  # 多尺度训练\n        if gpu_memory >= 15:\n            professional_command.append('--amp')       # 混合精度训练\n    \n    print(f\"\\n🔥 修复后的训练命令:\")\n    cmd_str = ' '.join(professional_command)\n    print(f\"   {cmd_str}\")\n    \n    print(f\"\\n🚀 开始专业训练...\")\n    print(f\"💡 策略: 官方预训练模型 + 简化稳定参数\")\n    print(f\"🎯 目标: 高质量树木检测模型\")\n    print(\"=\" * 70)\n    \n    # 执行专业训练\n    try:\n        memory_cleanup()  # 训练前清理\n        \n        print(\"⏳ 训练进行中，请耐心等待...\")\n        result = subprocess.run(\n            professional_command,\n            capture_output=True,\n            text=True,\n            timeout=7200  # 2小时超时\n        )\n        \n        if result.returncode == 0:\n            print(\"🎉 专业训练成功完成!\")\n            \n            # 检查训练结果\n            model_dir = 'runs/train/tree_detection_professional/weights'\n            if os.path.exists(model_dir):\n                best_model = os.path.join(model_dir, 'best.pt')\n                last_model = os.path.join(model_dir, 'last.pt')\n                \n                if os.path.exists(best_model):\n                    model_size_mb = os.path.getsize(best_model) / (1024 * 1024)\n                    print(f\"✅ 最佳模型: {best_model} ({model_size_mb:.1f}MB)\")\n                    final_model_path = best_model\n                elif os.path.exists(last_model):\n                    model_size_mb = os.path.getsize(last_model) / (1024 * 1024)\n                    print(f\"✅ 最新模型: {last_model} ({model_size_mb:.1f}MB)\")\n                    final_model_path = last_model\n                else:\n                    final_model_path = None\n                    print(\"⚠️ 未找到训练模型\")\n                \n                # 显示训练总结\n                print(\"\\n📊 训练输出总结:\")\n                output_lines = result.stdout.split('\\n')\n                key_lines = []\n                for line in output_lines:\n                    if any(keyword in line.lower() for keyword in \n                          ['results', 'best', 'map', 'precision', 'recall', 'fitness', 'epoch']):\n                        key_lines.append(line)\n                \n                # 显示最后15行关键信息\n                for line in key_lines[-15:]:\n                    if line.strip():\n                        print(f\"   {line}\")\n                        \n            else:\n                print(\"❌ 训练结果目录不存在\")\n                final_model_path = None\n                \n        else:\n            print(f\"❌ 训练失败，返回码: {result.returncode}\")\n            print(\"\\n错误输出:\")\n            if result.stderr:\n                print(result.stderr[:1500])\n            \n            print(\"\\n标准输出片段:\")\n            if result.stdout:\n                stdout_lines = result.stdout.split('\\n')\n                for line in stdout_lines[-20:]:\n                    if line.strip():\n                        print(f\"   {line}\")\n            \n            final_model_path = None\n            \n    except subprocess.TimeoutExpired:\n        print(\"⏰ 训练超时，检查部分结果...\")\n        model_dir = 'runs/train/tree_detection_professional/weights'\n        if os.path.exists(os.path.join(model_dir, 'last.pt')):\n            final_model_path = os.path.join(model_dir, 'last.pt')\n            print(f\"✅ 找到部分训练模型: {final_model_path}\")\n        else:\n            final_model_path = None\n            \n    except Exception as e:\n        print(f\"❌ 训练异常: {e}\")\n        final_model_path = None\n    \n    # 最终清理和总结\n    memory_cleanup()\n    \n    print(\"\\n🎯 YOLOv5专业训练完成\")\n    if final_model_path:\n        print(f\"✅ 专业训练模型: {final_model_path}\")\n        print(\"🔥 基于官方预训练模型和稳定参数\")\n        print(\"📈 使用简化配置确保兼容性\")\n        print(\"🚀 运行Cell 9进行模型推理测试\")\n    else:\n        print(\"❌ 训练未成功完成\")\n        print(\"🔧 可能的解决方案:\")\n        print(\"   1. 检查数据集格式和路径\")\n        print(\"   2. 确认GPU内存充足\")\n        print(\"   3. 检查YOLOv5环境安装\")\n        print(\"   4. 尝试降低batch_size参数\")\n    \n    print(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 专业模型推理测试 - Professional Model Inference Testing\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def comprehensive_cleanup():\n",
    "    \"\"\"综合内存清理\"\"\"\n",
    "    try:\n",
    "        print(\"🧹 执行内存清理...\")\n",
    "        \n",
    "        # Python垃圾回收\n",
    "        collected = gc.collect()\n",
    "        print(f\"   垃圾回收释放对象: {collected}\")\n",
    "        \n",
    "        # CUDA缓存清理\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "            print(f\"   CUDA内存: 已分配 {allocated:.1f}GB, 已保留 {reserved:.1f}GB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 清理过程警告: {e}\")\n",
    "        return False\n",
    "\n",
    "def professional_inference(model_path, test_images, confidence_threshold=0.25):\n",
    "    \"\"\"专业模型推理\"\"\"\n",
    "    print(f\"🔍 专业模型推理测试: {model_path}\")\n",
    "    print(f\"📊 测试图像数量: {len(test_images)}\")\n",
    "    print(f\"🎯 置信度阈值: {confidence_threshold}\")\n",
    "    \n",
    "    # 创建输出目录\n",
    "    output_dir = 'professional_inference_results'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # 构建推理命令\n",
    "    inference_command = [\n",
    "        sys.executable, 'yolov5/detect.py',\n",
    "        '--weights', model_path,\n",
    "        '--source', str(test_images[0].parent),  # 使用图像目录\n",
    "        '--project', output_dir,\n",
    "        '--name', 'detect',\n",
    "        '--img', '640',\n",
    "        '--conf', str(confidence_threshold),\n",
    "        '--iou', '0.45',\n",
    "        '--max-det', '1000',\n",
    "        '--save-txt',\n",
    "        '--save-conf',\n",
    "        '--exist-ok'\n",
    "    ]\n",
    "    \n",
    "    # 添加设备参数\n",
    "    if torch.cuda.is_available():\n",
    "        inference_command.extend(['--device', '0'])\n",
    "    else:\n",
    "        inference_command.extend(['--device', 'cpu'])\n",
    "    \n",
    "    print(f\"\\n🚀 执行推理命令:\")\n",
    "    print(f\"   {' '.join(inference_command)}\")\n",
    "    \n",
    "    # 执行推理\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            inference_command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300  # 5分钟超时\n",
    "        )\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ 推理成功完成 (耗时: {inference_time:.1f}s)\")\n",
    "            \n",
    "            # 分析推理结果\n",
    "            results = analyze_inference_results(output_dir)\n",
    "            return results\n",
    "            \n",
    "        else:\n",
    "            print(f\"❌ 推理失败: {result.stderr[:500]}\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"⏰ 推理超时\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 推理异常: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_inference_results(output_dir):\n",
    "    \"\"\"分析推理结果\"\"\"\n",
    "    print(\"\\n📊 分析推理结果...\")\n",
    "    \n",
    "    results_dir = Path(output_dir) / 'detect'\n",
    "    if not results_dir.exists():\n",
    "        print(\"❌ 推理结果目录不存在\")\n",
    "        return None\n",
    "    \n",
    "    # 统计检测结果\n",
    "    total_detections = 0\n",
    "    confidence_scores = []\n",
    "    image_counts = {}\n",
    "    \n",
    "    # 分析标签文件\n",
    "    label_dir = results_dir / 'labels'\n",
    "    if label_dir.exists():\n",
    "        label_files = list(label_dir.glob('*.txt'))\n",
    "        print(f\"   处理标签文件: {len(label_files)}\")\n",
    "        \n",
    "        for label_file in label_files:\n",
    "            image_name = label_file.stem\n",
    "            detections_in_image = 0\n",
    "            \n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 6:\n",
    "                            conf = float(parts[5])\n",
    "                            confidence_scores.append(conf)\n",
    "                            total_detections += 1\n",
    "                            detections_in_image += 1\n",
    "            \n",
    "            image_counts[image_name] = detections_in_image\n",
    "    \n",
    "    # 检查输出图像\n",
    "    output_images = list(results_dir.glob('*.jpg')) + list(results_dir.glob('*.png'))\n",
    "    \n",
    "    # 计算统计信息\n",
    "    if confidence_scores:\n",
    "        avg_confidence = np.mean(confidence_scores)\n",
    "        max_confidence = np.max(confidence_scores)\n",
    "        min_confidence = np.min(confidence_scores)\n",
    "        std_confidence = np.std(confidence_scores)\n",
    "        \n",
    "        # 置信度分布统计\n",
    "        high_conf_count = sum(1 for c in confidence_scores if c > 0.7)\n",
    "        medium_conf_count = sum(1 for c in confidence_scores if 0.4 <= c <= 0.7)\n",
    "        low_conf_count = sum(1 for c in confidence_scores if c < 0.4)\n",
    "        \n",
    "        print(f\"\\n📈 检测统计:\")\n",
    "        print(f\"   总检测数量: {total_detections}\")\n",
    "        print(f\"   平均置信度: {avg_confidence:.3f}\")\n",
    "        print(f\"   置信度范围: {min_confidence:.3f} - {max_confidence:.3f}\")\n",
    "        print(f\"   置信度标准差: {std_confidence:.3f}\")\n",
    "        print(f\"   高置信度检测(>0.7): {high_conf_count}\")\n",
    "        print(f\"   中等置信度检测(0.4-0.7): {medium_conf_count}\")\n",
    "        print(f\"   低置信度检测(<0.4): {low_conf_count}\")\n",
    "        \n",
    "        # 每张图像检测统计\n",
    "        if image_counts:\n",
    "            avg_detections_per_image = np.mean(list(image_counts.values()))\n",
    "            max_detections = max(image_counts.values())\n",
    "            print(f\"\\n🖼️ 图像检测统计:\")\n",
    "            print(f\"   处理图像数量: {len(image_counts)}\")\n",
    "            print(f\"   平均每图检测数: {avg_detections_per_image:.1f}\")\n",
    "            print(f\"   最大单图检测数: {max_detections}\")\n",
    "            \n",
    "            # 显示检测最多的图像\n",
    "            top_images = sorted(image_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"   检测数最多的图像:\")\n",
    "            for img_name, count in top_images:\n",
    "                print(f\"     {img_name}: {count} 个检测\")\n",
    "        \n",
    "        # 输出图像信息\n",
    "        if output_images:\n",
    "            print(f\"\\n📁 输出文件:\")\n",
    "            print(f\"   检测结果图像: {len(output_images)}\")\n",
    "            for img_path in output_images[:5]:  # 显示前5个\n",
    "                print(f\"     {img_path.name}\")\n",
    "            if len(output_images) > 5:\n",
    "                print(f\"     ... 还有 {len(output_images)-5} 个文件\")\n",
    "        \n",
    "        return {\n",
    "            'total_detections': total_detections,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'max_confidence': max_confidence,\n",
    "            'min_confidence': min_confidence,\n",
    "            'std_confidence': std_confidence,\n",
    "            'high_conf_count': high_conf_count,\n",
    "            'medium_conf_count': medium_conf_count,\n",
    "            'low_conf_count': low_conf_count,\n",
    "            'confidence_scores': confidence_scores,\n",
    "            'image_counts': image_counts,\n",
    "            'output_images': output_images,\n",
    "            'results_dir': str(results_dir)\n",
    "        }\n",
    "    else:\n",
    "        print(\"⚠️ 未检测到任何对象\")\n",
    "        return {\n",
    "            'total_detections': 0,\n",
    "            'avg_confidence': 0,\n",
    "            'results_dir': str(results_dir)\n",
    "        }\n",
    "\n",
    "def visualize_results(results):\n",
    "    \"\"\"可视化推理结果\"\"\"\n",
    "    if not results or results['total_detections'] == 0:\n",
    "        print(\"📊 无检测结果可视化\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n📊 创建结果可视化图表...\")\n",
    "    \n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 置信度分布直方图\n",
    "        confidences = results['confidence_scores']\n",
    "        axes[0].hist(confidences, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0].set_title('🎯 检测置信度分布', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('置信度')\n",
    "        axes[0].set_ylabel('频次')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 添加统计线\n",
    "        avg_conf = results['avg_confidence']\n",
    "        axes[0].axvline(avg_conf, color='red', linestyle='--', linewidth=2,\n",
    "                       label=f'平均置信度: {avg_conf:.3f}')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # 置信度等级饼图\n",
    "        high_count = results['high_conf_count']\n",
    "        medium_count = results['medium_conf_count'] \n",
    "        low_count = results['low_conf_count']\n",
    "        \n",
    "        labels = ['高置信度(>0.7)', '中等置信度(0.4-0.7)', '低置信度(<0.4)']\n",
    "        sizes = [high_count, medium_count, low_count]\n",
    "        colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "        \n",
    "        # 过滤掉为0的项\n",
    "        filtered_data = [(label, size, color) for label, size, color in zip(labels, sizes, colors) if size > 0]\n",
    "        if filtered_data:\n",
    "            labels, sizes, colors = zip(*filtered_data)\n",
    "            axes[1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "            axes[1].set_title('🎯 置信度等级分布', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, '无检测数据', ha='center', va='center', \n",
    "                        transform=axes[1].transAxes, fontsize=16)\n",
    "            axes[1].set_title('🎯 置信度等级分布', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✅ 可视化图表创建完成\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 可视化创建失败: {e}\")\n",
    "\n",
    "def show_sample_results(results):\n",
    "    \"\"\"显示示例检测结果\"\"\"\n",
    "    if not results or not results.get('output_images'):\n",
    "        print(\"📷 无示例图像可显示\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n📷 显示检测结果示例...\")\n",
    "    \n",
    "    try:\n",
    "        output_images = results['output_images']\n",
    "        sample_count = min(3, len(output_images))\n",
    "        \n",
    "        if sample_count > 0:\n",
    "            fig, axes = plt.subplots(1, sample_count, figsize=(5*sample_count, 5))\n",
    "            if sample_count == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i in range(sample_count):\n",
    "                img_path = output_images[i]\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is not None:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    axes[i].imshow(img_rgb)\n",
    "                    axes[i].set_title(f'检测结果: {img_path.name}', fontsize=12)\n",
    "                    axes[i].axis('off')\n",
    "                else:\n",
    "                    axes[i].text(0.5, 0.5, '图像加载失败', ha='center', va='center',\n",
    "                               transform=axes[i].transAxes)\n",
    "                    axes[i].set_title(f'错误: {img_path.name}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            print(f\"✅ 显示了 {sample_count} 个检测结果示例\")\n",
    "        else:\n",
    "            print(\"📷 没有可显示的检测结果图像\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 示例显示失败: {e}\")\n",
    "\n",
    "# 主执行流程\n",
    "print(\"🚀 启动专业模型推理测试...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 清理内存\n",
    "comprehensive_cleanup()\n",
    "\n",
    "# 查找训练好的模型\n",
    "model_candidates = [\n",
    "    'runs/train/tree_detection_professional/weights/best.pt',\n",
    "    'runs/train/tree_detection_professional/weights/last.pt',\n",
    "    'runs/train/tree_detection_modern/weights/best.pt',\n",
    "    'runs/train/tree_detection_basic/weights/best.pt'\n",
    "]\n",
    "\n",
    "available_model = None\n",
    "for model_path in model_candidates:\n",
    "    if os.path.exists(model_path):\n",
    "        model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "        print(f\"🎯 找到训练模型: {model_path} ({model_size:.1f}MB)\")\n",
    "        available_model = model_path\n",
    "        break\n",
    "\n",
    "if not available_model:\n",
    "    print(\"❌ 未找到训练好的模型\")\n",
    "    print(\"💡 请先运行Cell 8完成模型训练\")\n",
    "else:\n",
    "    # 查找测试图像\n",
    "    test_dirs = [\n",
    "        'data/processed/images/test',\n",
    "        'data/processed/images/val', \n",
    "        'data/processed/images/train'\n",
    "    ]\n",
    "    \n",
    "    test_images = []\n",
    "    for test_dir in test_dirs:\n",
    "        if os.path.exists(test_dir):\n",
    "            images = list(Path(test_dir).glob('*.jpg'))\n",
    "            test_images.extend(images)\n",
    "            if len(test_images) >= 10:  # 限制测试图像数量\n",
    "                test_images = test_images[:10]\n",
    "                break\n",
    "    \n",
    "    if not test_images:\n",
    "        print(\"❌ 未找到测试图像\")\n",
    "    else:\n",
    "        print(f\"📸 找到测试图像: {len(test_images)} 张\")\n",
    "        \n",
    "        # 执行专业推理测试\n",
    "        results = professional_inference(available_model, test_images)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"\\n🎉 推理测试完成!\")\n",
    "            \n",
    "            # 可视化结果\n",
    "            visualize_results(results)\n",
    "            \n",
    "            # 显示示例结果\n",
    "            show_sample_results(results)\n",
    "            \n",
    "            # 总结报告\n",
    "            print(f\"\\n📋 推理测试总结报告:\")\n",
    "            print(f\"   模型: {available_model}\")\n",
    "            print(f\"   测试图像: {len(test_images)}\")\n",
    "            print(f\"   总检测数: {results['total_detections']}\")\n",
    "            if results['total_detections'] > 0:\n",
    "                print(f\"   平均置信度: {results['avg_confidence']:.3f}\")\n",
    "                print(f\"   高质量检测(>0.7): {results['high_conf_count']}\")\n",
    "            print(f\"   结果目录: {results['results_dir']}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"❌ 推理测试失败\")\n",
    "\n",
    "# 最终清理\n",
    "comprehensive_cleanup()\n",
    "print(\"✅ 专业模型推理测试完成\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. YOLOv5模型评估和性能分析 - YOLOv5 Model Evaluation and Performance Analysis\n",
    "def evaluate_yolov5_performance(model_path, test_images):\n",
    "    \"\"\"\n",
    "    评估YOLOv5模型性能\n",
    "    Evaluate YOLOv5 model performance\n",
    "    \"\"\"\n",
    "    print(\"📊 开始YOLOv5模型性能评估 Starting YOLOv5 model performance evaluation...\")\n",
    "    \n",
    "    total_images = len(test_images)\n",
    "    total_detections = 0\n",
    "    confidence_scores = []\n",
    "    processing_times = []\n",
    "    \n",
    "    # 创建临时目录存储评估结果 Create temporary directory for evaluation results\n",
    "    eval_dir = 'temp_eval'\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"🔍 评估 {total_images} 张图像 Evaluating {total_images} images...\")\n",
    "    \n",
    "    # 对每张图像进行检测 Detect on each image\n",
    "    for i, img_path in enumerate(test_images):\n",
    "        try:\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # 构建检测命令 Build detection command\n",
    "            detect_command = [\n",
    "                sys.executable, 'yolov5/detect.py',\n",
    "                '--weights', model_path,\n",
    "                '--source', str(img_path),\n",
    "                '--project', eval_dir,\n",
    "                '--name', f'eval_{i}',\n",
    "                '--save-txt',  # 保存检测结果 save detection results\n",
    "                '--save-conf', # 保存置信度 save confidence scores\n",
    "                '--exist-ok',\n",
    "                '--nosave'     # 不保存图像，只要文本结果 don't save images, only text results\n",
    "            ]\n",
    "            \n",
    "            # 运行检测 Run detection\n",
    "            result = subprocess.run(detect_command, \n",
    "                                  capture_output=True, \n",
    "                                  text=True, \n",
    "                                  cwd='.')\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            processing_times.append(processing_time)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                # 读取检测结果 Read detection results\n",
    "                result_dir = Path(eval_dir) / f'eval_{i}' / 'labels'\n",
    "                if result_dir.exists():\n",
    "                    for label_file in result_dir.glob('*.txt'):\n",
    "                        with open(label_file, 'r') as f:\n",
    "                            lines = f.readlines()\n",
    "                            for line in lines:\n",
    "                                if line.strip():\n",
    "                                    parts = line.strip().split()\n",
    "                                    if len(parts) >= 6:  # class x y w h conf\n",
    "                                        confidence = float(parts[5])\n",
    "                                        confidence_scores.append(confidence)\n",
    "                                        total_detections += 1\n",
    "                                        \n",
    "            if (i + 1) % 10 == 0 or i == total_images - 1:\n",
    "                print(f\"   已处理 {i+1}/{total_images} 张图像 Processed {i+1}/{total_images} images\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"评估图像 {i+1} 时出错 Error evaluating image {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 清理临时目录 Clean up temporary directory\n",
    "    import shutil\n",
    "    if os.path.exists(eval_dir):\n",
    "        shutil.rmtree(eval_dir)\n",
    "    \n",
    "    # 计算统计信息 Calculate statistics\n",
    "    avg_detections_per_image = total_detections / total_images if total_images > 0 else 0\n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "    max_confidence = np.max(confidence_scores) if confidence_scores else 0\n",
    "    min_confidence = np.min(confidence_scores) if confidence_scores else 0\n",
    "    avg_processing_time = np.mean(processing_times) if processing_times else 0\n",
    "    \n",
    "    print(\"\\n📈 YOLOv5模型性能统计 YOLOv5 Model Performance Statistics\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"总测试图像数量 Total test images: {total_images}\")\n",
    "    print(f\"总检测数量 Total detections: {total_detections}\")\n",
    "    print(f\"平均每张图像检测数量 Average detections per image: {avg_detections_per_image:.2f}\")\n",
    "    print(f\"平均置信度 Average confidence: {avg_confidence:.3f}\")\n",
    "    print(f\"最高置信度 Maximum confidence: {max_confidence:.3f}\")\n",
    "    print(f\"最低置信度 Minimum confidence: {min_confidence:.3f}\")\n",
    "    print(f\"平均处理时间 Average processing time: {avg_processing_time:.3f} 秒/张 seconds per image\")\n",
    "    \n",
    "    # 创建可视化图表 Create visualization charts\n",
    "    if confidence_scores or processing_times:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # 置信度分布图 Confidence distribution\n",
    "        if confidence_scores:\n",
    "            axes[0].hist(confidence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[0].set_title('🎯 检测置信度分布 Detection Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "            axes[0].set_xlabel('置信度 Confidence Score')\n",
    "            axes[0].set_ylabel('频次 Frequency')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 添加统计线 Add statistical lines\n",
    "            axes[0].axvline(avg_confidence, color='red', linestyle='--', \n",
    "                          label=f'平均值 Mean: {avg_confidence:.3f}')\n",
    "            axes[0].legend()\n",
    "        else:\n",
    "            axes[0].text(0.5, 0.5, '无置信度数据\\nNo Confidence Data', \n",
    "                       ha='center', va='center', transform=axes[0].transAxes)\n",
    "        \n",
    "        # 处理时间分布图 Processing time distribution\n",
    "        if processing_times:\n",
    "            axes[1].hist(processing_times, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "            axes[1].set_title('⏱️ 处理时间分布 Processing Time Distribution', fontsize=12, fontweight='bold')\n",
    "            axes[1].set_xlabel('处理时间 Processing Time (seconds)')\n",
    "            axes[1].set_ylabel('频次 Frequency')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # 添加统计线 Add statistical lines\n",
    "            axes[1].axvline(avg_processing_time, color='red', linestyle='--', \n",
    "                          label=f'平均值 Mean: {avg_processing_time:.3f}s')\n",
    "            axes[1].legend()\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, '无处理时间数据\\nNo Processing Time Data', \n",
    "                       ha='center', va='center', transform=axes[1].transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'total_images': total_images,\n",
    "        'total_detections': total_detections,\n",
    "        'avg_detections_per_image': avg_detections_per_image,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'processing_times': processing_times,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "\n",
    "def run_yolov5_validation(model_path, dataset_yaml):\n",
    "    \"\"\"\n",
    "    运行YOLOv5官方验证脚本\n",
    "    Run YOLOv5 official validation script\n",
    "    \"\"\"\n",
    "    print(\"🔬 运行YOLOv5官方验证 Running YOLOv5 official validation...\")\n",
    "    \n",
    "    try:\n",
    "        # 构建验证命令 Build validation command\n",
    "        val_command = [\n",
    "            sys.executable, 'yolov5/val.py',\n",
    "            '--weights', model_path,\n",
    "            '--data', dataset_yaml,\n",
    "            '--img', '640',\n",
    "            '--batch', '8',\n",
    "            '--conf', '0.001',  # 低置信度阈值以获得更多检测 low confidence threshold for more detections\n",
    "            '--iou', '0.6',     # IoU阈值 IoU threshold\n",
    "            '--task', 'val',\n",
    "            '--device', '0' if torch.cuda.is_available() else 'cpu',\n",
    "            '--save-txt',\n",
    "            '--save-conf',\n",
    "            '--project', 'runs/val',\n",
    "            '--name', 'tree_detection_val',\n",
    "            '--exist-ok'\n",
    "        ]\n",
    "        \n",
    "        print(f\"验证命令 Validation command: {' '.join(val_command)}\")\n",
    "        \n",
    "        # 运行验证 Run validation\n",
    "        result = subprocess.run(val_command, \n",
    "                              capture_output=True, \n",
    "                              text=True, \n",
    "                              cwd='.')\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"✅ YOLOv5官方验证完成 YOLOv5 official validation completed!\")\n",
    "            \n",
    "            # 显示验证输出的关键信息 Show key information from validation output\n",
    "            output_lines = result.stdout.split('\\n')\n",
    "            print(\"\\n📊 验证结果摘要 Validation Results Summary:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for line in output_lines:\n",
    "                if any(keyword in line.lower() for keyword in ['precision', 'recall', 'map', 'f1']):\n",
    "                    print(f\"   {line.strip()}\")\n",
    "            \n",
    "            # 检查是否生成了结果文件 Check if result files were generated\n",
    "            val_results_dir = Path('runs/val/tree_detection_val')\n",
    "            if val_results_dir.exists():\n",
    "                print(f\"\\n📁 验证结果保存在 Validation results saved in: {val_results_dir}\")\n",
    "                \n",
    "                # 列出生成的文件 List generated files\n",
    "                result_files = list(val_results_dir.glob('*'))\n",
    "                for f in result_files:\n",
    "                    if f.is_file():\n",
    "                        print(f\"   {f.name}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"❌ YOLOv5验证失败 YOLOv5 validation failed\")\n",
    "            print(f\"错误输出 Error output: {result.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 验证过程中出现错误 Error during validation: {e}\")\n",
    "\n",
    "# 如果有训练好的模型，进行性能评估 If trained model exists, perform performance evaluation\n",
    "if 'model_path' in locals() and model_path is not None and os.path.exists(model_path):\n",
    "    print(f\"🎯 使用模型进行性能评估 Using model for performance evaluation: {model_path}\")\n",
    "    \n",
    "    # 获取测试图像 Get test images\n",
    "    test_image_dir = Path('data/processed/images/test')\n",
    "    eval_images = []\n",
    "    \n",
    "    if test_image_dir.exists():\n",
    "        eval_images = list(test_image_dir.glob('*.jpg'))\n",
    "    \n",
    "    # 如果没有处理过的测试图像，使用原始评估图像 If no processed test images, use original evaluation images\n",
    "    if not eval_images:\n",
    "        eval_rgb_dir = Path('data/raw/evaluation/RGB')\n",
    "        if eval_rgb_dir.exists():\n",
    "            eval_images = list(eval_rgb_dir.glob('*.tif'))[:20]  # 限制为20张图像\n",
    "            print(f\"📸 使用原始评估图像 Using original evaluation images: {len(eval_images)}\")\n",
    "    \n",
    "    if eval_images:\n",
    "        # 进行性能评估 Perform performance evaluation\n",
    "        performance_stats = evaluate_yolov5_performance(model_path, eval_images)\n",
    "        \n",
    "        # 运行官方验证（如果数据集配置存在）Run official validation if dataset config exists\n",
    "        if 'dataset_yaml_path' in locals() and os.path.exists(dataset_yaml_path):\n",
    "            run_yolov5_validation(model_path, dataset_yaml_path)\n",
    "        \n",
    "    else:\n",
    "        print(\"⚠️  未找到测试图像，跳过性能评估 No test images found, skipping performance evaluation\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  未找到训练好的模型，跳过性能评估 No trained model found, skipping performance evaluation\")\n",
    "    print(\"请先完成模型训练 Please complete model training first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📝 项目总结 Project Summary\n",
    "\n",
    "### 完成的功能 Completed Features\n",
    "1. ✅ **环境检测** - 自动检测GPU和Colab环境\n",
    "2. ✅ **数据下载** - 自动下载NeonTreeEvaluation数据集\n",
    "3. ✅ **数据转换** - 将NeonTree格式转换为YOLO格式\n",
    "4. ✅ **模型训练** - 使用YOLOv5进行树木检测训练\n",
    "5. ✅ **结果推理** - 对测试图像进行树木检测\n",
    "6. ✅ **结果可视化** - 显示检测结果和性能统计\n",
    "\n",
    "### 主要参数说明 Key Parameter Explanations\n",
    "- **训练轮数 Epochs**: 50轮（可根据效果调整）\n",
    "- **批次大小 Batch Size**: 16（根据GPU内存调整）\n",
    "- **图像尺寸 Image Size**: 640x640像素\n",
    "- **学习率 Learning Rate**: 0.001（AdamW优化器）\n",
    "- **数据分割 Data Split**: 70%训练/20%验证/10%测试\n",
    "\n",
    "### 使用说明 Usage Instructions\n",
    "1. 在Google Colab中运行所有代码单元\n",
    "2. 确保GPU环境可用以加速训练\n",
    "3. 根据需要调整训练参数\n",
    "4. 查看results目录中的检测结果图像\n",
    "\n",
    "### 故障排除 Troubleshooting\n",
    "- 如果内存不足，减小batch_size参数\n",
    "- 如果训练时间过长，减少epochs数量\n",
    "- 如果检测效果不佳，尝试增加训练轮数\n",
    "- 确保数据集正确下载和解压\n",
    "\n",
    "### 进一步改进 Further Improvements\n",
    "- 数据增强技术提升模型泛化能力\n",
    "- 超参数调优优化模型性能\n",
    "- 多尺度训练提高检测精度\n",
    "- 模型集成提升整体效果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}