{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ² æ ‘æœ¨æ£€æµ‹é¡¹ç›® - Tree Detection Project\n",
    "\n",
    "## é¡¹ç›®æ¦‚è¿° Project Overview\n",
    "\n",
    "æœ¬é¡¹ç›®ä½¿ç”¨YOLOv5å’ŒPyTorchå®ç°åŸºäºèˆªç©ºå½±åƒçš„æ ‘æœ¨æ£€æµ‹ä»»åŠ¡  \n",
    "This project implements tree detection from aerial imagery using YOLOv5 and PyTorch\n",
    "\n",
    "**æ•°æ®é›† Dataset**: NeonTreeEvaluation Benchmark  \n",
    "**ç›®æ ‡ Goal**: æ£€æµ‹èˆªç©ºæ­£å°„å½±åƒä¸­çš„æ ‘æœ¨ - Detect trees in aerial orthoimagery  \n",
    "**å¹³å° Platform**: Google Colab (è‡ªåŠ¨GPUæ£€æµ‹ - Auto GPU detection)\n",
    "\n",
    "## æŠ€æœ¯æ ˆ Tech Stack\n",
    "- **æ·±åº¦å­¦ä¹ æ¡†æ¶ Deep Learning**: PyTorch + YOLOv5\n",
    "- **æ•°æ®å¤„ç† Data Processing**: OpenCV, PIL, pandas\n",
    "- **å¯è§†åŒ– Visualization**: matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 1. ç¯å¢ƒæ£€æµ‹å’ŒåŸºç¡€è®¾ç½® - Environment Detection and Basic Setup\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸš€ ç¯å¢ƒæ£€æµ‹ Environment Detection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥Pythonç‰ˆæœ¬ Check Python version\n",
    "print(f\"Pythonç‰ˆæœ¬ Python Version: {sys.version}\")\n",
    "\n",
    "# æ£€æŸ¥æ“ä½œç³»ç»Ÿ Check OS\n",
    "print(f\"æ“ä½œç³»ç»Ÿ Operating System: {platform.system()}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦åœ¨Colabç¯å¢ƒ Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… è¿è¡Œç¯å¢ƒ: Google Colab - Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"âŒ è¿è¡Œç¯å¢ƒ: æœ¬åœ°ç¯å¢ƒ - Running locally\")\n",
    "\n",
    "# æ£€æŸ¥GPUå¯ç”¨æ€§ Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPUå¯ç”¨ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDAç‰ˆæœ¬ CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   GPUæ•°é‡ GPU Count: {torch.cuda.device_count()}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU - GPU not available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"è®¾å¤‡ Device: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. å®‰è£…å¿…è¦ä¾èµ– - Install Required Dependencies\n",
    "print(\"ğŸ“¦ å®‰è£…ä¾èµ–åŒ… Installing Dependencies...\")\n",
    "\n",
    "# å®‰è£…YOLOv5å’Œç›¸å…³ä¾èµ– Install YOLOv5 and dependencies\n",
    "!pip install -q ultralytics\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q Pillow\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q pandas\n",
    "!pip install -q tqdm\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q PyYAML\n",
    "\n",
    "# å¦‚æœåœ¨Colabç¯å¢ƒï¼ŒæŒ‚è½½Google Drive (å¯é€‰)\n",
    "# Mount Google Drive in Colab (optional)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/drive')  # å–æ¶ˆæ³¨é‡Šä»¥æŒ‚è½½Drive - Uncomment to mount Drive\n",
    "\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. å¯¼å…¥æ‰€éœ€åº“ - Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ Set matplotlib Chinese font support\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥å®Œæˆ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. æ•°æ®ä¸‹è½½å’Œè§£å‹ - Data Download and Extraction\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    ä¸‹è½½æ–‡ä»¶çš„å‡½æ•° Function to download files\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ å¼€å§‹ä¸‹è½½ Starting download: {filename}\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filename, 'wb') as file, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    print(f\"âœ… ä¸‹è½½å®Œæˆ Download completed: {filename}\")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®ç›®å½• Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# æ•°æ®é›†URL Dataset URLs\n",
    "DATASET_URL = \"https://zenodo.org/records/5914554/files/evaluation.zip?download=1\"\n",
    "ANNOTATIONS_URL = \"https://zenodo.org/records/5914554/files/annotations.zip?download=1\"\n",
    "\n",
    "# ä¸‹è½½æ•°æ®é›† Download datasets\n",
    "if not os.path.exists('data/raw/evaluation.zip'):\n",
    "    download_file(DATASET_URL, 'data/raw/evaluation.zip')\n",
    "else:\n",
    "    print(\"âœ… è¯„ä¼°æ•°æ®é›†å·²å­˜åœ¨ Evaluation dataset already exists\")\n",
    "\n",
    "if not os.path.exists('data/raw/annotations.zip'):\n",
    "    download_file(ANNOTATIONS_URL, 'data/raw/annotations.zip')\n",
    "else:\n",
    "    print(\"âœ… æ ‡æ³¨æ•°æ®å·²å­˜åœ¨ Annotations already exist\")\n",
    "\n",
    "print(\"ğŸ“ æ•°æ®ä¸‹è½½å®Œæˆ Data download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. è§£å‹æ•°æ®é›† - Extract Datasets\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    è§£å‹ZIPæ–‡ä»¶çš„å‡½æ•° Function to extract ZIP files\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“¦ è§£å‹æ–‡ä»¶ Extracting: {zip_path}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    print(f\"âœ… è§£å‹å®Œæˆ Extraction completed: {extract_to}\")\n",
    "\n",
    "# è§£å‹è¯„ä¼°æ•°æ®é›† Extract evaluation dataset\n",
    "if not os.path.exists('data/raw/evaluation'):\n",
    "    extract_zip('data/raw/evaluation.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"âœ… è¯„ä¼°æ•°æ®é›†å·²è§£å‹ Evaluation dataset already extracted\")\n",
    "\n",
    "# è§£å‹æ ‡æ³¨æ•°æ® Extract annotations\n",
    "if not os.path.exists('data/raw/annotations'):\n",
    "    extract_zip('data/raw/annotations.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"âœ… æ ‡æ³¨æ•°æ®å·²è§£å‹ Annotations already extracted\")\n",
    "\n",
    "# æŸ¥çœ‹æ•°æ®ç»“æ„ Explore data structure\n",
    "print(\"\\nğŸ“Š æ•°æ®ç»“æ„åˆ†æ Data Structure Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥è¯„ä¼°æ•°æ®é›†ç»“æ„ Check evaluation dataset structure\n",
    "eval_path = Path('data/raw/evaluation')\n",
    "if eval_path.exists():\n",
    "    print(f\"è¯„ä¼°æ•°æ®é›†è·¯å¾„ Evaluation dataset path: {eval_path}\")\n",
    "    subdirs = [d for d in eval_path.iterdir() if d.is_dir()]\n",
    "    print(f\"å­ç›®å½•æ•°é‡ Number of subdirectories: {len(subdirs)}\")\n",
    "    for subdir in subdirs[:5]:  # æ˜¾ç¤ºå‰5ä¸ªå­ç›®å½• Show first 5 subdirectories\n",
    "        print(f\"  - {subdir.name}\")\n",
    "    if len(subdirs) > 5:\n",
    "        print(f\"  ... è¿˜æœ‰ {len(subdirs)-5} ä¸ªç›®å½• and {len(subdirs)-5} more directories\")\n",
    "\n",
    "# æ£€æŸ¥æ ‡æ³¨æ•°æ®ç»“æ„ Check annotations structure\n",
    "ann_path = Path('data/raw/annotations')\n",
    "if ann_path.exists():\n",
    "    print(f\"\\næ ‡æ³¨æ•°æ®è·¯å¾„ Annotations path: {ann_path}\")\n",
    "    ann_files = list(ann_path.glob('*.csv'))\n",
    "    print(f\"CSVæ ‡æ³¨æ–‡ä»¶æ•°é‡ Number of CSV annotation files: {len(ann_files)}\")\n",
    "    for ann_file in ann_files[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ ‡æ³¨æ–‡ä»¶ Show first 3 annotation files\n",
    "        print(f\"  - {ann_file.name}\")\n",
    "\n",
    "print(\"\\nâœ… æ•°æ®è§£å‹å’Œç»“æ„åˆ†æå®Œæˆ Data extraction and structure analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 6. æ•°æ®æ ¼å¼è½¬æ¢ - Data Format Conversion (NeonTree -> YOLO) - ä½¿ç”¨çœŸå®å›¾åƒ\nimport xml.etree.ElementTree as ET\n\nclass NeonTreeToYOLO:\n    \"\"\"\n    å°†NeonTreeæ•°æ®é›†è½¬æ¢ä¸ºYOLOæ ¼å¼çš„ç±»\n    Class to convert NeonTree dataset to YOLO format\n    \"\"\"\n    \n    def __init__(self, annotations_root, evaluation_root, output_root):\n        self.annotations_root = Path(annotations_root)\n        self.evaluation_root = Path(evaluation_root)\n        self.output_root = Path(output_root)\n        \n        # åˆ›å»ºè¾“å‡ºç›®å½• Create output directories\n        self.create_yolo_structure()\n        \n        # åˆ›å»ºå›¾åƒæ–‡ä»¶åæ˜ å°„ Create image filename mapping\n        self.create_image_mapping()\n    \n    def create_yolo_structure(self):\n        \"\"\"åˆ›å»ºYOLOæ•°æ®é›†ç›®å½•ç»“æ„ Create YOLO dataset directory structure\"\"\"\n        folders = [\n            'images/train', 'images/val', 'images/test',\n            'labels/train', 'labels/val', 'labels/test'\n        ]\n        \n        for folder in folders:\n            (self.output_root / folder).mkdir(parents=True, exist_ok=True)\n        \n        print(\"âœ… YOLOç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ YOLO directory structure created\")\n    \n    def create_image_mapping(self):\n        \"\"\"\n        åˆ›å»ºå›¾åƒæ–‡ä»¶ååˆ°è·¯å¾„çš„æ˜ å°„\n        Create mapping from image filenames to paths\n        \"\"\"\n        self.image_mapping = {}\n        \n        # æ£€æŸ¥ evaluation/RGB ç›®å½•\n        rgb_dir = self.evaluation_root / 'RGB'\n        if rgb_dir.exists():\n            for img_file in rgb_dir.glob('*.tif'):\n                # å»æ‰æ‰©å±•åä½œä¸ºkey\n                base_name = img_file.stem\n                self.image_mapping[base_name] = img_file\n                \n                # åŒæ—¶æ·»åŠ ä¸å¸¦æ‰©å±•åçš„æ˜ å°„\n                if base_name.endswith('.tif'):\n                    base_name_no_ext = base_name[:-4]\n                    self.image_mapping[base_name_no_ext] = img_file\n        \n        print(f\"ğŸ“‹ åˆ›å»ºå›¾åƒæ˜ å°„ Created image mapping with {len(self.image_mapping)} entries\")\n    \n    def find_matching_image(self, xml_filename):\n        \"\"\"\n        æ ¹æ®XMLæ–‡ä»¶åæŸ¥æ‰¾å¯¹åº”çš„RGBå›¾åƒæ–‡ä»¶\n        Find matching RGB image file based on XML filename\n        \"\"\"\n        # ä»XMLæ–‡ä»¶åä¸­æå–åŸºç¡€åç§°\n        base_name = xml_filename.replace('.xml', '')\n        \n        # ç›´æ¥åŒ¹é…\n        if base_name in self.image_mapping:\n            return self.image_mapping[base_name]\n        \n        # å°è¯•æ¨¡ç³ŠåŒ¹é… - æŸ¥æ‰¾åŒ…å«base_nameçš„å›¾åƒæ–‡ä»¶\n        for img_name, img_path in self.image_mapping.items():\n            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ï¼ˆå»æ‰å¹´ä»½ç­‰å˜åŒ–éƒ¨åˆ†ï¼‰\n            if self.files_match(base_name, img_name):\n                return img_path\n        \n        return None\n    \n    def files_match(self, xml_name, img_name):\n        \"\"\"\n        åˆ¤æ–­XMLæ–‡ä»¶åå’Œå›¾åƒæ–‡ä»¶åæ˜¯å¦åŒ¹é…\n        Check if XML filename matches image filename\n        \"\"\"\n        # ç§»é™¤å¸¸è§çš„åç¼€å’Œå‰ç¼€\n        xml_clean = xml_name.lower()\n        img_clean = img_name.lower()\n        \n        # æå–æ ¸å¿ƒç«™ç‚¹ä»£ç \n        xml_parts = xml_clean.split('_')\n        img_parts = img_clean.split('_')\n        \n        # å¦‚æœç«™ç‚¹ä»£ç åŒ¹é…\n        if len(xml_parts) >= 2 and len(img_parts) >= 2:\n            xml_site = xml_parts[0]\n            img_site = img_parts[0]\n            \n            if xml_site == img_site:\n                # è¿›ä¸€æ­¥æ£€æŸ¥ç¼–å·åŒ¹é…\n                if len(xml_parts) >= 3 and len(img_parts) >= 2:\n                    try:\n                        xml_num = xml_parts[1]\n                        img_num = img_parts[1]\n                        return xml_num == img_num\n                    except:\n                        pass\n        \n        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åŒçš„å…³é”®éƒ¨åˆ†\n        return any(part in img_clean for part in xml_parts if len(part) > 2)\n    \n    def parse_xml_annotation(self, xml_file):\n        \"\"\"\n        è§£æXMLæ ‡æ³¨æ–‡ä»¶\n        Parse XML annotation file\n        \"\"\"\n        try:\n            tree = ET.parse(xml_file)\n            root = tree.getroot()\n            \n            # è·å–å›¾åƒä¿¡æ¯ Get image information\n            filename_elem = root.find('filename')\n            if filename_elem is not None:\n                filename = filename_elem.text\n            else:\n                filename = xml_file.stem + '.tif'  # é»˜è®¤æ‰©å±•å\n            \n            size = root.find('size')\n            width = int(size.find('width').text)\n            height = int(size.find('height').text)\n            \n            # è·å–æ‰€æœ‰æ ‘æœ¨å¯¹è±¡ Get all tree objects\n            objects = []\n            for obj in root.findall('object'):\n                name_elem = obj.find('name')\n                if name_elem is not None and name_elem.text.lower() == 'tree':\n                    bndbox = obj.find('bndbox')\n                    xmin = int(float(bndbox.find('xmin').text))\n                    ymin = int(float(bndbox.find('ymin').text))\n                    xmax = int(float(bndbox.find('xmax').text))\n                    ymax = int(float(bndbox.find('ymax').text))\n                    \n                    # éªŒè¯è¾¹ç•Œæ¡†æœ‰æ•ˆæ€§ Validate bounding box\n                    if xmax > xmin and ymax > ymin:\n                        objects.append({\n                            'xmin': xmin,\n                            'ymin': ymin,\n                            'xmax': xmax,\n                            'ymax': ymax\n                        })\n            \n            return {\n                'filename': filename,\n                'width': width,\n                'height': height,\n                'objects': objects\n            }\n            \n        except Exception as e:\n            print(f\"è§£æXMLæ–‡ä»¶æ—¶å‡ºé”™ Error parsing XML file {xml_file}: {e}\")\n            return None\n    \n    def convert_bbox_to_yolo(self, bbox, img_width, img_height):\n        \"\"\"\n        å°†è¾¹ç•Œæ¡†åæ ‡è½¬æ¢ä¸ºYOLOæ ¼å¼\n        Convert bounding box coordinates to YOLO format\n        \n        YOLOæ ¼å¼: [class_id, x_center, y_center, width, height] (å½’ä¸€åŒ– normalized)\n        \"\"\"\n        xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']\n        \n        # è®¡ç®—ä¸­å¿ƒç‚¹å’Œå®½é«˜ Calculate center point and dimensions\n        x_center = (xmin + xmax) / 2.0\n        y_center = (ymin + ymax) / 2.0\n        width = xmax - xmin\n        height = ymax - ymin\n        \n        # å½’ä¸€åŒ– Normalize\n        x_center /= img_width\n        y_center /= img_height\n        width /= img_width\n        height /= img_height\n        \n        return [0, x_center, y_center, width, height]  # ç±»åˆ«IDä¸º0 (æ ‘æœ¨ tree)\n    \n    def process_annotations(self):\n        \"\"\"\n        å¤„ç†æ‰€æœ‰XMLæ ‡æ³¨æ–‡ä»¶å¹¶è½¬æ¢ä¸ºYOLOæ ¼å¼\n        Process all XML annotation files and convert to YOLO format\n        \"\"\"\n        xml_files = list(self.annotations_root.glob('*.xml'))\n        processed_count = 0\n        skipped_count = 0\n        \n        print(f\"ğŸ“ æ‰¾åˆ° {len(xml_files)} ä¸ªXMLæ ‡æ³¨æ–‡ä»¶ Found {len(xml_files)} XML annotation files\")\n        \n        for i, xml_file in enumerate(tqdm(xml_files, desc=\"å¤„ç†æ ‡æ³¨ Processing annotations\")):\n            try:\n                # è§£æXMLæ–‡ä»¶ Parse XML file\n                annotation_data = self.parse_xml_annotation(xml_file)\n                \n                if annotation_data is None or not annotation_data['objects']:\n                    skipped_count += 1\n                    continue\n                \n                # æŸ¥æ‰¾å¯¹åº”çš„RGBå›¾åƒæ–‡ä»¶ Find corresponding RGB image file\n                img_path = self.find_matching_image(xml_file.name)\n                \n                if img_path is None:\n                    # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œä½†ä¸ä¸­æ–­å¤„ç† Print debug info but don't interrupt\n                    if processed_count < 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯\n                        print(f\"âš ï¸  æœªæ‰¾åˆ°å¯¹åº”å›¾åƒ No matching image for: {xml_file.name}\")\n                    skipped_count += 1\n                    continue\n                \n                # è¯»å–çœŸå®å›¾åƒ Load real image\n                img = cv2.imread(str(img_path))\n                if img is None:\n                    print(f\"âš ï¸  æ— æ³•è¯»å–å›¾åƒ Cannot read image: {img_path}\")\n                    skipped_count += 1\n                    continue\n                \n                # éªŒè¯å›¾åƒå°ºå¯¸ Verify image dimensions\n                actual_height, actual_width = img.shape[:2]\n                if actual_width != annotation_data['width'] or actual_height != annotation_data['height']:\n                    # ä½¿ç”¨å®é™…å›¾åƒå°ºå¯¸ Use actual image dimensions\n                    annotation_data['width'] = actual_width\n                    annotation_data['height'] = actual_height\n                \n                # ç¡®å®šæ•°æ®é›†åˆ†å‰² Determine dataset split\n                if i % 10 < 7:  # 70% è®­ç»ƒé›† training set\n                    split = 'train'\n                elif i % 10 < 9:  # 20% éªŒè¯é›† validation set\n                    split = 'val'\n                else:  # 10% æµ‹è¯•é›† test set\n                    split = 'test'\n                \n                # ä¿å­˜å›¾åƒ Save image\n                img_filename = f\"tree_{processed_count:06d}.jpg\"\n                img_save_path = self.output_root / 'images' / split / img_filename\n                cv2.imwrite(str(img_save_path), img)\n                \n                # ä¿å­˜YOLOæ ¼å¼æ ‡ç­¾ Save YOLO format labels\n                label_filename = f\"tree_{processed_count:06d}.txt\"\n                label_save_path = self.output_root / 'labels' / split / label_filename\n                \n                with open(label_save_path, 'w') as f:\n                    for obj in annotation_data['objects']:\n                        yolo_bbox = self.convert_bbox_to_yolo(\n                            obj, annotation_data['width'], annotation_data['height']\n                        )\n                        # YOLOæ ¼å¼: class_id x_center y_center width height (ä¿®å¤æ¢è¡Œç¬¦)\n                        f.write(f\"{yolo_bbox[0]} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f} {yolo_bbox[4]:.6f}\\n\")\n                \n                processed_count += 1\n                \n            except Exception as e:\n                print(f\"å¤„ç†æ–‡ä»¶ {xml_file.name} æ—¶å‡ºé”™ Error processing file {xml_file.name}: {e}\")\n                skipped_count += 1\n                continue\n        \n        print(f\"âœ… å¤„ç†å®Œæˆ Processing completed: {processed_count} ä¸ªæ ·æœ¬ samples processed, {skipped_count} ä¸ªè·³è¿‡ skipped\")\n        return processed_count\n\n# æ‰§è¡Œæ•°æ®è½¬æ¢ Execute data conversion\nprint(\"ğŸ”„ å¼€å§‹æ•°æ®æ ¼å¼è½¬æ¢ Starting data format conversion...\")\nprint(\"ğŸ“‹ ä½¿ç”¨XMLæ ¼å¼çš„æ ‡æ³¨æ•°æ®å’ŒçœŸå®RGBå›¾åƒ Using XML annotations with real RGB images\")\n\n# åˆ›å»ºdataç›®å½• Create data directories\nos.makedirs('data', exist_ok=True)\nos.makedirs('data/processed', exist_ok=True)\n\n# æ£€æŸ¥annotationså’Œevaluationæ•°æ®æ˜¯å¦å­˜åœ¨ Check if annotations and evaluation data exist\nannotation_paths = ['data/raw/annotations', 'annotations', '/content/annotations']\neval_paths = ['data/raw/evaluation', 'evaluation', '/content/evaluation']\n\nannotations_root = None\neval_root = None\n\n# æŸ¥æ‰¾annotationsç›®å½• Find annotations directory\nfor path in annotation_paths:\n    if Path(path).exists():\n        annotations_root = Path(path)\n        xml_count = len(list(annotations_root.glob('*.xml')))\n        if xml_count > 0:\n            print(f\"âœ… æ‰¾åˆ°annotationsæ•°æ®é›† Found annotations dataset at: {annotations_root} ({xml_count} XML files)\")\n            break\n\nif annotations_root is None:\n    print(\"âŒ æœªæ‰¾åˆ°annotationsæ•°æ®é›†æˆ–XMLæ–‡ä»¶\")\n    print(\"ğŸ“ è¯·ç¡®ä¿annotationsç›®å½•å­˜åœ¨ä¸”åŒ…å«XMLæ–‡ä»¶\")\n\n# æŸ¥æ‰¾evaluationç›®å½• Find evaluation directory  \nfor path in eval_paths:\n    if Path(path).exists():\n        eval_root = Path(path)\n        # æ£€æŸ¥RGBå­ç›®å½•\n        rgb_dir = eval_root / 'RGB'\n        if rgb_dir.exists():\n            img_count = len(list(rgb_dir.glob('*.tif')))\n            print(f\"âœ… æ‰¾åˆ°evaluationæ•°æ®é›† Found evaluation dataset at: {eval_root} ({img_count} images)\")\n            break\n\nif eval_root is None:\n    print(\"âŒ æœªæ‰¾åˆ°evaluationæ•°æ®é›†ï¼Œè¯·ç¡®ä¿å·²ä¸‹è½½å¹¶è§£å‹evaluation.zip\")\n\n# åªæœ‰å½“ä¸¤ä¸ªæ•°æ®é›†éƒ½æ‰¾åˆ°æ—¶æ‰è¿›è¡Œè½¬æ¢ Only proceed if both datasets are found\nif annotations_root is not None and eval_root is not None:\n    # åˆå§‹åŒ–è½¬æ¢å™¨ Initialize converter\n    converter = NeonTreeToYOLO(annotations_root, eval_root, 'data/processed')\n    \n    # å¤„ç†æ ‡æ³¨æ•°æ® Process annotation data\n    processed_samples = converter.process_annotations()\n    \n    if processed_samples > 0:\n        print(f\"âœ… æ•°æ®è½¬æ¢å®Œæˆ Data conversion completed: {processed_samples} ä¸ªæ ·æœ¬ samples\")\n        \n        # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ Show dataset statistics\n        train_images = len(list(Path('data/processed/images/train').glob('*.jpg')))\n        val_images = len(list(Path('data/processed/images/val').glob('*.jpg')))\n        test_images = len(list(Path('data/processed/images/test').glob('*.jpg')))\n        \n        print(f\"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ Dataset Statistics:\")\n        print(f\"   è®­ç»ƒé›† Training: {train_images} å¼ å›¾åƒ\")\n        print(f\"   éªŒè¯é›† Validation: {val_images} å¼ å›¾åƒ\")\n        print(f\"   æµ‹è¯•é›† Testing: {test_images} å¼ å›¾åƒ\")\n        print(f\"   æ€»è®¡ Total: {train_images + val_images + test_images} å¼ å›¾åƒ\")\n    else:\n        print(\"âŒ æœªèƒ½å¤„ç†ä»»ä½•æ ·æœ¬ No samples were processed\")\n        print(\"è¯·æ£€æŸ¥XMLæ–‡ä»¶å’Œå›¾åƒæ–‡ä»¶çš„åŒ¹é…å…³ç³»\")\nelse:\n    print(\"âŒ ç¼ºå°‘å¿…è¦çš„æ•°æ®é›†ï¼Œæ— æ³•è¿›è¡Œè½¬æ¢\")\n    print(\"è¯·ç¡®ä¿ä»¥ä¸‹ç›®å½•å­˜åœ¨ï¼š\")\n    print(\"  - annotations ç›®å½•ï¼ˆåŒ…å«XMLæ–‡ä»¶ï¼‰\")\n    print(\"  - evaluation/RGB ç›®å½•ï¼ˆåŒ…å«.tifå›¾åƒæ–‡ä»¶ï¼‰\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 7. åˆ›å»ºYOLOé…ç½®æ–‡ä»¶ - Create YOLO Configuration Files (ä¿®å¤è·¯å¾„é—®é¢˜)\nimport os\nfrom pathlib import Path\nimport yaml\n\ndef create_dataset_yaml():\n    \"\"\"\n    åˆ›å»ºYOLOæ•°æ®é›†é…ç½®æ–‡ä»¶\n    Create YOLO dataset configuration file\n    \"\"\"\n    # è·å–å½“å‰å·¥ä½œç›®å½• Get current working directory\n    current_dir = os.getcwd()\n    \n    # æ„å»ºç»å¯¹è·¯å¾„ Build absolute paths\n    processed_dir = os.path.join(current_dir, 'data/processed')\n    train_path = os.path.join(processed_dir, 'images/train')\n    val_path = os.path.join(processed_dir, 'images/val')\n    test_path = os.path.join(processed_dir, 'images/test')\n    \n    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ Check if paths exist\n    paths_info = {\n        'train': train_path,\n        'val': val_path,\n        'test': test_path\n    }\n    \n    print(\"ğŸ“‹ æ£€æŸ¥æ•°æ®é›†è·¯å¾„ Checking dataset paths:\")\n    existing_paths = {}\n    \n    for split, path in paths_info.items():\n        if os.path.exists(path):\n            img_count = len([f for f in os.listdir(path) if f.endswith('.jpg')])\n            print(f\"âœ… {split}: {path} ({img_count} å¼ å›¾åƒ images)\")\n            existing_paths[split] = path\n        else:\n            print(f\"âŒ {split}: {path} (è·¯å¾„ä¸å­˜åœ¨ path does not exist)\")\n    \n    # ç¡®ä¿è‡³å°‘æœ‰è®­ç»ƒé›†å­˜åœ¨ Ensure at least training set exists\n    if 'train' not in existing_paths:\n        print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°è®­ç»ƒé›† Error: Training set not found\")\n        return None\n    \n    # åˆ›å»ºæ•°æ®é›†é…ç½® Create dataset configuration\n    dataset_config = {\n        'path': processed_dir,  # æ•°æ®é›†æ ¹ç›®å½• dataset root dir\n        'train': 'images/train',  # ç›¸å¯¹äºpathçš„è®­ç»ƒå›¾åƒè·¯å¾„ train images relative to path\n        'val': 'images/val' if 'val' in existing_paths else 'images/train',  # éªŒè¯é›†ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨è®­ç»ƒé›†\n        'test': 'images/test' if 'test' in existing_paths else 'images/train',  # æµ‹è¯•é›†ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨è®­ç»ƒé›†\n        'nc': 1,                  # ç±»åˆ«æ•°é‡ number of classes\n        'names': ['tree']         # ç±»åˆ«åç§° class names\n    }\n    \n    # ç‰¹åˆ«å¤„ç†éªŒè¯é›†è·¯å¾„\n    if 'val' not in existing_paths:\n        print(\"âš ï¸  éªŒè¯é›†ä¸å­˜åœ¨ï¼Œä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºéªŒè¯é›† Validation set not found, using training set as validation\")\n        dataset_config['val'] = 'images/train'\n    \n    if 'test' not in existing_paths:\n        print(\"âš ï¸  æµ‹è¯•é›†ä¸å­˜åœ¨ï¼Œä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºæµ‹è¯•é›† Test set not found, using training set as test\")\n        dataset_config['test'] = 'images/train'\n    \n    # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨\n    os.makedirs('data', exist_ok=True)\n    \n    # ä¿å­˜é…ç½®æ–‡ä»¶ Save configuration file\n    config_path = 'data/tree_dataset.yaml'\n    with open(config_path, 'w') as f:\n        yaml.dump(dataset_config, f, default_flow_style=False)\n    \n    print(f\"âœ… YOLOæ•°æ®é›†é…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ YOLO dataset configuration file created: {config_path}\")\n    \n    # éªŒè¯é…ç½®æ–‡ä»¶å†…å®¹\n    print(\"\\nğŸ” éªŒè¯é…ç½®æ–‡ä»¶å†…å®¹ Verifying configuration file:\")\n    for key, value in dataset_config.items():\n        if key in ['train', 'val', 'test']:\n            full_path = os.path.join(processed_dir, value)\n            exists = os.path.exists(full_path)\n            print(f\"   {key}: {value} -> {full_path} ({'âœ…' if exists else 'âŒ'})\")\n        else:\n            print(f\"   {key}: {value}\")\n    \n    return config_path\n\ndef fix_data_structure():\n    \"\"\"\n    ä¿®å¤æ•°æ®ç»“æ„é—®é¢˜\n    Fix data structure issues\n    \"\"\"\n    print(\"\\nğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®ç»“æ„ Checking and fixing data structure...\")\n    \n    processed_dir = Path('data/processed')\n    \n    if not processed_dir.exists():\n        print(\"âŒ data/processed ç›®å½•ä¸å­˜åœ¨ data/processed directory does not exist\")\n        return False\n    \n    # æ£€æŸ¥å¿…è¦çš„ç›®å½•ç»“æ„\n    required_dirs = [\n        'images/train', 'images/val', 'images/test',\n        'labels/train', 'labels/val', 'labels/test'\n    ]\n    \n    missing_dirs = []\n    for dir_path in required_dirs:\n        full_path = processed_dir / dir_path\n        if not full_path.exists():\n            missing_dirs.append(dir_path)\n    \n    if missing_dirs:\n        print(f\"âš ï¸  ç¼ºå°‘ç›®å½• Missing directories: {missing_dirs}\")\n        \n        # å¦‚æœåªæ˜¯éªŒè¯é›†å’Œæµ‹è¯•é›†ç¼ºå¤±ï¼Œåˆ›å»ºå®ƒä»¬\n        for missing_dir in missing_dirs:\n            if 'val' in missing_dir or 'test' in missing_dir:\n                (processed_dir / missing_dir).mkdir(parents=True, exist_ok=True)\n                print(f\"âœ… åˆ›å»ºç›®å½• Created directory: {missing_dir}\")\n    \n    # æ£€æŸ¥è®­ç»ƒé›†æ˜¯å¦æœ‰æ•°æ®\n    train_images = list((processed_dir / 'images/train').glob('*.jpg'))\n    train_labels = list((processed_dir / 'labels/train').glob('*.txt'))\n    \n    print(f\"ğŸ“Š è®­ç»ƒé›†ç»Ÿè®¡ Training set statistics:\")\n    print(f\"   å›¾åƒæ–‡ä»¶ Image files: {len(train_images)}\")\n    print(f\"   æ ‡ç­¾æ–‡ä»¶ Label files: {len(train_labels)}\")\n    \n    if len(train_images) == 0:\n        print(\"âŒ è®­ç»ƒé›†ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œæ•°æ®è½¬æ¢ Training set is empty, please run data conversion first\")\n        return False\n    \n    return True\n\n# ä¿®å¤æ•°æ®ç»“æ„\ndata_structure_ok = fix_data_structure()\n\nif data_structure_ok:\n    # åˆ›å»ºé…ç½®æ–‡ä»¶ Create configuration file\n    dataset_yaml_path = create_dataset_yaml()\n    \n    if dataset_yaml_path:\n        # æ˜¾ç¤ºé…ç½®æ–‡ä»¶å†…å®¹ Display configuration file content\n        with open(dataset_yaml_path, 'r') as f:\n            config_content = f.read()\n            print(\"\\nğŸ“‹ æ•°æ®é›†é…ç½®å†…å®¹ Dataset Configuration Content:\")\n            print(\"=\" * 40)\n            print(config_content)\n            print(\"=\" * 40)\n        \n        # æœ€ç»ˆéªŒè¯\n        print(\"\\nâœ… é…ç½®æ–‡ä»¶åˆ›å»ºå¹¶éªŒè¯å®Œæˆ Configuration file created and verified\")\n    else:\n        print(\"âŒ é…ç½®æ–‡ä»¶åˆ›å»ºå¤±è´¥ Failed to create configuration file\")\nelse:\n    print(\"âŒ æ•°æ®ç»“æ„ä¿®å¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®è½¬æ¢æ­¥éª¤ Data structure fix failed, please check data conversion step\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 8. YOLOv5æ¨¡å‹è®­ç»ƒ - YOLOv5 Model Training (è§£å†³resumeå‚æ•°é—®é¢˜)\nimport torch\nimport subprocess\nimport sys\nimport pickle\nimport gc  # åƒåœ¾å›æ”¶\n\n# é¦–å…ˆå®‰è£…YOLOv5 First install YOLOv5\nprint(\"ğŸ“¦ å®‰è£…YOLOv5... Installing YOLOv5...\")\ntry:\n    # å…‹éš†YOLOv5ä»“åº“ Clone YOLOv5 repository\n    if not os.path.exists('yolov5'):\n        !git clone https://github.com/ultralytics/yolov5.git\n        os.chdir('yolov5')\n        !pip install -r requirements.txt\n        os.chdir('..')\n    print(\"âœ… YOLOv5å®‰è£…å®Œæˆ YOLOv5 installation completed\")\nexcept Exception as e:\n    print(f\"YOLOv5å®‰è£…è­¦å‘Š YOLOv5 installation warning: {e}\")\n\n# æ·»åŠ yolov5åˆ°ç³»ç»Ÿè·¯å¾„ Add yolov5 to system path\nif os.path.exists('yolov5'):\n    sys.path.append('yolov5')\n\ndef check_system_resources():\n    \"\"\"\n    æ£€æŸ¥ç³»ç»Ÿèµ„æº\n    Check system resources\n    \"\"\"\n    print(\"ğŸ’» æ£€æŸ¥ç³»ç»Ÿèµ„æº Checking system resources...\")\n    \n    # æ£€æŸ¥GPUå†…å­˜\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3\n        gpu_reserved = torch.cuda.memory_reserved(0) / 1024**3\n        \n        print(f\"ğŸ® GPUå†…å­˜ GPU Memory:\")\n        print(f\"   æ€»å†…å­˜ Total: {gpu_memory:.1f} GB\")\n        print(f\"   å·²åˆ†é… Allocated: {gpu_allocated:.1f} GB\")\n        print(f\"   å·²ä¿ç•™ Reserved: {gpu_reserved:.1f} GB\")\n        print(f\"   å¯ç”¨ Available: {gpu_memory - gpu_reserved:.1f} GB\")\n        \n        # å¦‚æœå¯ç”¨å†…å­˜å°‘äº2GBï¼Œå»ºè®®é™ä½batch size\n        if (gpu_memory - gpu_reserved) < 2.0:\n            print(\"âš ï¸  GPUå†…å­˜ä¸è¶³ï¼Œå»ºè®®é™ä½batch_size\")\n            return 'low_memory'\n    \n    # æ£€æŸ¥ç£ç›˜ç©ºé—´\n    import shutil\n    disk_usage = shutil.disk_usage('/')\n    free_space = disk_usage.free / 1024**3\n    \n    print(f\"ğŸ’¾ ç£ç›˜ç©ºé—´ Disk Space:\")\n    print(f\"   å¯ç”¨ç©ºé—´ Free space: {free_space:.1f} GB\")\n    \n    if free_space < 2.0:\n        print(\"âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³\")\n        return 'low_disk'\n    \n    return 'ok'\n\ndef fix_best_pt_compatibility(model_path, output_path='best_fixed.pt'):\n    \"\"\"\n    ä¿®å¤best.ptæ¨¡å‹çš„å…¼å®¹æ€§é—®é¢˜ (å†…å­˜ä¼˜åŒ–ç‰ˆ)\n    Fix compatibility issues with best.pt model (memory optimized)\n    \"\"\"\n    print(f\"ğŸ”§ å°è¯•ä¿®å¤æ¨¡å‹å…¼å®¹æ€§ Attempting to fix model compatibility: {model_path}\")\n    \n    try:\n        # æ–¹æ³•1: ä½¿ç”¨weights_only=FalseåŠ è½½\n        print(\"   æ–¹æ³•1: ä½¿ç”¨weights_only=FalseåŠ è½½ Method 1: Load with weights_only=False\")\n        checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)\n        \n        if 'model' in checkpoint:\n            print(\"   âœ… æˆåŠŸåŠ è½½æ¨¡å‹ Successfully loaded model\")\n            \n            # æå–æ¨¡å‹æƒé‡å’Œå…³é”®ä¿¡æ¯\n            model_state = checkpoint['model']\n            \n            # åˆ›å»ºæ–°çš„å…¼å®¹æ£€æŸ¥ç‚¹ Create new compatible checkpoint\n            new_checkpoint = {\n                'model': model_state,\n                'epoch': checkpoint.get('epoch', 0),\n                'best_fitness': checkpoint.get('best_fitness', 0.0),\n            }\n            \n            # æ¸…ç†ä¸å¿…è¦çš„å†…å­˜å ç”¨\n            del checkpoint\n            gc.collect()\n            \n            # ä¿å­˜ä¿®å¤åçš„æ¨¡å‹ Save fixed model\n            torch.save(new_checkpoint, output_path)\n            print(f\"   âœ… ä¿®å¤åçš„æ¨¡å‹å·²ä¿å­˜ Fixed model saved: {output_path}\")\n            return output_path\n            \n    except Exception as e1:\n        print(f\"   âŒ æ–¹æ³•1å¤±è´¥ Method 1 failed: {e1}\")\n        # æ¸…ç†å†…å­˜\n        gc.collect()\n        \n        try:\n            # æ–¹æ³•3: ä»…æå–æƒé‡å­—å…¸ (æœ€è½»é‡)\n            print(\"   æ–¹æ³•2: ä»…æå–state_dict Method 2: Extract state_dict only\")\n            \n            checkpoint = torch.load(model_path, map_location='cpu')\n            \n            # å°è¯•å„ç§å¯èƒ½çš„ç»“æ„\n            state_dict = None\n            if 'model' in checkpoint:\n                if hasattr(checkpoint['model'], 'state_dict'):\n                    state_dict = checkpoint['model'].state_dict()\n                elif isinstance(checkpoint['model'], dict):\n                    state_dict = checkpoint['model']\n            elif 'state_dict' in checkpoint:\n                state_dict = checkpoint['state_dict']\n            \n            if state_dict is not None:\n                # ä¿å­˜çº¯æƒé‡æ–‡ä»¶\n                torch.save({'model': state_dict}, output_path)\n                print(f\"   âœ… state_dictæå–æˆåŠŸ state_dict extraction successful: {output_path}\")\n                return output_path\n                    \n        except Exception as e3:\n            print(f\"   âŒ æ–¹æ³•2å¤±è´¥ Method 2 failed: {e3}\")\n    \n    print(\"   âŒ æ‰€æœ‰ä¿®å¤æ–¹æ³•éƒ½å¤±è´¥äº† All fix methods failed\")\n    return None\n\n# æ£€æŸ¥ç³»ç»Ÿèµ„æº\nresource_status = check_system_resources()\n\n# æ ¹æ®èµ„æºæƒ…å†µè°ƒæ•´å‚æ•°\nif resource_status == 'low_memory':\n    batch_size = 4\n    img_size = 416  # é™ä½å›¾åƒå°ºå¯¸\n    epochs = 10     # å‡å°‘è®­ç»ƒè½®æ•°\n    workers = 1\n    print(\"ğŸ”§ æ£€æµ‹åˆ°ä½å†…å­˜ï¼Œä½¿ç”¨è½»é‡åŒ–é…ç½®\")\nelif resource_status == 'low_disk':\n    batch_size = 8\n    img_size = 640\n    epochs = 15\n    workers = 1\n    print(\"ğŸ”§ æ£€æµ‹åˆ°ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå‡å°‘epochs\")\nelse:\n    batch_size = 8\n    img_size = 640\n    epochs = 20  # å‡å°‘é»˜è®¤epochsé¿å…é•¿æ—¶é—´è¿è¡Œ\n    workers = 2\n\n# æ£€æŸ¥å¹¶ä¿®å¤é¢„è®­ç»ƒæ¨¡å‹ Check and fix pre-trained model\npretrained_model_path = 'best.pt'\nuse_custom_model = False\nfixed_model_path = None\n\nif os.path.exists(pretrained_model_path):\n    print(f\"âœ… æ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹ Pre-trained model found: {pretrained_model_path}\")\n    \n    # å°è¯•ä¿®å¤æ¨¡å‹å…¼å®¹æ€§\n    fixed_model_path = fix_best_pt_compatibility(pretrained_model_path)\n    \n    if fixed_model_path and os.path.exists(fixed_model_path):\n        print(f\"ğŸ¯ ä½¿ç”¨ä¿®å¤åçš„æ¨¡å‹ Using fixed model: {fixed_model_path}\")\n        weights_path = fixed_model_path\n        use_custom_model = True\n    else:\n        print(\"âš ï¸  æ¨¡å‹ä¿®å¤å¤±è´¥ï¼Œå°†ä½¿ç”¨YOLOv5sä½œä¸ºåŸºç¡€æ¨¡å‹\")\n        weights_path = 'yolov5s.pt'\nelse:\n    print(\"âš ï¸  æœªæ‰¾åˆ°best.ptï¼Œå°†ä½¿ç”¨YOLOv5sé¢„è®­ç»ƒæ¨¡å‹\")\n    weights_path = 'yolov5s.pt'\n\nprint(\"\\nâš™ï¸  è®­ç»ƒå‚æ•°é…ç½® Training parameters configuration:\")\nprint(f\"   å›¾åƒå°ºå¯¸ Image size: {img_size}\")\nprint(f\"   æ‰¹æ¬¡å¤§å° Batch size: {batch_size}\")\nprint(f\"   è®­ç»ƒè½®æ•° Epochs: {epochs}\")\nprint(f\"   æ•°æ®é›†é…ç½® Dataset config: {dataset_yaml_path}\")\nprint(f\"   æ¨¡å‹æƒé‡ Model weights: {weights_path}\")\nprint(f\"   è®¾å¤‡ Device: {device}\")\nprint(f\"   èµ„æºçŠ¶æ€ Resource status: {resource_status}\")\nprint(\"=\" * 60)\n\n# æ„å»ºè®­ç»ƒå‘½ä»¤ Build training command (ä¿®å¤resumeå‚æ•°é—®é¢˜)\ntraining_command = [\n    sys.executable, 'yolov5/train.py',\n    '--img', str(img_size),\n    '--batch', str(batch_size),\n    '--epochs', str(epochs),\n    '--data', dataset_yaml_path,\n    '--weights', weights_path,\n    '--project', 'runs/train',\n    '--name', 'tree_detection_light',  # è½»é‡ç‰ˆå‘½å\n    '--patience', '5',    # æ›´æ—©çš„æ—©åœ\n    '--save-period', '3', # æ›´é¢‘ç¹ä¿å­˜\n    '--workers', str(workers),\n    '--cache', 'ram',     # ä½¿ç”¨RAMç¼“å­˜è€Œä¸æ˜¯ç£ç›˜\n    '--exist-ok'\n    # ç§»é™¤äº†æœ‰é—®é¢˜çš„ --resume å‚æ•°\n]\n\n# å¦‚æœæœ‰GPUï¼Œæ·»åŠ deviceå‚æ•° If GPU available, add device parameter\nif torch.cuda.is_available():\n    training_command.extend(['--device', '0'])\n\nprint(\"ğŸš€ å¼€å§‹YOLOv5è½»é‡åŒ–è®­ç»ƒ Starting YOLOv5 lightweight training...\")\nprint(f\"è®­ç»ƒå‘½ä»¤ Training command: {' '.join(training_command)}\")\nprint(\"ğŸ’¡ å·²ä¼˜åŒ–å†…å­˜ä½¿ç”¨å¹¶ä¿®å¤resumeå‚æ•°é—®é¢˜ Memory usage optimized and resume parameter issue fixed\")\nprint(\"=\" * 60)\n\n# æ¸…ç†å†…å­˜\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\n# å¼€å§‹è®­ç»ƒ Start training\ntry:\n    print(\"â³ è®­ç»ƒå¼€å§‹ï¼Œè¯·è€å¿ƒç­‰å¾…... Training started, please wait...\")\n    result = subprocess.run(training_command, \n                          capture_output=True, \n                          text=True, \n                          cwd='.',\n                          timeout=1800)  # 30åˆ†é’Ÿè¶…æ—¶\n    \n    if result.returncode == 0:\n        print(\"âœ… YOLOv5æ¨¡å‹è®­ç»ƒå®Œæˆ YOLOv5 model training completed!\")\n        print(f\"è®­ç»ƒç»“æœä¿å­˜åœ¨ Training results saved in: runs/train/tree_detection_light\")\n        \n        # æ˜¾ç¤ºè®­ç»ƒè¾“å‡ºçš„æœ€åå‡ è¡Œ Show last few lines of training output\n        output_lines = result.stdout.split('\\n')\n        print(\"\\nğŸ“Š è®­ç»ƒè¾“å‡ºæ‘˜è¦ Training output summary:\")\n        for line in output_lines[-10:]:\n            if line.strip():\n                print(f\"   {line}\")\n                \n    else:\n        print(f\"âŒ è®­ç»ƒå¤±è´¥ Training failed with return code: {result.returncode}\")\n        print(\"é”™è¯¯è¾“å‡º Error output:\")\n        print(result.stderr[:1000])  # é™åˆ¶é”™è¯¯è¾“å‡ºé•¿åº¦\n\nexcept subprocess.TimeoutExpired:\n    print(\"â° è®­ç»ƒè¶…æ—¶ï¼Œå¯èƒ½éœ€è¦å‡å°‘epochsæˆ–batch_size\")\nexcept KeyboardInterrupt:\n    print(\"âš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­ Training interrupted by user\")\nexcept Exception as e:\n    print(f\"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ Error during training: {e}\")\n\n# æ£€æŸ¥è®­ç»ƒç»“æœ Check training results\nweights_dir = Path('runs/train/tree_detection_light/weights')\nif weights_dir.exists():\n    best_pt = weights_dir / 'best.pt'\n    last_pt = weights_dir / 'last.pt'\n    \n    if best_pt.exists():\n        print(f\"âœ… æœ€ä½³æ¨¡å‹ä¿å­˜äº Best model saved at: {best_pt}\")\n        # è®¾ç½®æ¨¡å‹è·¯å¾„ä¾›åç»­ä½¿ç”¨\n        model_path = str(best_pt)\n    elif last_pt.exists():\n        print(f\"âœ… æœ€åæ¨¡å‹ä¿å­˜äº Last model saved at: {last_pt}\")\n        model_path = str(last_pt)\n    else:\n        model_path = None\n        \nelse:\n    print(\"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒç»“æœç›®å½• Training results directory not found\")\n    model_path = None\n\n# æ¸…ç†å†…å­˜\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n\nprint(\"ğŸ§¹ å†…å­˜æ¸…ç†å®Œæˆ Memory cleanup completed\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 9. é˜²å´©æºƒGPUä¼˜åŒ–æ¨ç† - Crash-Resistant GPU Optimized Inference\nimport torch\nimport gc\nimport os\nimport sys\nimport subprocess\nimport time\nimport signal\nfrom pathlib import Path\n\n# è®¾ç½®è¿›ç¨‹è¶…æ—¶å¤„ç†\ndef timeout_handler(signum, frame):\n    print(\"â° æ¨ç†è¿›ç¨‹è¶…æ—¶ï¼Œå¼ºåˆ¶ç»“æŸ\")\n    raise TimeoutError(\"Inference process timeout\")\n\ndef comprehensive_cleanup():\n    \"\"\"\n    ç»¼åˆå†…å­˜å’ŒGPUæ¸…ç† - å¢å¼ºç‰ˆ\n    Enhanced comprehensive memory and GPU cleanup\n    \"\"\"\n    try:\n        print(\"ğŸ§¹ æ‰§è¡Œæ·±åº¦å†…å­˜æ¸…ç†...\")\n        \n        # 1. Pythonåƒåœ¾å›æ”¶\n        collected = gc.collect()\n        print(f\"   åƒåœ¾å›æ”¶é‡Šæ”¾å¯¹è±¡: {collected}\")\n        \n        # 2. CUDAç¼“å­˜æ¸…ç†\n        if torch.cuda.is_available():\n            # æ¸…ç©ºæ‰€æœ‰GPUç¼“å­˜\n            torch.cuda.empty_cache()\n            \n            # åŒæ­¥æ‰€æœ‰CUDAæµ\n            torch.cuda.synchronize()\n            \n            # æ¸…ç†IPCç¼“å­˜\n            torch.cuda.ipc_collect()\n            \n            # é‡ç½®å³°å€¼å†…å­˜ç»Ÿè®¡\n            torch.cuda.reset_peak_memory_stats()\n            \n            # è·å–æ¸…ç†åçš„å†…å­˜çŠ¶æ€\n            allocated = torch.cuda.memory_allocated() / 1024**3\n            reserved = torch.cuda.memory_reserved() / 1024**3\n            print(f\"   CUDAå†…å­˜æ¸…ç†å: å·²åˆ†é… {allocated:.1f}GB, å·²ä¿ç•™ {reserved:.1f}GB\")\n        \n        # 3. å¼ºåˆ¶ç³»ç»Ÿå†…å­˜é‡Šæ”¾\n        import psutil\n        process = psutil.Process(os.getpid())\n        memory_mb = process.memory_info().rss / 1024 / 1024\n        print(f\"   ç³»ç»Ÿå†…å­˜ä½¿ç”¨: {memory_mb:.1f} MB\")\n        \n        # 4. åˆ é™¤ä¸´æ—¶å˜é‡\n        import sys\n        local_vars = list(locals().keys())\n        for var in local_vars:\n            if var.startswith('temp_') or var.startswith('_'):\n                try:\n                    del locals()[var]\n                except:\n                    pass\n        \n        return True\n        \n    except Exception as e:\n        print(f\"âš ï¸  æ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°è­¦å‘Š: {e}\")\n        return False\n\ndef check_system_stability():\n    \"\"\"\n    æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§\n    Check system stability\n    \"\"\"\n    print(\"ğŸ” æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§...\")\n    \n    try:\n        # æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡\n        import psutil\n        memory = psutil.virtual_memory()\n        memory_usage = memory.percent\n        \n        print(f\"   ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡: {memory_usage:.1f}%\")\n        \n        if memory_usage > 90:\n            print(\"âš ï¸  ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¯èƒ½å¯¼è‡´å´©æºƒ\")\n            return 'critical'\n        elif memory_usage > 80:\n            print(\"âš ï¸  ç³»ç»Ÿå†…å­˜ä½¿ç”¨ç‡è¾ƒé«˜\")\n            return 'warning'\n        \n        # æ£€æŸ¥GPUçŠ¶æ€\n        if torch.cuda.is_available():\n            try:\n                # ç®€å•çš„GPUæµ‹è¯•\n                test_tensor = torch.zeros(10, 10).cuda()\n                test_result = test_tensor.sum()\n                del test_tensor\n                torch.cuda.empty_cache()\n                \n                gpu_memory = torch.cuda.get_device_properties(0).total_memory\n                allocated = torch.cuda.memory_allocated()\n                usage_percent = (allocated / gpu_memory) * 100\n                \n                print(f\"   GPUå†…å­˜ä½¿ç”¨ç‡: {usage_percent:.1f}%\")\n                \n                if usage_percent > 90:\n                    return 'critical'\n                elif usage_percent > 70:\n                    return 'warning'\n                    \n            except Exception as e:\n                print(f\"   GPUå¥åº·æ£€æŸ¥å¤±è´¥: {e}\")\n                return 'gpu_error'\n        \n        # æ£€æŸ¥ç£ç›˜ç©ºé—´\n        disk = psutil.disk_usage('/')\n        disk_usage = (disk.used / disk.total) * 100\n        \n        print(f\"   ç£ç›˜ä½¿ç”¨ç‡: {disk_usage:.1f}%\")\n        \n        if disk_usage > 95:\n            return 'critical'\n        \n        print(\"âœ… ç³»ç»ŸçŠ¶æ€è‰¯å¥½\")\n        return 'stable'\n        \n    except Exception as e:\n        print(f\"âŒ ç³»ç»Ÿç¨³å®šæ€§æ£€æŸ¥å¤±è´¥: {e}\")\n        return 'error'\n\ndef safe_gpu_test():\n    \"\"\"\n    å®‰å…¨çš„GPUæµ‹è¯•\n    Safe GPU test\n    \"\"\"\n    print(\"ğŸ§ª æ‰§è¡Œå®‰å…¨GPUæµ‹è¯•...\")\n    \n    try:\n        if not torch.cuda.is_available():\n            print(\"âŒ CUDAä¸å¯ç”¨\")\n            return False\n        \n        # æµ‹è¯•1: åŸºç¡€å¼ é‡æ“ä½œ\n        test1 = torch.randn(100, 100, device='cuda')\n        result1 = test1 @ test1.T\n        del test1, result1\n        \n        # æµ‹è¯•2: å†…å­˜åˆ†é…å’Œé‡Šæ”¾\n        for size in [100, 500, 1000]:\n            test_tensor = torch.zeros(size, size, device='cuda')\n            del test_tensor\n            torch.cuda.empty_cache()\n        \n        # æµ‹è¯•3: è®¾å¤‡åŒæ­¥\n        torch.cuda.synchronize()\n        \n        print(\"âœ… GPUæµ‹è¯•é€šè¿‡\")\n        return True\n        \n    except Exception as e:\n        print(f\"âŒ GPUæµ‹è¯•å¤±è´¥: {e}\")\n        comprehensive_cleanup()\n        return False\n\ndef ultra_safe_inference(model_path, test_image_path, output_dir='ultra_safe_results'):\n    \"\"\"\n    è¶…å®‰å…¨æ¨ç†æ¨¡å¼ - å•å¼ å›¾åƒå¤„ç†\n    Ultra-safe inference mode - single image processing\n    \"\"\"\n    print(f\"ğŸ›¡ï¸  è¶…å®‰å…¨æ¨ç†æ¨¡å¼: {test_image_path}\")\n    \n    try:\n        # åˆ›å»ºç‹¬ç«‹çš„è¾“å‡ºç›®å½•\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # è®¾ç½®æœ€ä¿å®ˆçš„å‚æ•°\n        safe_params = {\n            'img_size': 320,  # æœ€å°å›¾åƒå°ºå¯¸\n            'conf': 0.5,      # è¾ƒé«˜ç½®ä¿¡åº¦é˜ˆå€¼\n            'device': '0' if torch.cuda.is_available() else 'cpu'\n        }\n        \n        print(f\"   å®‰å…¨å‚æ•°: {safe_params}\")\n        \n        # æ„å»ºè¶…ä¿å®ˆçš„æ¨ç†å‘½ä»¤\n        cmd = [\n            sys.executable, 'yolov5/detect.py',\n            '--weights', model_path,\n            '--source', str(test_image_path),\n            '--project', output_dir,\n            '--name', 'safe_detect',\n            '--img', str(safe_params['img_size']),\n            '--conf', str(safe_params['conf']),\n            '--device', safe_params['device'],\n            '--save-txt',\n            '--line-thickness', '1',\n            '--exist-ok',\n            '--nosave'  # ä¸ä¿å­˜å›¾åƒï¼Œåªè¦æ£€æµ‹ç»“æœ\n        ]\n        \n        print(f\"   æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\")\n        \n        # è®¾ç½®è¶…æ—¶å¤„ç†\n        start_time = time.time()\n        \n        # æ‰§è¡Œæ¨ç†\n        result = subprocess.run(\n            cmd,\n            capture_output=True,\n            text=True,\n            timeout=60,  # 1åˆ†é’Ÿè¶…æ—¶\n            cwd='.'\n        )\n        \n        inference_time = time.time() - start_time\n        \n        if result.returncode == 0:\n            print(f\"âœ… æ¨ç†æˆåŠŸ (è€—æ—¶: {inference_time:.1f}s)\")\n            \n            # æ£€æŸ¥ç»“æœ\n            result_dir = Path(output_dir) / 'safe_detect'\n            detection_count = 0\n            \n            if result_dir.exists():\n                label_dir = result_dir / 'labels'\n                if label_dir.exists():\n                    for label_file in label_dir.glob('*.txt'):\n                        with open(label_file, 'r') as f:\n                            lines = f.readlines()\n                            detection_count = len([l for l in lines if l.strip()])\n            \n            print(f\"   æ£€æµ‹åˆ° {detection_count} ä¸ªå¯¹è±¡\")\n            return {\n                'success': True,\n                'detections': detection_count,\n                'time': inference_time,\n                'output': result_dir\n            }\n        else:\n            print(f\"âŒ æ¨ç†å¤±è´¥: {result.stderr[:200]}\")\n            return {'success': False, 'error': result.stderr}\n            \n    except subprocess.TimeoutExpired:\n        print(\"â° æ¨ç†è¶…æ—¶\")\n        return {'success': False, 'error': 'timeout'}\n    except Exception as e:\n        print(f\"âŒ æ¨ç†å¼‚å¸¸: {e}\")\n        return {'success': False, 'error': str(e)}\n    finally:\n        # å¼ºåˆ¶æ¸…ç†\n        comprehensive_cleanup()\n\ndef progressive_inference_test(model_path, test_images_dir):\n    \"\"\"\n    æ¸è¿›å¼æ¨ç†æµ‹è¯• - é€æ­¥å¢åŠ å¤æ‚åº¦\n    Progressive inference test - gradually increase complexity\n    \"\"\"\n    print(\"ğŸ”„ å¼€å§‹æ¸è¿›å¼æ¨ç†æµ‹è¯•...\")\n    \n    # è·å–æµ‹è¯•å›¾åƒ\n    test_images = list(Path(test_images_dir).glob('*.jpg'))\n    if not test_images:\n        print(\"âŒ æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒ\")\n        return False\n    \n    print(f\"æ‰¾åˆ° {len(test_images)} å¼ æµ‹è¯•å›¾åƒ\")\n    \n    results = []\n    max_images = min(3, len(test_images))  # æœ€å¤šæµ‹è¯•3å¼ \n    \n    for i in range(max_images):\n        print(f\"\\n--- æµ‹è¯•å›¾åƒ {i+1}/{max_images} ---\")\n        \n        # æ¯æ¬¡æ¨ç†å‰æ£€æŸ¥ç³»ç»ŸçŠ¶æ€\n        stability = check_system_stability()\n        \n        if stability in ['critical', 'error']:\n            print(f\"âš ï¸  ç³»ç»ŸçŠ¶æ€å¼‚å¸¸ ({stability})ï¼Œåœæ­¢æµ‹è¯•\")\n            break\n        \n        # æ‰§è¡Œè¶…å®‰å…¨æ¨ç†\n        img_path = test_images[i]\n        result = ultra_safe_inference(model_path, img_path, f'progressive_test_{i+1}')\n        \n        if result['success']:\n            results.append(result)\n            print(f\"âœ… å›¾åƒ {i+1} å¤„ç†æˆåŠŸ\")\n        else:\n            print(f\"âŒ å›¾åƒ {i+1} å¤„ç†å¤±è´¥: {result.get('error', 'unknown')}\")\n            # å¤±è´¥åè¿›è¡Œé¢å¤–æ¸…ç†\n            comprehensive_cleanup()\n            time.sleep(2)  # ä¼‘æ¯2ç§’\n        \n        # å¼ºåˆ¶æ¸…ç†å’ŒçŸ­æš‚ä¼‘æ¯\n        comprehensive_cleanup()\n        time.sleep(1)\n    \n    # æ±‡æ€»ç»“æœ\n    success_count = len(results)\n    print(f\"\\nğŸ“Š æ¸è¿›å¼æµ‹è¯•æ±‡æ€»:\")\n    print(f\"   æˆåŠŸç‡: {success_count}/{max_images} ({success_count/max_images*100:.1f}%)\")\n    \n    if results:\n        total_detections = sum(r['detections'] for r in results)\n        avg_time = sum(r['time'] for r in results) / len(results)\n        print(f\"   æ€»æ£€æµ‹æ•°: {total_detections}\")\n        print(f\"   å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.1f}s\")\n    \n    return success_count > 0\n\n# ä¸»æ‰§è¡Œæµç¨‹ - é˜²å´©æºƒç‰ˆæœ¬\nprint(\"ğŸš€ å¯åŠ¨é˜²å´©æºƒGPUæ¨ç†ç³»ç»Ÿ...\")\n\n# åˆå§‹æ·±åº¦æ¸…ç†\ncomprehensive_cleanup()\n\n# ç³»ç»Ÿç¨³å®šæ€§æ£€æŸ¥\nstability_status = check_system_stability()\n\nif stability_status in ['critical', 'error']:\n    print(\"âŒ ç³»ç»ŸçŠ¶æ€ä¸ç¨³å®šï¼Œæ— æ³•å®‰å…¨è¿›è¡Œæ¨ç†\")\n    print(\"å»ºè®®é‡å¯å†…æ ¸æˆ–å‡å°‘å…¶ä»–ç¨‹åºçš„èµ„æºå ç”¨\")\nelse:\n    # GPUå®‰å…¨æµ‹è¯•\n    gpu_safe = safe_gpu_test()\n    \n    if not gpu_safe:\n        print(\"âŒ GPUæµ‹è¯•å¤±è´¥ï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼\")\n    \n    # æŸ¥æ‰¾å¯ç”¨æ¨¡å‹\n    model_candidates = [\n        'runs/train/tree_detection_light/weights/best.pt',\n        'runs/train/tree_detection/weights/best.pt', \n        'best_fixed.pt',\n        'yolov5s.pt'\n    ]\n    \n    selected_model = None\n    for model_path in model_candidates:\n        if os.path.exists(model_path):\n            model_size = os.path.getsize(model_path) / (1024 * 1024)\n            print(f\"ğŸ¯ æ‰¾åˆ°æ¨¡å‹: {model_path} ({model_size:.1f}MB)\")\n            selected_model = model_path\n            break\n    \n    if not selected_model:\n        print(\"âŒ æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹\")\n    else:\n        # æŸ¥æ‰¾æµ‹è¯•å›¾åƒ\n        test_dirs = [\n            'data/processed/images/test',\n            'data/processed/images/val',\n            'data/processed/images/train'\n        ]\n        \n        test_images_dir = None\n        for test_dir in test_dirs:\n            if os.path.exists(test_dir):\n                img_count = len(list(Path(test_dir).glob('*.jpg')))\n                if img_count > 0:\n                    test_images_dir = test_dir\n                    print(f\"ğŸ“ ä½¿ç”¨æµ‹è¯•å›¾åƒ: {test_dir} ({img_count}å¼ )\")\n                    break\n        \n        if test_images_dir:\n            # æ‰§è¡Œæ¸è¿›å¼æ¨ç†æµ‹è¯•\n            if progressive_inference_test(selected_model, test_images_dir):\n                print(\"âœ… é˜²å´©æºƒæ¨ç†æµ‹è¯•æˆåŠŸå®Œæˆ\")\n            else:\n                print(\"âŒ æ¨ç†æµ‹è¯•å¤±è´¥\")\n        else:\n            print(\"âŒ æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒ\")\n\n# æœ€ç»ˆç³»ç»Ÿæ¸…ç†\nprint(\"\\nğŸ§¹ æ‰§è¡Œæœ€ç»ˆç³»ç»Ÿæ¸…ç†...\")\ncomprehensive_cleanup()\n\n# æœ€ç»ˆçŠ¶æ€æ£€æŸ¥\nfinal_status = check_system_stability()\nprint(f\"ğŸ“‹ æœ€ç»ˆç³»ç»ŸçŠ¶æ€: {final_status}\")\nprint(\"âœ… é˜²å´©æºƒæ¨ç†ç³»ç»Ÿè¿è¡Œå®Œæˆ\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 10. YOLOv5æ¨¡å‹è¯„ä¼°å’Œæ€§èƒ½åˆ†æ - YOLOv5 Model Evaluation and Performance Analysis\ndef evaluate_yolov5_performance(model_path, test_images):\n    \"\"\"\n    è¯„ä¼°YOLOv5æ¨¡å‹æ€§èƒ½\n    Evaluate YOLOv5 model performance\n    \"\"\"\n    print(\"ğŸ“Š å¼€å§‹YOLOv5æ¨¡å‹æ€§èƒ½è¯„ä¼° Starting YOLOv5 model performance evaluation...\")\n    \n    total_images = len(test_images)\n    total_detections = 0\n    confidence_scores = []\n    processing_times = []\n    \n    # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜å‚¨è¯„ä¼°ç»“æœ Create temporary directory for evaluation results\n    eval_dir = 'temp_eval'\n    os.makedirs(eval_dir, exist_ok=True)\n    \n    print(f\"ğŸ” è¯„ä¼° {total_images} å¼ å›¾åƒ Evaluating {total_images} images...\")\n    \n    # å¯¹æ¯å¼ å›¾åƒè¿›è¡Œæ£€æµ‹ Detect on each image\n    for i, img_path in enumerate(test_images):\n        try:\n            import time\n            start_time = time.time()\n            \n            # æ„å»ºæ£€æµ‹å‘½ä»¤ Build detection command\n            detect_command = [\n                sys.executable, 'yolov5/detect.py',\n                '--weights', model_path,\n                '--source', str(img_path),\n                '--project', eval_dir,\n                '--name', f'eval_{i}',\n                '--save-txt',  # ä¿å­˜æ£€æµ‹ç»“æœ save detection results\n                '--save-conf', # ä¿å­˜ç½®ä¿¡åº¦ save confidence scores\n                '--exist-ok',\n                '--nosave'     # ä¸ä¿å­˜å›¾åƒï¼Œåªè¦æ–‡æœ¬ç»“æœ don't save images, only text results\n            ]\n            \n            # è¿è¡Œæ£€æµ‹ Run detection\n            result = subprocess.run(detect_command, \n                                  capture_output=True, \n                                  text=True, \n                                  cwd='.')\n            \n            processing_time = time.time() - start_time\n            processing_times.append(processing_time)\n            \n            if result.returncode == 0:\n                # è¯»å–æ£€æµ‹ç»“æœ Read detection results\n                result_dir = Path(eval_dir) / f'eval_{i}' / 'labels'\n                if result_dir.exists():\n                    for label_file in result_dir.glob('*.txt'):\n                        with open(label_file, 'r') as f:\n                            lines = f.readlines()\n                            for line in lines:\n                                if line.strip():\n                                    parts = line.strip().split()\n                                    if len(parts) >= 6:  # class x y w h conf\n                                        confidence = float(parts[5])\n                                        confidence_scores.append(confidence)\n                                        total_detections += 1\n                                        \n            if (i + 1) % 10 == 0 or i == total_images - 1:\n                print(f\"   å·²å¤„ç† {i+1}/{total_images} å¼ å›¾åƒ Processed {i+1}/{total_images} images\")\n                \n        except Exception as e:\n            print(f\"è¯„ä¼°å›¾åƒ {i+1} æ—¶å‡ºé”™ Error evaluating image {i+1}: {e}\")\n            continue\n    \n    # æ¸…ç†ä¸´æ—¶ç›®å½• Clean up temporary directory\n    import shutil\n    if os.path.exists(eval_dir):\n        shutil.rmtree(eval_dir)\n    \n    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ Calculate statistics\n    avg_detections_per_image = total_detections / total_images if total_images > 0 else 0\n    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n    max_confidence = np.max(confidence_scores) if confidence_scores else 0\n    min_confidence = np.min(confidence_scores) if confidence_scores else 0\n    avg_processing_time = np.mean(processing_times) if processing_times else 0\n    \n    print(\"\\nğŸ“ˆ YOLOv5æ¨¡å‹æ€§èƒ½ç»Ÿè®¡ YOLOv5 Model Performance Statistics\")\n    print(\"=\" * 60)\n    print(f\"æ€»æµ‹è¯•å›¾åƒæ•°é‡ Total test images: {total_images}\")\n    print(f\"æ€»æ£€æµ‹æ•°é‡ Total detections: {total_detections}\")\n    print(f\"å¹³å‡æ¯å¼ å›¾åƒæ£€æµ‹æ•°é‡ Average detections per image: {avg_detections_per_image:.2f}\")\n    print(f\"å¹³å‡ç½®ä¿¡åº¦ Average confidence: {avg_confidence:.3f}\")\n    print(f\"æœ€é«˜ç½®ä¿¡åº¦ Maximum confidence: {max_confidence:.3f}\")\n    print(f\"æœ€ä½ç½®ä¿¡åº¦ Minimum confidence: {min_confidence:.3f}\")\n    print(f\"å¹³å‡å¤„ç†æ—¶é—´ Average processing time: {avg_processing_time:.3f} ç§’/å¼  seconds per image\")\n    \n    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ Create visualization charts\n    if confidence_scores or processing_times:\n        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n        \n        # ç½®ä¿¡åº¦åˆ†å¸ƒå›¾ Confidence distribution\n        if confidence_scores:\n            axes[0].hist(confidence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n            axes[0].set_title('ğŸ¯ æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ Detection Confidence Distribution', fontsize=12, fontweight='bold')\n            axes[0].set_xlabel('ç½®ä¿¡åº¦ Confidence Score')\n            axes[0].set_ylabel('é¢‘æ¬¡ Frequency')\n            axes[0].grid(True, alpha=0.3)\n            \n            # æ·»åŠ ç»Ÿè®¡çº¿ Add statistical lines\n            axes[0].axvline(avg_confidence, color='red', linestyle='--', \n                          label=f'å¹³å‡å€¼ Mean: {avg_confidence:.3f}')\n            axes[0].legend()\n        else:\n            axes[0].text(0.5, 0.5, 'æ— ç½®ä¿¡åº¦æ•°æ®\\nNo Confidence Data', \n                       ha='center', va='center', transform=axes[0].transAxes)\n        \n        # å¤„ç†æ—¶é—´åˆ†å¸ƒå›¾ Processing time distribution\n        if processing_times:\n            axes[1].hist(processing_times, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n            axes[1].set_title('â±ï¸ å¤„ç†æ—¶é—´åˆ†å¸ƒ Processing Time Distribution', fontsize=12, fontweight='bold')\n            axes[1].set_xlabel('å¤„ç†æ—¶é—´ Processing Time (seconds)')\n            axes[1].set_ylabel('é¢‘æ¬¡ Frequency')\n            axes[1].grid(True, alpha=0.3)\n            \n            # æ·»åŠ ç»Ÿè®¡çº¿ Add statistical lines\n            axes[1].axvline(avg_processing_time, color='red', linestyle='--', \n                          label=f'å¹³å‡å€¼ Mean: {avg_processing_time:.3f}s')\n            axes[1].legend()\n        else:\n            axes[1].text(0.5, 0.5, 'æ— å¤„ç†æ—¶é—´æ•°æ®\\nNo Processing Time Data', \n                       ha='center', va='center', transform=axes[1].transAxes)\n        \n        plt.tight_layout()\n        plt.show()\n    \n    return {\n        'total_images': total_images,\n        'total_detections': total_detections,\n        'avg_detections_per_image': avg_detections_per_image,\n        'confidence_scores': confidence_scores,\n        'avg_confidence': avg_confidence,\n        'processing_times': processing_times,\n        'avg_processing_time': avg_processing_time\n    }\n\ndef run_yolov5_validation(model_path, dataset_yaml):\n    \"\"\"\n    è¿è¡ŒYOLOv5å®˜æ–¹éªŒè¯è„šæœ¬\n    Run YOLOv5 official validation script\n    \"\"\"\n    print(\"ğŸ”¬ è¿è¡ŒYOLOv5å®˜æ–¹éªŒè¯ Running YOLOv5 official validation...\")\n    \n    try:\n        # æ„å»ºéªŒè¯å‘½ä»¤ Build validation command\n        val_command = [\n            sys.executable, 'yolov5/val.py',\n            '--weights', model_path,\n            '--data', dataset_yaml,\n            '--img', '640',\n            '--batch', '8',\n            '--conf', '0.001',  # ä½ç½®ä¿¡åº¦é˜ˆå€¼ä»¥è·å¾—æ›´å¤šæ£€æµ‹ low confidence threshold for more detections\n            '--iou', '0.6',     # IoUé˜ˆå€¼ IoU threshold\n            '--task', 'val',\n            '--device', '0' if torch.cuda.is_available() else 'cpu',\n            '--save-txt',\n            '--save-conf',\n            '--project', 'runs/val',\n            '--name', 'tree_detection_val',\n            '--exist-ok'\n        ]\n        \n        print(f\"éªŒè¯å‘½ä»¤ Validation command: {' '.join(val_command)}\")\n        \n        # è¿è¡ŒéªŒè¯ Run validation\n        result = subprocess.run(val_command, \n                              capture_output=True, \n                              text=True, \n                              cwd='.')\n        \n        if result.returncode == 0:\n            print(\"âœ… YOLOv5å®˜æ–¹éªŒè¯å®Œæˆ YOLOv5 official validation completed!\")\n            \n            # æ˜¾ç¤ºéªŒè¯è¾“å‡ºçš„å…³é”®ä¿¡æ¯ Show key information from validation output\n            output_lines = result.stdout.split('\\n')\n            print(\"\\nğŸ“Š éªŒè¯ç»“æœæ‘˜è¦ Validation Results Summary:\")\n            print(\"-\" * 50)\n            \n            for line in output_lines:\n                if any(keyword in line.lower() for keyword in ['precision', 'recall', 'map', 'f1']):\n                    print(f\"   {line.strip()}\")\n            \n            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æœæ–‡ä»¶ Check if result files were generated\n            val_results_dir = Path('runs/val/tree_detection_val')\n            if val_results_dir.exists():\n                print(f\"\\nğŸ“ éªŒè¯ç»“æœä¿å­˜åœ¨ Validation results saved in: {val_results_dir}\")\n                \n                # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶ List generated files\n                result_files = list(val_results_dir.glob('*'))\n                for f in result_files:\n                    if f.is_file():\n                        print(f\"   {f.name}\")\n            \n        else:\n            print(f\"âŒ YOLOv5éªŒè¯å¤±è´¥ YOLOv5 validation failed\")\n            print(f\"é”™è¯¯è¾“å‡º Error output: {result.stderr}\")\n            \n    except Exception as e:\n        print(f\"âŒ éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ Error during validation: {e}\")\n\n# å¦‚æœæœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿›è¡Œæ€§èƒ½è¯„ä¼° If trained model exists, perform performance evaluation\nif 'model_path' in locals() and model_path is not None and os.path.exists(model_path):\n    print(f\"ğŸ¯ ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ€§èƒ½è¯„ä¼° Using model for performance evaluation: {model_path}\")\n    \n    # è·å–æµ‹è¯•å›¾åƒ Get test images\n    test_image_dir = Path('data/processed/images/test')\n    eval_images = []\n    \n    if test_image_dir.exists():\n        eval_images = list(test_image_dir.glob('*.jpg'))\n    \n    # å¦‚æœæ²¡æœ‰å¤„ç†è¿‡çš„æµ‹è¯•å›¾åƒï¼Œä½¿ç”¨åŸå§‹è¯„ä¼°å›¾åƒ If no processed test images, use original evaluation images\n    if not eval_images:\n        eval_rgb_dir = Path('data/raw/evaluation/RGB')\n        if eval_rgb_dir.exists():\n            eval_images = list(eval_rgb_dir.glob('*.tif'))[:20]  # é™åˆ¶ä¸º20å¼ å›¾åƒ\n            print(f\"ğŸ“¸ ä½¿ç”¨åŸå§‹è¯„ä¼°å›¾åƒ Using original evaluation images: {len(eval_images)}\")\n    \n    if eval_images:\n        # è¿›è¡Œæ€§èƒ½è¯„ä¼° Perform performance evaluation\n        performance_stats = evaluate_yolov5_performance(model_path, eval_images)\n        \n        # è¿è¡Œå®˜æ–¹éªŒè¯ï¼ˆå¦‚æœæ•°æ®é›†é…ç½®å­˜åœ¨ï¼‰Run official validation if dataset config exists\n        if 'dataset_yaml_path' in locals() and os.path.exists(dataset_yaml_path):\n            run_yolov5_validation(model_path, dataset_yaml_path)\n        \n    else:\n        print(\"âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒï¼Œè·³è¿‡æ€§èƒ½è¯„ä¼° No test images found, skipping performance evaluation\")\n        \nelse:\n    print(\"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡æ€§èƒ½è¯„ä¼° No trained model found, skipping performance evaluation\")\n    print(\"è¯·å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒ Please complete model training first\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ é¡¹ç›®æ€»ç»“ Project Summary\n",
    "\n",
    "### å®Œæˆçš„åŠŸèƒ½ Completed Features\n",
    "1. âœ… **ç¯å¢ƒæ£€æµ‹** - è‡ªåŠ¨æ£€æµ‹GPUå’ŒColabç¯å¢ƒ\n",
    "2. âœ… **æ•°æ®ä¸‹è½½** - è‡ªåŠ¨ä¸‹è½½NeonTreeEvaluationæ•°æ®é›†\n",
    "3. âœ… **æ•°æ®è½¬æ¢** - å°†NeonTreeæ ¼å¼è½¬æ¢ä¸ºYOLOæ ¼å¼\n",
    "4. âœ… **æ¨¡å‹è®­ç»ƒ** - ä½¿ç”¨YOLOv5è¿›è¡Œæ ‘æœ¨æ£€æµ‹è®­ç»ƒ\n",
    "5. âœ… **ç»“æœæ¨ç†** - å¯¹æµ‹è¯•å›¾åƒè¿›è¡Œæ ‘æœ¨æ£€æµ‹\n",
    "6. âœ… **ç»“æœå¯è§†åŒ–** - æ˜¾ç¤ºæ£€æµ‹ç»“æœå’Œæ€§èƒ½ç»Ÿè®¡\n",
    "\n",
    "### ä¸»è¦å‚æ•°è¯´æ˜ Key Parameter Explanations\n",
    "- **è®­ç»ƒè½®æ•° Epochs**: 50è½®ï¼ˆå¯æ ¹æ®æ•ˆæœè°ƒæ•´ï¼‰\n",
    "- **æ‰¹æ¬¡å¤§å° Batch Size**: 16ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼‰\n",
    "- **å›¾åƒå°ºå¯¸ Image Size**: 640x640åƒç´ \n",
    "- **å­¦ä¹ ç‡ Learning Rate**: 0.001ï¼ˆAdamWä¼˜åŒ–å™¨ï¼‰\n",
    "- **æ•°æ®åˆ†å‰² Data Split**: 70%è®­ç»ƒ/20%éªŒè¯/10%æµ‹è¯•\n",
    "\n",
    "### ä½¿ç”¨è¯´æ˜ Usage Instructions\n",
    "1. åœ¨Google Colabä¸­è¿è¡Œæ‰€æœ‰ä»£ç å•å…ƒ\n",
    "2. ç¡®ä¿GPUç¯å¢ƒå¯ç”¨ä»¥åŠ é€Ÿè®­ç»ƒ\n",
    "3. æ ¹æ®éœ€è¦è°ƒæ•´è®­ç»ƒå‚æ•°\n",
    "4. æŸ¥çœ‹resultsç›®å½•ä¸­çš„æ£€æµ‹ç»“æœå›¾åƒ\n",
    "\n",
    "### æ•…éšœæ’é™¤ Troubleshooting\n",
    "- å¦‚æœå†…å­˜ä¸è¶³ï¼Œå‡å°batch_sizeå‚æ•°\n",
    "- å¦‚æœè®­ç»ƒæ—¶é—´è¿‡é•¿ï¼Œå‡å°‘epochsæ•°é‡\n",
    "- å¦‚æœæ£€æµ‹æ•ˆæœä¸ä½³ï¼Œå°è¯•å¢åŠ è®­ç»ƒè½®æ•°\n",
    "- ç¡®ä¿æ•°æ®é›†æ­£ç¡®ä¸‹è½½å’Œè§£å‹\n",
    "\n",
    "### è¿›ä¸€æ­¥æ”¹è¿› Further Improvements\n",
    "- æ•°æ®å¢å¼ºæŠ€æœ¯æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›\n",
    "- è¶…å‚æ•°è°ƒä¼˜ä¼˜åŒ–æ¨¡å‹æ€§èƒ½\n",
    "- å¤šå°ºåº¦è®­ç»ƒæé«˜æ£€æµ‹ç²¾åº¦\n",
    "- æ¨¡å‹é›†æˆæå‡æ•´ä½“æ•ˆæœ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}