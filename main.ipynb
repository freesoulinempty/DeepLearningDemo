{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ² æ ‘æœ¨æ£€æµ‹é¡¹ç›® - Tree Detection Project\n",
    "\n",
    "## é¡¹ç›®æ¦‚è¿° Project Overview\n",
    "\n",
    "æœ¬é¡¹ç›®ä½¿ç”¨YOLOv5å’ŒPyTorchå®ç°åŸºäºèˆªç©ºå½±åƒçš„æ ‘æœ¨æ£€æµ‹ä»»åŠ¡  \n",
    "This project implements tree detection from aerial imagery using YOLOv5 and PyTorch\n",
    "\n",
    "**æ•°æ®é›† Dataset**: NeonTreeEvaluation Benchmark  \n",
    "**ç›®æ ‡ Goal**: æ£€æµ‹èˆªç©ºæ­£å°„å½±åƒä¸­çš„æ ‘æœ¨ - Detect trees in aerial orthoimagery  \n",
    "**å¹³å° Platform**: Google Colab (è‡ªåŠ¨GPUæ£€æµ‹ - Auto GPU detection)\n",
    "\n",
    "## æŠ€æœ¯æ ˆ Tech Stack\n",
    "- **æ·±åº¦å­¦ä¹ æ¡†æ¶ Deep Learning**: PyTorch + YOLOv5\n",
    "- **æ•°æ®å¤„ç† Data Processing**: OpenCV, PIL, pandas\n",
    "- **å¯è§†åŒ– Visualization**: matplotlib, seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# 1. ç¯å¢ƒæ£€æµ‹å’ŒåŸºç¡€è®¾ç½® - Environment Detection and Basic Setup\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import platform\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸš€ ç¯å¢ƒæ£€æµ‹ Environment Detection\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥Pythonç‰ˆæœ¬ Check Python version\n",
    "print(f\"Pythonç‰ˆæœ¬ Python Version: {sys.version}\")\n",
    "\n",
    "# æ£€æŸ¥æ“ä½œç³»ç»Ÿ Check OS\n",
    "print(f\"æ“ä½œç³»ç»Ÿ Operating System: {platform.system()}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦åœ¨Colabç¯å¢ƒ Check if running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… è¿è¡Œç¯å¢ƒ: Google Colab - Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"âŒ è¿è¡Œç¯å¢ƒ: æœ¬åœ°ç¯å¢ƒ - Running locally\")\n",
    "\n",
    "# æ£€æŸ¥GPUå¯ç”¨æ€§ Check GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPUå¯ç”¨ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDAç‰ˆæœ¬ CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   GPUæ•°é‡ GPU Count: {torch.cuda.device_count()}\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"âš ï¸  GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU - GPU not available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"PyTorchç‰ˆæœ¬ PyTorch Version: {torch.__version__}\")\n",
    "print(f\"è®¾å¤‡ Device: {device}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. å®‰è£…å¿…è¦ä¾èµ– - Install Required Dependencies\n",
    "print(\"ğŸ“¦ å®‰è£…ä¾èµ–åŒ… Installing Dependencies...\")\n",
    "\n",
    "# å®‰è£…YOLOv5å’Œç›¸å…³ä¾èµ– Install YOLOv5 and dependencies\n",
    "!pip install -q ultralytics\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q Pillow\n",
    "!pip install -q matplotlib\n",
    "!pip install -q seaborn\n",
    "!pip install -q pandas\n",
    "!pip install -q tqdm\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q PyYAML\n",
    "\n",
    "# å¦‚æœåœ¨Colabç¯å¢ƒï¼ŒæŒ‚è½½Google Drive (å¯é€‰)\n",
    "# Mount Google Drive in Colab (optional)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    # drive.mount('/content/drive')  # å–æ¶ˆæ³¨é‡Šä»¥æŒ‚è½½Drive - Uncomment to mount Drive\n",
    "\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. å¯¼å…¥æ‰€éœ€åº“ - Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import json\n",
    "import zipfile\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“æ”¯æŒ Set matplotlib Chinese font support\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"âœ… åº“å¯¼å…¥å®Œæˆ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. æ•°æ®ä¸‹è½½å’Œè§£å‹ - Data Download and Extraction\n",
    "def download_file(url, filename):\n",
    "    \"\"\"\n",
    "    ä¸‹è½½æ–‡ä»¶çš„å‡½æ•° Function to download files\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ”„ å¼€å§‹ä¸‹è½½ Starting download: {filename}\")\n",
    "    \n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filename, 'wb') as file, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    \n",
    "    print(f\"âœ… ä¸‹è½½å®Œæˆ Download completed: {filename}\")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®ç›®å½• Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# æ•°æ®é›†URL Dataset URLs\n",
    "DATASET_URL = \"https://zenodo.org/records/5914554/files/evaluation.zip?download=1\"\n",
    "ANNOTATIONS_URL = \"https://zenodo.org/records/5914554/files/annotations.zip?download=1\"\n",
    "\n",
    "# ä¸‹è½½æ•°æ®é›† Download datasets\n",
    "if not os.path.exists('data/raw/evaluation.zip'):\n",
    "    download_file(DATASET_URL, 'data/raw/evaluation.zip')\n",
    "else:\n",
    "    print(\"âœ… è¯„ä¼°æ•°æ®é›†å·²å­˜åœ¨ Evaluation dataset already exists\")\n",
    "\n",
    "if not os.path.exists('data/raw/annotations.zip'):\n",
    "    download_file(ANNOTATIONS_URL, 'data/raw/annotations.zip')\n",
    "else:\n",
    "    print(\"âœ… æ ‡æ³¨æ•°æ®å·²å­˜åœ¨ Annotations already exist\")\n",
    "\n",
    "print(\"ğŸ“ æ•°æ®ä¸‹è½½å®Œæˆ Data download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. è§£å‹æ•°æ®é›† - Extract Datasets\n",
    "def extract_zip(zip_path, extract_to):\n",
    "    \"\"\"\n",
    "    è§£å‹ZIPæ–‡ä»¶çš„å‡½æ•° Function to extract ZIP files\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“¦ è§£å‹æ–‡ä»¶ Extracting: {zip_path}\")\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    print(f\"âœ… è§£å‹å®Œæˆ Extraction completed: {extract_to}\")\n",
    "\n",
    "# è§£å‹è¯„ä¼°æ•°æ®é›† Extract evaluation dataset\n",
    "if not os.path.exists('data/raw/evaluation'):\n",
    "    extract_zip('data/raw/evaluation.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"âœ… è¯„ä¼°æ•°æ®é›†å·²è§£å‹ Evaluation dataset already extracted\")\n",
    "\n",
    "# è§£å‹æ ‡æ³¨æ•°æ® Extract annotations\n",
    "if not os.path.exists('data/raw/annotations'):\n",
    "    extract_zip('data/raw/annotations.zip', 'data/raw/')\n",
    "else:\n",
    "    print(\"âœ… æ ‡æ³¨æ•°æ®å·²è§£å‹ Annotations already extracted\")\n",
    "\n",
    "# æŸ¥çœ‹æ•°æ®ç»“æ„ Explore data structure\n",
    "print(\"\\nğŸ“Š æ•°æ®ç»“æ„åˆ†æ Data Structure Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æ£€æŸ¥è¯„ä¼°æ•°æ®é›†ç»“æ„ Check evaluation dataset structure\n",
    "eval_path = Path('data/raw/evaluation')\n",
    "if eval_path.exists():\n",
    "    print(f\"è¯„ä¼°æ•°æ®é›†è·¯å¾„ Evaluation dataset path: {eval_path}\")\n",
    "    subdirs = [d for d in eval_path.iterdir() if d.is_dir()]\n",
    "    print(f\"å­ç›®å½•æ•°é‡ Number of subdirectories: {len(subdirs)}\")\n",
    "    for subdir in subdirs[:5]:  # æ˜¾ç¤ºå‰5ä¸ªå­ç›®å½• Show first 5 subdirectories\n",
    "        print(f\"  - {subdir.name}\")\n",
    "    if len(subdirs) > 5:\n",
    "        print(f\"  ... è¿˜æœ‰ {len(subdirs)-5} ä¸ªç›®å½• and {len(subdirs)-5} more directories\")\n",
    "\n",
    "# æ£€æŸ¥æ ‡æ³¨æ•°æ®ç»“æ„ Check annotations structure\n",
    "ann_path = Path('data/raw/annotations')\n",
    "if ann_path.exists():\n",
    "    print(f\"\\næ ‡æ³¨æ•°æ®è·¯å¾„ Annotations path: {ann_path}\")\n",
    "    ann_files = list(ann_path.glob('*.csv'))\n",
    "    print(f\"CSVæ ‡æ³¨æ–‡ä»¶æ•°é‡ Number of CSV annotation files: {len(ann_files)}\")\n",
    "    for ann_file in ann_files[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ ‡æ³¨æ–‡ä»¶ Show first 3 annotation files\n",
    "        print(f\"  - {ann_file.name}\")\n",
    "\n",
    "print(\"\\nâœ… æ•°æ®è§£å‹å’Œç»“æ„åˆ†æå®Œæˆ Data extraction and structure analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. æ•°æ®æ ¼å¼è½¬æ¢ - Data Format Conversion (NeonTree -> YOLO) - ä½¿ç”¨çœŸå®å›¾åƒ\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "class NeonTreeToYOLO:\n",
    "    \"\"\"\n",
    "    å°†NeonTreeæ•°æ®é›†è½¬æ¢ä¸ºYOLOæ ¼å¼çš„ç±»\n",
    "    Class to convert NeonTree dataset to YOLO format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, annotations_root, evaluation_root, output_root):\n",
    "        self.annotations_root = Path(annotations_root)\n",
    "        self.evaluation_root = Path(evaluation_root)\n",
    "        self.output_root = Path(output_root)\n",
    "        \n",
    "        # åˆ›å»ºè¾“å‡ºç›®å½• Create output directories\n",
    "        self.create_yolo_structure()\n",
    "        \n",
    "        # åˆ›å»ºå›¾åƒæ–‡ä»¶åæ˜ å°„ Create image filename mapping\n",
    "        self.create_image_mapping()\n",
    "    \n",
    "    def create_yolo_structure(self):\n",
    "        \"\"\"åˆ›å»ºYOLOæ•°æ®é›†ç›®å½•ç»“æ„ Create YOLO dataset directory structure\"\"\"\n",
    "        folders = [\n",
    "            'images/train', 'images/val', 'images/test',\n",
    "            'labels/train', 'labels/val', 'labels/test'\n",
    "        ]\n",
    "        \n",
    "        for folder in folders:\n",
    "            (self.output_root / folder).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"âœ… YOLOç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ YOLO directory structure created\")\n",
    "    \n",
    "    def create_image_mapping(self):\n",
    "        \"\"\"\n",
    "        åˆ›å»ºå›¾åƒæ–‡ä»¶ååˆ°è·¯å¾„çš„æ˜ å°„\n",
    "        Create mapping from image filenames to paths\n",
    "        \"\"\"\n",
    "        self.image_mapping = {}\n",
    "        \n",
    "        # æ£€æŸ¥ evaluation/RGB ç›®å½•\n",
    "        rgb_dir = self.evaluation_root / 'RGB'\n",
    "        if rgb_dir.exists():\n",
    "            for img_file in rgb_dir.glob('*.tif'):\n",
    "                # å»æ‰æ‰©å±•åä½œä¸ºkey\n",
    "                base_name = img_file.stem\n",
    "                self.image_mapping[base_name] = img_file\n",
    "                \n",
    "                # åŒæ—¶æ·»åŠ ä¸å¸¦æ‰©å±•åçš„æ˜ å°„\n",
    "                if base_name.endswith('.tif'):\n",
    "                    base_name_no_ext = base_name[:-4]\n",
    "                    self.image_mapping[base_name_no_ext] = img_file\n",
    "        \n",
    "        print(f\"ğŸ“‹ åˆ›å»ºå›¾åƒæ˜ å°„ Created image mapping with {len(self.image_mapping)} entries\")\n",
    "    \n",
    "    def find_matching_image(self, xml_filename):\n",
    "        \"\"\"\n",
    "        æ ¹æ®XMLæ–‡ä»¶åæŸ¥æ‰¾å¯¹åº”çš„RGBå›¾åƒæ–‡ä»¶\n",
    "        Find matching RGB image file based on XML filename\n",
    "        \"\"\"\n",
    "        # ä»XMLæ–‡ä»¶åä¸­æå–åŸºç¡€åç§°\n",
    "        base_name = xml_filename.replace('.xml', '')\n",
    "        \n",
    "        # ç›´æ¥åŒ¹é…\n",
    "        if base_name in self.image_mapping:\n",
    "            return self.image_mapping[base_name]\n",
    "        \n",
    "        # å°è¯•æ¨¡ç³ŠåŒ¹é… - æŸ¥æ‰¾åŒ…å«base_nameçš„å›¾åƒæ–‡ä»¶\n",
    "        for img_name, img_path in self.image_mapping.items():\n",
    "            # æ£€æŸ¥æ˜¯å¦åŒ¹é…ï¼ˆå»æ‰å¹´ä»½ç­‰å˜åŒ–éƒ¨åˆ†ï¼‰\n",
    "            if self.files_match(base_name, img_name):\n",
    "                return img_path\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def files_match(self, xml_name, img_name):\n",
    "        \"\"\"\n",
    "        åˆ¤æ–­XMLæ–‡ä»¶åå’Œå›¾åƒæ–‡ä»¶åæ˜¯å¦åŒ¹é…\n",
    "        Check if XML filename matches image filename\n",
    "        \"\"\"\n",
    "        # ç§»é™¤å¸¸è§çš„åç¼€å’Œå‰ç¼€\n",
    "        xml_clean = xml_name.lower()\n",
    "        img_clean = img_name.lower()\n",
    "        \n",
    "        # æå–æ ¸å¿ƒç«™ç‚¹ä»£ç \n",
    "        xml_parts = xml_clean.split('_')\n",
    "        img_parts = img_clean.split('_')\n",
    "        \n",
    "        # å¦‚æœç«™ç‚¹ä»£ç åŒ¹é…\n",
    "        if len(xml_parts) >= 2 and len(img_parts) >= 2:\n",
    "            xml_site = xml_parts[0]\n",
    "            img_site = img_parts[0]\n",
    "            \n",
    "            if xml_site == img_site:\n",
    "                # è¿›ä¸€æ­¥æ£€æŸ¥ç¼–å·åŒ¹é…\n",
    "                if len(xml_parts) >= 3 and len(img_parts) >= 2:\n",
    "                    try:\n",
    "                        xml_num = xml_parts[1]\n",
    "                        img_num = img_parts[1]\n",
    "                        return xml_num == img_num\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸åŒçš„å…³é”®éƒ¨åˆ†\n",
    "        return any(part in img_clean for part in xml_parts if len(part) > 2)\n",
    "    \n",
    "    def parse_xml_annotation(self, xml_file):\n",
    "        \"\"\"\n",
    "        è§£æXMLæ ‡æ³¨æ–‡ä»¶\n",
    "        Parse XML annotation file\n",
    "        \"\"\"\n",
    "        try:\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # è·å–å›¾åƒä¿¡æ¯ Get image information\n",
    "            filename_elem = root.find('filename')\n",
    "            if filename_elem is not None:\n",
    "                filename = filename_elem.text\n",
    "            else:\n",
    "                filename = xml_file.stem + '.tif'  # é»˜è®¤æ‰©å±•å\n",
    "            \n",
    "            size = root.find('size')\n",
    "            width = int(size.find('width').text)\n",
    "            height = int(size.find('height').text)\n",
    "            \n",
    "            # è·å–æ‰€æœ‰æ ‘æœ¨å¯¹è±¡ Get all tree objects\n",
    "            objects = []\n",
    "            for obj in root.findall('object'):\n",
    "                name_elem = obj.find('name')\n",
    "                if name_elem is not None and name_elem.text.lower() == 'tree':\n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    xmin = int(float(bndbox.find('xmin').text))\n",
    "                    ymin = int(float(bndbox.find('ymin').text))\n",
    "                    xmax = int(float(bndbox.find('xmax').text))\n",
    "                    ymax = int(float(bndbox.find('ymax').text))\n",
    "                    \n",
    "                    # éªŒè¯è¾¹ç•Œæ¡†æœ‰æ•ˆæ€§ Validate bounding box\n",
    "                    if xmax > xmin and ymax > ymin:\n",
    "                        objects.append({\n",
    "                            'xmin': xmin,\n",
    "                            'ymin': ymin,\n",
    "                            'xmax': xmax,\n",
    "                            'ymax': ymax\n",
    "                        })\n",
    "            \n",
    "            return {\n",
    "                'filename': filename,\n",
    "                'width': width,\n",
    "                'height': height,\n",
    "                'objects': objects\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"è§£æXMLæ–‡ä»¶æ—¶å‡ºé”™ Error parsing XML file {xml_file}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def convert_bbox_to_yolo(self, bbox, img_width, img_height):\n",
    "        \"\"\"\n",
    "        å°†è¾¹ç•Œæ¡†åæ ‡è½¬æ¢ä¸ºYOLOæ ¼å¼\n",
    "        Convert bounding box coordinates to YOLO format\n",
    "        \n",
    "        YOLOæ ¼å¼: [class_id, x_center, y_center, width, height] (å½’ä¸€åŒ– normalized)\n",
    "        \"\"\"\n",
    "        xmin, ymin, xmax, ymax = bbox['xmin'], bbox['ymin'], bbox['xmax'], bbox['ymax']\n",
    "        \n",
    "        # è®¡ç®—ä¸­å¿ƒç‚¹å’Œå®½é«˜ Calculate center point and dimensions\n",
    "        x_center = (xmin + xmax) / 2.0\n",
    "        y_center = (ymin + ymax) / 2.0\n",
    "        width = xmax - xmin\n",
    "        height = ymax - ymin\n",
    "        \n",
    "        # å½’ä¸€åŒ– Normalize\n",
    "        x_center /= img_width\n",
    "        y_center /= img_height\n",
    "        width /= img_width\n",
    "        height /= img_height\n",
    "        \n",
    "        return [0, x_center, y_center, width, height]  # ç±»åˆ«IDä¸º0 (æ ‘æœ¨ tree)\n",
    "    \n",
    "    def process_annotations(self):\n",
    "        \"\"\"\n",
    "        å¤„ç†æ‰€æœ‰XMLæ ‡æ³¨æ–‡ä»¶å¹¶è½¬æ¢ä¸ºYOLOæ ¼å¼\n",
    "        Process all XML annotation files and convert to YOLO format\n",
    "        \"\"\"\n",
    "        xml_files = list(self.annotations_root.glob('*.xml'))\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        print(f\"ğŸ“ æ‰¾åˆ° {len(xml_files)} ä¸ªXMLæ ‡æ³¨æ–‡ä»¶ Found {len(xml_files)} XML annotation files\")\n",
    "        \n",
    "        for i, xml_file in enumerate(tqdm(xml_files, desc=\"å¤„ç†æ ‡æ³¨ Processing annotations\")):\n",
    "            try:\n",
    "                # è§£æXMLæ–‡ä»¶ Parse XML file\n",
    "                annotation_data = self.parse_xml_annotation(xml_file)\n",
    "                \n",
    "                if annotation_data is None or not annotation_data['objects']:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # æŸ¥æ‰¾å¯¹åº”çš„RGBå›¾åƒæ–‡ä»¶ Find corresponding RGB image file\n",
    "                img_path = self.find_matching_image(xml_file.name)\n",
    "                \n",
    "                if img_path is None:\n",
    "                    # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œä½†ä¸ä¸­æ–­å¤„ç† Print debug info but don't interrupt\n",
    "                    if processed_count < 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯\n",
    "                        print(f\"âš ï¸  æœªæ‰¾åˆ°å¯¹åº”å›¾åƒ No matching image for: {xml_file.name}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # è¯»å–çœŸå®å›¾åƒ Load real image\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is None:\n",
    "                    print(f\"âš ï¸  æ— æ³•è¯»å–å›¾åƒ Cannot read image: {img_path}\")\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # éªŒè¯å›¾åƒå°ºå¯¸ Verify image dimensions\n",
    "                actual_height, actual_width = img.shape[:2]\n",
    "                if actual_width != annotation_data['width'] or actual_height != annotation_data['height']:\n",
    "                    # ä½¿ç”¨å®é™…å›¾åƒå°ºå¯¸ Use actual image dimensions\n",
    "                    annotation_data['width'] = actual_width\n",
    "                    annotation_data['height'] = actual_height\n",
    "                \n",
    "                # ç¡®å®šæ•°æ®é›†åˆ†å‰² Determine dataset split\n",
    "                if i % 10 < 7:  # 70% è®­ç»ƒé›† training set\n",
    "                    split = 'train'\n",
    "                elif i % 10 < 9:  # 20% éªŒè¯é›† validation set\n",
    "                    split = 'val'\n",
    "                else:  # 10% æµ‹è¯•é›† test set\n",
    "                    split = 'test'\n",
    "                \n",
    "                # ä¿å­˜å›¾åƒ Save image\n",
    "                img_filename = f\"tree_{processed_count:06d}.jpg\"\n",
    "                img_save_path = self.output_root / 'images' / split / img_filename\n",
    "                cv2.imwrite(str(img_save_path), img)\n",
    "                \n",
    "                # ä¿å­˜YOLOæ ¼å¼æ ‡ç­¾ Save YOLO format labels\n",
    "                label_filename = f\"tree_{processed_count:06d}.txt\"\n",
    "                label_save_path = self.output_root / 'labels' / split / label_filename\n",
    "                \n",
    "                with open(label_save_path, 'w') as f:\n",
    "                    for obj in annotation_data['objects']:\n",
    "                        yolo_bbox = self.convert_bbox_to_yolo(\n",
    "                            obj, annotation_data['width'], annotation_data['height']\n",
    "                        )\n",
    "                        # YOLOæ ¼å¼: class_id x_center y_center width height (ä¿®å¤æ¢è¡Œç¬¦)\n",
    "                        f.write(f\"{yolo_bbox[0]} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f} {yolo_bbox[4]:.6f}\\n\")\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"å¤„ç†æ–‡ä»¶ {xml_file.name} æ—¶å‡ºé”™ Error processing file {xml_file.name}: {e}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "        \n",
    "        print(f\"âœ… å¤„ç†å®Œæˆ Processing completed: {processed_count} ä¸ªæ ·æœ¬ samples processed, {skipped_count} ä¸ªè·³è¿‡ skipped\")\n",
    "        return processed_count\n",
    "\n",
    "# æ‰§è¡Œæ•°æ®è½¬æ¢ Execute data conversion\n",
    "print(\"ğŸ”„ å¼€å§‹æ•°æ®æ ¼å¼è½¬æ¢ Starting data format conversion...\")\n",
    "print(\"ğŸ“‹ ä½¿ç”¨XMLæ ¼å¼çš„æ ‡æ³¨æ•°æ®å’ŒçœŸå®RGBå›¾åƒ Using XML annotations with real RGB images\")\n",
    "\n",
    "# åˆ›å»ºdataç›®å½• Create data directories\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "# æ£€æŸ¥annotationså’Œevaluationæ•°æ®æ˜¯å¦å­˜åœ¨ Check if annotations and evaluation data exist\n",
    "annotation_paths = ['data/raw/annotations', 'annotations', '/content/annotations']\n",
    "eval_paths = ['data/raw/evaluation', 'evaluation', '/content/evaluation']\n",
    "\n",
    "annotations_root = None\n",
    "eval_root = None\n",
    "\n",
    "# æŸ¥æ‰¾annotationsç›®å½• Find annotations directory\n",
    "for path in annotation_paths:\n",
    "    if Path(path).exists():\n",
    "        annotations_root = Path(path)\n",
    "        xml_count = len(list(annotations_root.glob('*.xml')))\n",
    "        if xml_count > 0:\n",
    "            print(f\"âœ… æ‰¾åˆ°annotationsæ•°æ®é›† Found annotations dataset at: {annotations_root} ({xml_count} XML files)\")\n",
    "            break\n",
    "\n",
    "if annotations_root is None:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°annotationsæ•°æ®é›†æˆ–XMLæ–‡ä»¶\")\n",
    "    print(\"ğŸ“ è¯·ç¡®ä¿annotationsç›®å½•å­˜åœ¨ä¸”åŒ…å«XMLæ–‡ä»¶\")\n",
    "\n",
    "# æŸ¥æ‰¾evaluationç›®å½• Find evaluation directory  \n",
    "for path in eval_paths:\n",
    "    if Path(path).exists():\n",
    "        eval_root = Path(path)\n",
    "        # æ£€æŸ¥RGBå­ç›®å½•\n",
    "        rgb_dir = eval_root / 'RGB'\n",
    "        if rgb_dir.exists():\n",
    "            img_count = len(list(rgb_dir.glob('*.tif')))\n",
    "            print(f\"âœ… æ‰¾åˆ°evaluationæ•°æ®é›† Found evaluation dataset at: {eval_root} ({img_count} images)\")\n",
    "            break\n",
    "\n",
    "if eval_root is None:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°evaluationæ•°æ®é›†ï¼Œè¯·ç¡®ä¿å·²ä¸‹è½½å¹¶è§£å‹evaluation.zip\")\n",
    "\n",
    "# åªæœ‰å½“ä¸¤ä¸ªæ•°æ®é›†éƒ½æ‰¾åˆ°æ—¶æ‰è¿›è¡Œè½¬æ¢ Only proceed if both datasets are found\n",
    "if annotations_root is not None and eval_root is not None:\n",
    "    # åˆå§‹åŒ–è½¬æ¢å™¨ Initialize converter\n",
    "    converter = NeonTreeToYOLO(annotations_root, eval_root, 'data/processed')\n",
    "    \n",
    "    # å¤„ç†æ ‡æ³¨æ•°æ® Process annotation data\n",
    "    processed_samples = converter.process_annotations()\n",
    "    \n",
    "    if processed_samples > 0:\n",
    "        print(f\"âœ… æ•°æ®è½¬æ¢å®Œæˆ Data conversion completed: {processed_samples} ä¸ªæ ·æœ¬ samples\")\n",
    "        \n",
    "        # æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ Show dataset statistics\n",
    "        train_images = len(list(Path('data/processed/images/train').glob('*.jpg')))\n",
    "        val_images = len(list(Path('data/processed/images/val').glob('*.jpg')))\n",
    "        test_images = len(list(Path('data/processed/images/test').glob('*.jpg')))\n",
    "        \n",
    "        print(f\"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ Dataset Statistics:\")\n",
    "        print(f\"   è®­ç»ƒé›† Training: {train_images} å¼ å›¾åƒ\")\n",
    "        print(f\"   éªŒè¯é›† Validation: {val_images} å¼ å›¾åƒ\")\n",
    "        print(f\"   æµ‹è¯•é›† Testing: {test_images} å¼ å›¾åƒ\")\n",
    "        print(f\"   æ€»è®¡ Total: {train_images + val_images + test_images} å¼ å›¾åƒ\")\n",
    "    else:\n",
    "        print(\"âŒ æœªèƒ½å¤„ç†ä»»ä½•æ ·æœ¬ No samples were processed\")\n",
    "        print(\"è¯·æ£€æŸ¥XMLæ–‡ä»¶å’Œå›¾åƒæ–‡ä»¶çš„åŒ¹é…å…³ç³»\")\n",
    "else:\n",
    "    print(\"âŒ ç¼ºå°‘å¿…è¦çš„æ•°æ®é›†ï¼Œæ— æ³•è¿›è¡Œè½¬æ¢\")\n",
    "    print(\"è¯·ç¡®ä¿ä»¥ä¸‹ç›®å½•å­˜åœ¨ï¼š\")\n",
    "    print(\"  - annotations ç›®å½•ï¼ˆåŒ…å«XMLæ–‡ä»¶ï¼‰\")\n",
    "    print(\"  - evaluation/RGB ç›®å½•ï¼ˆåŒ…å«.tifå›¾åƒæ–‡ä»¶ï¼‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. åˆ›å»ºYOLOé…ç½®æ–‡ä»¶ - Create YOLO Configuration Files (ä¿®å¤è·¯å¾„é—®é¢˜)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "def create_dataset_yaml():\n",
    "    \"\"\"\n",
    "    åˆ›å»ºYOLOæ•°æ®é›†é…ç½®æ–‡ä»¶\n",
    "    Create YOLO dataset configuration file\n",
    "    \"\"\"\n",
    "    # è·å–å½“å‰å·¥ä½œç›®å½• Get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    \n",
    "    # æ„å»ºç»å¯¹è·¯å¾„ Build absolute paths\n",
    "    processed_dir = os.path.join(current_dir, 'data/processed')\n",
    "    train_path = os.path.join(processed_dir, 'images/train')\n",
    "    val_path = os.path.join(processed_dir, 'images/val')\n",
    "    test_path = os.path.join(processed_dir, 'images/test')\n",
    "    \n",
    "    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ Check if paths exist\n",
    "    paths_info = {\n",
    "        'train': train_path,\n",
    "        'val': val_path,\n",
    "        'test': test_path\n",
    "    }\n",
    "    \n",
    "    print(\"ğŸ“‹ æ£€æŸ¥æ•°æ®é›†è·¯å¾„ Checking dataset paths:\")\n",
    "    existing_paths = {}\n",
    "    \n",
    "    for split, path in paths_info.items():\n",
    "        if os.path.exists(path):\n",
    "            img_count = len([f for f in os.listdir(path) if f.endswith('.jpg')])\n",
    "            print(f\"âœ… {split}: {path} ({img_count} å¼ å›¾åƒ images)\")\n",
    "            existing_paths[split] = path\n",
    "        else:\n",
    "            print(f\"âŒ {split}: {path} (è·¯å¾„ä¸å­˜åœ¨ path does not exist)\")\n",
    "    \n",
    "    # ç¡®ä¿è‡³å°‘æœ‰è®­ç»ƒé›†å­˜åœ¨ Ensure at least training set exists\n",
    "    if 'train' not in existing_paths:\n",
    "        print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°è®­ç»ƒé›† Error: Training set not found\")\n",
    "        return None\n",
    "    \n",
    "    # åˆ›å»ºæ•°æ®é›†é…ç½® Create dataset configuration\n",
    "    dataset_config = {\n",
    "        'path': processed_dir,  # æ•°æ®é›†æ ¹ç›®å½• dataset root dir\n",
    "        'train': 'images/train',  # ç›¸å¯¹äºpathçš„è®­ç»ƒå›¾åƒè·¯å¾„ train images relative to path\n",
    "        'val': 'images/val' if 'val' in existing_paths else 'images/train',  # éªŒè¯é›†ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨è®­ç»ƒé›†\n",
    "        'test': 'images/test' if 'test' in existing_paths else 'images/train',  # æµ‹è¯•é›†ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨è®­ç»ƒé›†\n",
    "        'nc': 1,                  # ç±»åˆ«æ•°é‡ number of classes\n",
    "        'names': ['tree']         # ç±»åˆ«åç§° class names\n",
    "    }\n",
    "    \n",
    "    # ç‰¹åˆ«å¤„ç†éªŒè¯é›†è·¯å¾„\n",
    "    if 'val' not in existing_paths:\n",
    "        print(\"âš ï¸  éªŒè¯é›†ä¸å­˜åœ¨ï¼Œä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºéªŒè¯é›† Validation set not found, using training set as validation\")\n",
    "        dataset_config['val'] = 'images/train'\n",
    "    \n",
    "    if 'test' not in existing_paths:\n",
    "        print(\"âš ï¸  æµ‹è¯•é›†ä¸å­˜åœ¨ï¼Œä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºæµ‹è¯•é›† Test set not found, using training set as test\")\n",
    "        dataset_config['test'] = 'images/train'\n",
    "    \n",
    "    # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    \n",
    "    # ä¿å­˜é…ç½®æ–‡ä»¶ Save configuration file\n",
    "    config_path = 'data/tree_dataset.yaml'\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "    \n",
    "    print(f\"âœ… YOLOæ•°æ®é›†é…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆ YOLO dataset configuration file created: {config_path}\")\n",
    "    \n",
    "    # éªŒè¯é…ç½®æ–‡ä»¶å†…å®¹\n",
    "    print(\"\\nğŸ” éªŒè¯é…ç½®æ–‡ä»¶å†…å®¹ Verifying configuration file:\")\n",
    "    for key, value in dataset_config.items():\n",
    "        if key in ['train', 'val', 'test']:\n",
    "            full_path = os.path.join(processed_dir, value)\n",
    "            exists = os.path.exists(full_path)\n",
    "            print(f\"   {key}: {value} -> {full_path} ({'âœ…' if exists else 'âŒ'})\")\n",
    "        else:\n",
    "            print(f\"   {key}: {value}\")\n",
    "    \n",
    "    return config_path\n",
    "\n",
    "def fix_data_structure():\n",
    "    \"\"\"\n",
    "    ä¿®å¤æ•°æ®ç»“æ„é—®é¢˜\n",
    "    Fix data structure issues\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ”§ æ£€æŸ¥å¹¶ä¿®å¤æ•°æ®ç»“æ„ Checking and fixing data structure...\")\n",
    "    \n",
    "    processed_dir = Path('data/processed')\n",
    "    \n",
    "    if not processed_dir.exists():\n",
    "        print(\"âŒ data/processed ç›®å½•ä¸å­˜åœ¨ data/processed directory does not exist\")\n",
    "        return False\n",
    "    \n",
    "    # æ£€æŸ¥å¿…è¦çš„ç›®å½•ç»“æ„\n",
    "    required_dirs = [\n",
    "        'images/train', 'images/val', 'images/test',\n",
    "        'labels/train', 'labels/val', 'labels/test'\n",
    "    ]\n",
    "    \n",
    "    missing_dirs = []\n",
    "    for dir_path in required_dirs:\n",
    "        full_path = processed_dir / dir_path\n",
    "        if not full_path.exists():\n",
    "            missing_dirs.append(dir_path)\n",
    "    \n",
    "    if missing_dirs:\n",
    "        print(f\"âš ï¸  ç¼ºå°‘ç›®å½• Missing directories: {missing_dirs}\")\n",
    "        \n",
    "        # å¦‚æœåªæ˜¯éªŒè¯é›†å’Œæµ‹è¯•é›†ç¼ºå¤±ï¼Œåˆ›å»ºå®ƒä»¬\n",
    "        for missing_dir in missing_dirs:\n",
    "            if 'val' in missing_dir or 'test' in missing_dir:\n",
    "                (processed_dir / missing_dir).mkdir(parents=True, exist_ok=True)\n",
    "                print(f\"âœ… åˆ›å»ºç›®å½• Created directory: {missing_dir}\")\n",
    "    \n",
    "    # æ£€æŸ¥è®­ç»ƒé›†æ˜¯å¦æœ‰æ•°æ®\n",
    "    train_images = list((processed_dir / 'images/train').glob('*.jpg'))\n",
    "    train_labels = list((processed_dir / 'labels/train').glob('*.txt'))\n",
    "    \n",
    "    print(f\"ğŸ“Š è®­ç»ƒé›†ç»Ÿè®¡ Training set statistics:\")\n",
    "    print(f\"   å›¾åƒæ–‡ä»¶ Image files: {len(train_images)}\")\n",
    "    print(f\"   æ ‡ç­¾æ–‡ä»¶ Label files: {len(train_labels)}\")\n",
    "    \n",
    "    if len(train_images) == 0:\n",
    "        print(\"âŒ è®­ç»ƒé›†ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œæ•°æ®è½¬æ¢ Training set is empty, please run data conversion first\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ä¿®å¤æ•°æ®ç»“æ„\n",
    "data_structure_ok = fix_data_structure()\n",
    "\n",
    "if data_structure_ok:\n",
    "    # åˆ›å»ºé…ç½®æ–‡ä»¶ Create configuration file\n",
    "    dataset_yaml_path = create_dataset_yaml()\n",
    "    \n",
    "    if dataset_yaml_path:\n",
    "        # æ˜¾ç¤ºé…ç½®æ–‡ä»¶å†…å®¹ Display configuration file content\n",
    "        with open(dataset_yaml_path, 'r') as f:\n",
    "            config_content = f.read()\n",
    "            print(\"\\nğŸ“‹ æ•°æ®é›†é…ç½®å†…å®¹ Dataset Configuration Content:\")\n",
    "            print(\"=\" * 40)\n",
    "            print(config_content)\n",
    "            print(\"=\" * 40)\n",
    "        \n",
    "        # æœ€ç»ˆéªŒè¯\n",
    "        print(\"\\nâœ… é…ç½®æ–‡ä»¶åˆ›å»ºå¹¶éªŒè¯å®Œæˆ Configuration file created and verified\")\n",
    "    else:\n",
    "        print(\"âŒ é…ç½®æ–‡ä»¶åˆ›å»ºå¤±è´¥ Failed to create configuration file\")\n",
    "else:\n",
    "    print(\"âŒ æ•°æ®ç»“æ„ä¿®å¤å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®è½¬æ¢æ­¥éª¤ Data structure fix failed, please check data conversion step\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 8. YOLOv5ä¸“ä¸šæ ‘æœ¨æ£€æµ‹è®­ç»ƒ - Professional YOLOv5 Tree Detection Training (ä¿®å¤å‚æ•°)\n\nimport subprocess\nimport sys\nimport yaml\nimport os\nimport torch\nimport gc\nfrom pathlib import Path\n\nprint(\"ğŸŒ² YOLOv5ä¸“ä¸šæ ‘æœ¨æ£€æµ‹è®­ç»ƒ\")\nprint(\"ğŸ’¡ åŸºäº2024å¹´æœ€æ–°YOLOv5æœ€ä½³å®è·µ\")\nprint(\"ğŸš€ å®Œå…¨æ”¾å¼ƒbest.ptï¼Œä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹ä»é›¶å¼€å§‹\")\nprint(\"ğŸ”§ ä¿®å¤è®­ç»ƒå‘½ä»¤å‚æ•°æ ¼å¼\")\nprint(\"=\" * 70)\n\ndef memory_cleanup():\n    \"\"\"å†…å­˜æ¸…ç†\"\"\"\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.synchronize()\n\ndef create_professional_dataset_config():\n    \"\"\"åˆ›å»ºä¸“ä¸šçš„æ•°æ®é›†é…ç½®\"\"\"\n    print(\"ğŸ“ åˆ›å»ºä¸“ä¸šYOLOv5æ•°æ®é›†é…ç½®...\")\n    \n    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„\n    dataset_path = os.path.join(os.getcwd(), 'data/processed')\n    if not os.path.exists(dataset_path):\n        print(f\"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}\")\n        return None\n    \n    # ä¸“ä¸šé…ç½® - éµå¾ªYOLOv5å®˜æ–¹æ ‡å‡†\n    professional_config = {\n        'path': dataset_path,\n        'train': 'images/train',\n        'val': 'images/train',  # ä½¿ç”¨è®­ç»ƒé›†ä½œä¸ºéªŒè¯é›†ï¼ˆå°æ•°æ®é›†æœ€ä½³å®è·µï¼‰\n        'test': 'images/test',\n        'nc': 1,\n        'names': ['tree']\n    }\n    \n    # æ£€æŸ¥è®­ç»ƒæ•°æ®\n    train_path = os.path.join(dataset_path, 'images/train')\n    \n    if os.path.exists(train_path):\n        train_count = len([f for f in os.listdir(train_path) if f.endswith('.jpg')])\n        print(f\"   è®­ç»ƒé›†å›¾åƒ: {train_count}\")\n        \n        if train_count == 0:\n            print(\"âŒ è®­ç»ƒæ•°æ®ä¸ºç©º\")\n            return None\n    else:\n        print(\"âŒ è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨\")\n        return None\n    \n    # ä¿å­˜ä¸“ä¸šé…ç½®\n    config_path = 'tree_detection_professional.yaml'\n    with open(config_path, 'w') as f:\n        yaml.dump(professional_config, f, default_flow_style=False)\n    \n    print(f\"âœ… ä¸“ä¸šé…ç½®æ–‡ä»¶: {config_path}\")\n    return config_path\n\n# æ‰§è¡Œå†…å­˜æ¸…ç†\nmemory_cleanup()\n\n# ç¡®ä¿YOLOv5ç¯å¢ƒ\nif not os.path.exists('yolov5'):\n    print(\"ğŸ“¦ å…‹éš†å®˜æ–¹YOLOv5ä»“åº“...\")\n    !git clone https://github.com/ultralytics/yolov5.git\n    print(\"âœ… YOLOv5ä»“åº“å…‹éš†å®Œæˆ\")\n\n# å®‰è£…/æ›´æ–°ä¾èµ–\nprint(\"ğŸ“¦ æ£€æŸ¥YOLOv5ä¾èµ–...\")\ntry:\n    os.chdir('yolov5')\n    !pip install -r requirements.txt --quiet\n    os.chdir('..')\n    print(\"âœ… YOLOv5ä¾èµ–æ£€æŸ¥å®Œæˆ\")\nexcept Exception as e:\n    print(f\"âš ï¸ ä¾èµ–å®‰è£…è­¦å‘Š: {e}\")\n\n# åˆ›å»ºä¸“ä¸šæ•°æ®é›†é…ç½®\ndataset_yaml = create_professional_dataset_config()\n\nif dataset_yaml is None:\n    print(\"âŒ æ— æ³•åˆ›å»ºæ•°æ®é›†é…ç½®ï¼Œè®­ç»ƒç»ˆæ­¢\")\nelse:\n    print(\"\\nğŸš€ å¼€å§‹YOLOv5ä¸“ä¸šè®­ç»ƒ...\")\n    \n    # æ ¹æ®ç¡¬ä»¶é…ç½®è®¾ç½®è®­ç»ƒå‚æ•°\n    if torch.cuda.is_available():\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        device_name = torch.cuda.get_device_name(0)\n        print(f\"ğŸ® GPU: {device_name} ({gpu_memory:.1f} GB)\")\n        \n        # æ ¹æ®GPUå†…å­˜ä¼˜åŒ–å‚æ•°\n        if gpu_memory >= 20:  # A100, RTX 4090ç­‰\n            model_size = 'yolov5l'\n            batch_size = 24\n            img_size = 640\n            epochs = 200\n            workers = 6\n        elif gpu_memory >= 15:  # L4, RTX 3090ç­‰\n            model_size = 'yolov5m'\n            batch_size = 20\n            img_size = 640\n            epochs = 150\n            workers = 4\n        elif gpu_memory >= 8:   # RTX 3070, V100ç­‰\n            model_size = 'yolov5s'\n            batch_size = 16\n            img_size = 640\n            epochs = 120\n            workers = 2\n        else:                   # GTX 1080, RTX 2070ç­‰\n            model_size = 'yolov5s'\n            batch_size = 8\n            img_size = 512\n            epochs = 100\n            workers = 2\n            \n        device_param = '0'\n    else:\n        print(\"ğŸ’» ä½¿ç”¨CPUè®­ç»ƒ (ä¸æ¨è)\")\n        model_size = 'yolov5s'\n        batch_size = 4\n        img_size = 512\n        epochs = 50\n        workers = 1\n        device_param = 'cpu'\n    \n    print(f\"\\nâš™ï¸  ä¸“ä¸šè®­ç»ƒé…ç½®:\")\n    print(f\"   æ¨¡å‹: {model_size}.pt (å®˜æ–¹é¢„è®­ç»ƒ)\")\n    print(f\"   å›¾åƒå°ºå¯¸: {img_size}\")\n    print(f\"   æ‰¹æ¬¡å¤§å°: {batch_size}\")\n    print(f\"   è®­ç»ƒè½®æ•°: {epochs}\")\n    print(f\"   æ•°æ®åŠ è½½çº¿ç¨‹: {workers}\")\n    print(f\"   è®¾å¤‡: {device_param}\")\n    \n    # æ„å»ºä¿®å¤çš„ä¸“ä¸šè®­ç»ƒå‘½ä»¤\n    professional_command = [\n        sys.executable, 'yolov5/train.py',\n        '--data', dataset_yaml,\n        '--weights', f'{model_size}.pt',        # å®˜æ–¹é¢„è®­ç»ƒæƒé‡\n        '--epochs', str(epochs),\n        '--batch-size', str(batch_size),\n        '--imgsz', str(img_size),\n        '--device', device_param,\n        '--workers', str(workers),\n        '--project', 'runs/train',\n        '--name', 'tree_detection_professional',\n        '--optimizer', 'SGD',                   # æ¨èçš„ä¼˜åŒ–å™¨\n        '--patience', '30',                     # æ—©åœè€å¿ƒå€¼\n        '--save-period', '25',                  # å®šæœŸä¿å­˜\n        '--cache',                              # ç¼“å­˜å›¾åƒ\n        '--exist-ok'\n    ]\n    \n    # é«˜ç«¯GPUçš„é¢å¤–ä¼˜åŒ–\n    if torch.cuda.is_available() and gpu_memory >= 8:\n        professional_command.append('--multi-scale')  # å¤šå°ºåº¦è®­ç»ƒ\n        if gpu_memory >= 15:\n            professional_command.append('--amp')       # æ··åˆç²¾åº¦è®­ç»ƒ\n    \n    print(f\"\\nğŸ”¥ ä¿®å¤åçš„è®­ç»ƒå‘½ä»¤:\")\n    cmd_str = ' '.join(professional_command)\n    print(f\"   {cmd_str}\")\n    \n    print(f\"\\nğŸš€ å¼€å§‹ä¸“ä¸šè®­ç»ƒ...\")\n    print(f\"ğŸ’¡ ç­–ç•¥: å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹ + ç®€åŒ–ç¨³å®šå‚æ•°\")\n    print(f\"ğŸ¯ ç›®æ ‡: é«˜è´¨é‡æ ‘æœ¨æ£€æµ‹æ¨¡å‹\")\n    print(\"=\" * 70)\n    \n    # æ‰§è¡Œä¸“ä¸šè®­ç»ƒ\n    try:\n        memory_cleanup()  # è®­ç»ƒå‰æ¸…ç†\n        \n        print(\"â³ è®­ç»ƒè¿›è¡Œä¸­ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n        result = subprocess.run(\n            professional_command,\n            capture_output=True,\n            text=True,\n            timeout=7200  # 2å°æ—¶è¶…æ—¶\n        )\n        \n        if result.returncode == 0:\n            print(\"ğŸ‰ ä¸“ä¸šè®­ç»ƒæˆåŠŸå®Œæˆ!\")\n            \n            # æ£€æŸ¥è®­ç»ƒç»“æœ\n            model_dir = 'runs/train/tree_detection_professional/weights'\n            if os.path.exists(model_dir):\n                best_model = os.path.join(model_dir, 'best.pt')\n                last_model = os.path.join(model_dir, 'last.pt')\n                \n                if os.path.exists(best_model):\n                    model_size_mb = os.path.getsize(best_model) / (1024 * 1024)\n                    print(f\"âœ… æœ€ä½³æ¨¡å‹: {best_model} ({model_size_mb:.1f}MB)\")\n                    final_model_path = best_model\n                elif os.path.exists(last_model):\n                    model_size_mb = os.path.getsize(last_model) / (1024 * 1024)\n                    print(f\"âœ… æœ€æ–°æ¨¡å‹: {last_model} ({model_size_mb:.1f}MB)\")\n                    final_model_path = last_model\n                else:\n                    final_model_path = None\n                    print(\"âš ï¸ æœªæ‰¾åˆ°è®­ç»ƒæ¨¡å‹\")\n                \n                # æ˜¾ç¤ºè®­ç»ƒæ€»ç»“\n                print(\"\\nğŸ“Š è®­ç»ƒè¾“å‡ºæ€»ç»“:\")\n                output_lines = result.stdout.split('\\n')\n                key_lines = []\n                for line in output_lines:\n                    if any(keyword in line.lower() for keyword in \n                          ['results', 'best', 'map', 'precision', 'recall', 'fitness', 'epoch']):\n                        key_lines.append(line)\n                \n                # æ˜¾ç¤ºæœ€å15è¡Œå…³é”®ä¿¡æ¯\n                for line in key_lines[-15:]:\n                    if line.strip():\n                        print(f\"   {line}\")\n                        \n            else:\n                print(\"âŒ è®­ç»ƒç»“æœç›®å½•ä¸å­˜åœ¨\")\n                final_model_path = None\n                \n        else:\n            print(f\"âŒ è®­ç»ƒå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}\")\n            print(\"\\né”™è¯¯è¾“å‡º:\")\n            if result.stderr:\n                print(result.stderr[:1500])\n            \n            print(\"\\næ ‡å‡†è¾“å‡ºç‰‡æ®µ:\")\n            if result.stdout:\n                stdout_lines = result.stdout.split('\\n')\n                for line in stdout_lines[-20:]:\n                    if line.strip():\n                        print(f\"   {line}\")\n            \n            final_model_path = None\n            \n    except subprocess.TimeoutExpired:\n        print(\"â° è®­ç»ƒè¶…æ—¶ï¼Œæ£€æŸ¥éƒ¨åˆ†ç»“æœ...\")\n        model_dir = 'runs/train/tree_detection_professional/weights'\n        if os.path.exists(os.path.join(model_dir, 'last.pt')):\n            final_model_path = os.path.join(model_dir, 'last.pt')\n            print(f\"âœ… æ‰¾åˆ°éƒ¨åˆ†è®­ç»ƒæ¨¡å‹: {final_model_path}\")\n        else:\n            final_model_path = None\n            \n    except Exception as e:\n        print(f\"âŒ è®­ç»ƒå¼‚å¸¸: {e}\")\n        final_model_path = None\n    \n    # æœ€ç»ˆæ¸…ç†å’Œæ€»ç»“\n    memory_cleanup()\n    \n    print(\"\\nğŸ¯ YOLOv5ä¸“ä¸šè®­ç»ƒå®Œæˆ\")\n    if final_model_path:\n        print(f\"âœ… ä¸“ä¸šè®­ç»ƒæ¨¡å‹: {final_model_path}\")\n        print(\"ğŸ”¥ åŸºäºå®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹å’Œç¨³å®šå‚æ•°\")\n        print(\"ğŸ“ˆ ä½¿ç”¨ç®€åŒ–é…ç½®ç¡®ä¿å…¼å®¹æ€§\")\n        print(\"ğŸš€ è¿è¡ŒCell 9è¿›è¡Œæ¨¡å‹æ¨ç†æµ‹è¯•\")\n    else:\n        print(\"âŒ è®­ç»ƒæœªæˆåŠŸå®Œæˆ\")\n        print(\"ğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:\")\n        print(\"   1. æ£€æŸ¥æ•°æ®é›†æ ¼å¼å’Œè·¯å¾„\")\n        print(\"   2. ç¡®è®¤GPUå†…å­˜å……è¶³\")\n        print(\"   3. æ£€æŸ¥YOLOv5ç¯å¢ƒå®‰è£…\")\n        print(\"   4. å°è¯•é™ä½batch_sizeå‚æ•°\")\n    \n    print(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. ä¸“ä¸šæ¨¡å‹æ¨ç†æµ‹è¯• - Professional Model Inference Testing\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def comprehensive_cleanup():\n",
    "    \"\"\"ç»¼åˆå†…å­˜æ¸…ç†\"\"\"\n",
    "    try:\n",
    "        print(\"ğŸ§¹ æ‰§è¡Œå†…å­˜æ¸…ç†...\")\n",
    "        \n",
    "        # Pythonåƒåœ¾å›æ”¶\n",
    "        collected = gc.collect()\n",
    "        print(f\"   åƒåœ¾å›æ”¶é‡Šæ”¾å¯¹è±¡: {collected}\")\n",
    "        \n",
    "        # CUDAç¼“å­˜æ¸…ç†\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "            print(f\"   CUDAå†…å­˜: å·²åˆ†é… {allocated:.1f}GB, å·²ä¿ç•™ {reserved:.1f}GB\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ æ¸…ç†è¿‡ç¨‹è­¦å‘Š: {e}\")\n",
    "        return False\n",
    "\n",
    "def professional_inference(model_path, test_images, confidence_threshold=0.25):\n",
    "    \"\"\"ä¸“ä¸šæ¨¡å‹æ¨ç†\"\"\"\n",
    "    print(f\"ğŸ” ä¸“ä¸šæ¨¡å‹æ¨ç†æµ‹è¯•: {model_path}\")\n",
    "    print(f\"ğŸ“Š æµ‹è¯•å›¾åƒæ•°é‡: {len(test_images)}\")\n",
    "    print(f\"ğŸ¯ ç½®ä¿¡åº¦é˜ˆå€¼: {confidence_threshold}\")\n",
    "    \n",
    "    # åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "    output_dir = 'professional_inference_results'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # æ„å»ºæ¨ç†å‘½ä»¤\n",
    "    inference_command = [\n",
    "        sys.executable, 'yolov5/detect.py',\n",
    "        '--weights', model_path,\n",
    "        '--source', str(test_images[0].parent),  # ä½¿ç”¨å›¾åƒç›®å½•\n",
    "        '--project', output_dir,\n",
    "        '--name', 'detect',\n",
    "        '--img', '640',\n",
    "        '--conf', str(confidence_threshold),\n",
    "        '--iou', '0.45',\n",
    "        '--max-det', '1000',\n",
    "        '--save-txt',\n",
    "        '--save-conf',\n",
    "        '--exist-ok'\n",
    "    ]\n",
    "    \n",
    "    # æ·»åŠ è®¾å¤‡å‚æ•°\n",
    "    if torch.cuda.is_available():\n",
    "        inference_command.extend(['--device', '0'])\n",
    "    else:\n",
    "        inference_command.extend(['--device', 'cpu'])\n",
    "    \n",
    "    print(f\"\\nğŸš€ æ‰§è¡Œæ¨ç†å‘½ä»¤:\")\n",
    "    print(f\"   {' '.join(inference_command)}\")\n",
    "    \n",
    "    # æ‰§è¡Œæ¨ç†\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            inference_command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶\n",
    "        )\n",
    "        \n",
    "        inference_time = time.time() - start_time\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"âœ… æ¨ç†æˆåŠŸå®Œæˆ (è€—æ—¶: {inference_time:.1f}s)\")\n",
    "            \n",
    "            # åˆ†ææ¨ç†ç»“æœ\n",
    "            results = analyze_inference_results(output_dir)\n",
    "            return results\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ æ¨ç†å¤±è´¥: {result.stderr[:500]}\")\n",
    "            return None\n",
    "            \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"â° æ¨ç†è¶…æ—¶\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ¨ç†å¼‚å¸¸: {e}\")\n",
    "        return None\n",
    "\n",
    "def analyze_inference_results(output_dir):\n",
    "    \"\"\"åˆ†ææ¨ç†ç»“æœ\"\"\"\n",
    "    print(\"\\nğŸ“Š åˆ†ææ¨ç†ç»“æœ...\")\n",
    "    \n",
    "    results_dir = Path(output_dir) / 'detect'\n",
    "    if not results_dir.exists():\n",
    "        print(\"âŒ æ¨ç†ç»“æœç›®å½•ä¸å­˜åœ¨\")\n",
    "        return None\n",
    "    \n",
    "    # ç»Ÿè®¡æ£€æµ‹ç»“æœ\n",
    "    total_detections = 0\n",
    "    confidence_scores = []\n",
    "    image_counts = {}\n",
    "    \n",
    "    # åˆ†ææ ‡ç­¾æ–‡ä»¶\n",
    "    label_dir = results_dir / 'labels'\n",
    "    if label_dir.exists():\n",
    "        label_files = list(label_dir.glob('*.txt'))\n",
    "        print(f\"   å¤„ç†æ ‡ç­¾æ–‡ä»¶: {len(label_files)}\")\n",
    "        \n",
    "        for label_file in label_files:\n",
    "            image_name = label_file.stem\n",
    "            detections_in_image = 0\n",
    "            \n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) >= 6:\n",
    "                            conf = float(parts[5])\n",
    "                            confidence_scores.append(conf)\n",
    "                            total_detections += 1\n",
    "                            detections_in_image += 1\n",
    "            \n",
    "            image_counts[image_name] = detections_in_image\n",
    "    \n",
    "    # æ£€æŸ¥è¾“å‡ºå›¾åƒ\n",
    "    output_images = list(results_dir.glob('*.jpg')) + list(results_dir.glob('*.png'))\n",
    "    \n",
    "    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯\n",
    "    if confidence_scores:\n",
    "        avg_confidence = np.mean(confidence_scores)\n",
    "        max_confidence = np.max(confidence_scores)\n",
    "        min_confidence = np.min(confidence_scores)\n",
    "        std_confidence = np.std(confidence_scores)\n",
    "        \n",
    "        # ç½®ä¿¡åº¦åˆ†å¸ƒç»Ÿè®¡\n",
    "        high_conf_count = sum(1 for c in confidence_scores if c > 0.7)\n",
    "        medium_conf_count = sum(1 for c in confidence_scores if 0.4 <= c <= 0.7)\n",
    "        low_conf_count = sum(1 for c in confidence_scores if c < 0.4)\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ æ£€æµ‹ç»Ÿè®¡:\")\n",
    "        print(f\"   æ€»æ£€æµ‹æ•°é‡: {total_detections}\")\n",
    "        print(f\"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}\")\n",
    "        print(f\"   ç½®ä¿¡åº¦èŒƒå›´: {min_confidence:.3f} - {max_confidence:.3f}\")\n",
    "        print(f\"   ç½®ä¿¡åº¦æ ‡å‡†å·®: {std_confidence:.3f}\")\n",
    "        print(f\"   é«˜ç½®ä¿¡åº¦æ£€æµ‹(>0.7): {high_conf_count}\")\n",
    "        print(f\"   ä¸­ç­‰ç½®ä¿¡åº¦æ£€æµ‹(0.4-0.7): {medium_conf_count}\")\n",
    "        print(f\"   ä½ç½®ä¿¡åº¦æ£€æµ‹(<0.4): {low_conf_count}\")\n",
    "        \n",
    "        # æ¯å¼ å›¾åƒæ£€æµ‹ç»Ÿè®¡\n",
    "        if image_counts:\n",
    "            avg_detections_per_image = np.mean(list(image_counts.values()))\n",
    "            max_detections = max(image_counts.values())\n",
    "            print(f\"\\nğŸ–¼ï¸ å›¾åƒæ£€æµ‹ç»Ÿè®¡:\")\n",
    "            print(f\"   å¤„ç†å›¾åƒæ•°é‡: {len(image_counts)}\")\n",
    "            print(f\"   å¹³å‡æ¯å›¾æ£€æµ‹æ•°: {avg_detections_per_image:.1f}\")\n",
    "            print(f\"   æœ€å¤§å•å›¾æ£€æµ‹æ•°: {max_detections}\")\n",
    "            \n",
    "            # æ˜¾ç¤ºæ£€æµ‹æœ€å¤šçš„å›¾åƒ\n",
    "            top_images = sorted(image_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            print(f\"   æ£€æµ‹æ•°æœ€å¤šçš„å›¾åƒ:\")\n",
    "            for img_name, count in top_images:\n",
    "                print(f\"     {img_name}: {count} ä¸ªæ£€æµ‹\")\n",
    "        \n",
    "        # è¾“å‡ºå›¾åƒä¿¡æ¯\n",
    "        if output_images:\n",
    "            print(f\"\\nğŸ“ è¾“å‡ºæ–‡ä»¶:\")\n",
    "            print(f\"   æ£€æµ‹ç»“æœå›¾åƒ: {len(output_images)}\")\n",
    "            for img_path in output_images[:5]:  # æ˜¾ç¤ºå‰5ä¸ª\n",
    "                print(f\"     {img_path.name}\")\n",
    "            if len(output_images) > 5:\n",
    "                print(f\"     ... è¿˜æœ‰ {len(output_images)-5} ä¸ªæ–‡ä»¶\")\n",
    "        \n",
    "        return {\n",
    "            'total_detections': total_detections,\n",
    "            'avg_confidence': avg_confidence,\n",
    "            'max_confidence': max_confidence,\n",
    "            'min_confidence': min_confidence,\n",
    "            'std_confidence': std_confidence,\n",
    "            'high_conf_count': high_conf_count,\n",
    "            'medium_conf_count': medium_conf_count,\n",
    "            'low_conf_count': low_conf_count,\n",
    "            'confidence_scores': confidence_scores,\n",
    "            'image_counts': image_counts,\n",
    "            'output_images': output_images,\n",
    "            'results_dir': str(results_dir)\n",
    "        }\n",
    "    else:\n",
    "        print(\"âš ï¸ æœªæ£€æµ‹åˆ°ä»»ä½•å¯¹è±¡\")\n",
    "        return {\n",
    "            'total_detections': 0,\n",
    "            'avg_confidence': 0,\n",
    "            'results_dir': str(results_dir)\n",
    "        }\n",
    "\n",
    "def visualize_results(results):\n",
    "    \"\"\"å¯è§†åŒ–æ¨ç†ç»“æœ\"\"\"\n",
    "    if not results or results['total_detections'] == 0:\n",
    "        print(\"ğŸ“Š æ— æ£€æµ‹ç»“æœå¯è§†åŒ–\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nğŸ“Š åˆ›å»ºç»“æœå¯è§†åŒ–å›¾è¡¨...\")\n",
    "    \n",
    "    try:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾\n",
    "        confidences = results['confidence_scores']\n",
    "        axes[0].hist(confidences, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[0].set_title('ğŸ¯ æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "        axes[0].set_xlabel('ç½®ä¿¡åº¦')\n",
    "        axes[0].set_ylabel('é¢‘æ¬¡')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # æ·»åŠ ç»Ÿè®¡çº¿\n",
    "        avg_conf = results['avg_confidence']\n",
    "        axes[0].axvline(avg_conf, color='red', linestyle='--', linewidth=2,\n",
    "                       label=f'å¹³å‡ç½®ä¿¡åº¦: {avg_conf:.3f}')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # ç½®ä¿¡åº¦ç­‰çº§é¥¼å›¾\n",
    "        high_count = results['high_conf_count']\n",
    "        medium_count = results['medium_conf_count'] \n",
    "        low_count = results['low_conf_count']\n",
    "        \n",
    "        labels = ['é«˜ç½®ä¿¡åº¦(>0.7)', 'ä¸­ç­‰ç½®ä¿¡åº¦(0.4-0.7)', 'ä½ç½®ä¿¡åº¦(<0.4)']\n",
    "        sizes = [high_count, medium_count, low_count]\n",
    "        colors = ['#2ecc71', '#f39c12', '#e74c3c']\n",
    "        \n",
    "        # è¿‡æ»¤æ‰ä¸º0çš„é¡¹\n",
    "        filtered_data = [(label, size, color) for label, size, color in zip(labels, sizes, colors) if size > 0]\n",
    "        if filtered_data:\n",
    "            labels, sizes, colors = zip(*filtered_data)\n",
    "            axes[1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "            axes[1].set_title('ğŸ¯ ç½®ä¿¡åº¦ç­‰çº§åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'æ— æ£€æµ‹æ•°æ®', ha='center', va='center', \n",
    "                        transform=axes[1].transAxes, fontsize=16)\n",
    "            axes[1].set_title('ğŸ¯ ç½®ä¿¡åº¦ç­‰çº§åˆ†å¸ƒ', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ… å¯è§†åŒ–å›¾è¡¨åˆ›å»ºå®Œæˆ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}\")\n",
    "\n",
    "def show_sample_results(results):\n",
    "    \"\"\"æ˜¾ç¤ºç¤ºä¾‹æ£€æµ‹ç»“æœ\"\"\"\n",
    "    if not results or not results.get('output_images'):\n",
    "        print(\"ğŸ“· æ— ç¤ºä¾‹å›¾åƒå¯æ˜¾ç¤º\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nğŸ“· æ˜¾ç¤ºæ£€æµ‹ç»“æœç¤ºä¾‹...\")\n",
    "    \n",
    "    try:\n",
    "        output_images = results['output_images']\n",
    "        sample_count = min(3, len(output_images))\n",
    "        \n",
    "        if sample_count > 0:\n",
    "            fig, axes = plt.subplots(1, sample_count, figsize=(5*sample_count, 5))\n",
    "            if sample_count == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            for i in range(sample_count):\n",
    "                img_path = output_images[i]\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is not None:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    axes[i].imshow(img_rgb)\n",
    "                    axes[i].set_title(f'æ£€æµ‹ç»“æœ: {img_path.name}', fontsize=12)\n",
    "                    axes[i].axis('off')\n",
    "                else:\n",
    "                    axes[i].text(0.5, 0.5, 'å›¾åƒåŠ è½½å¤±è´¥', ha='center', va='center',\n",
    "                               transform=axes[i].transAxes)\n",
    "                    axes[i].set_title(f'é”™è¯¯: {img_path.name}')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            print(f\"âœ… æ˜¾ç¤ºäº† {sample_count} ä¸ªæ£€æµ‹ç»“æœç¤ºä¾‹\")\n",
    "        else:\n",
    "            print(\"ğŸ“· æ²¡æœ‰å¯æ˜¾ç¤ºçš„æ£€æµ‹ç»“æœå›¾åƒ\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç¤ºä¾‹æ˜¾ç¤ºå¤±è´¥: {e}\")\n",
    "\n",
    "# ä¸»æ‰§è¡Œæµç¨‹\n",
    "print(\"ğŸš€ å¯åŠ¨ä¸“ä¸šæ¨¡å‹æ¨ç†æµ‹è¯•...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# æ¸…ç†å†…å­˜\n",
    "comprehensive_cleanup()\n",
    "\n",
    "# æŸ¥æ‰¾è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "model_candidates = [\n",
    "    'runs/train/tree_detection_professional/weights/best.pt',\n",
    "    'runs/train/tree_detection_professional/weights/last.pt',\n",
    "    'runs/train/tree_detection_modern/weights/best.pt',\n",
    "    'runs/train/tree_detection_basic/weights/best.pt'\n",
    "]\n",
    "\n",
    "available_model = None\n",
    "for model_path in model_candidates:\n",
    "    if os.path.exists(model_path):\n",
    "        model_size = os.path.getsize(model_path) / (1024 * 1024)\n",
    "        print(f\"ğŸ¯ æ‰¾åˆ°è®­ç»ƒæ¨¡å‹: {model_path} ({model_size:.1f}MB)\")\n",
    "        available_model = model_path\n",
    "        break\n",
    "\n",
    "if not available_model:\n",
    "    print(\"âŒ æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹\")\n",
    "    print(\"ğŸ’¡ è¯·å…ˆè¿è¡ŒCell 8å®Œæˆæ¨¡å‹è®­ç»ƒ\")\n",
    "else:\n",
    "    # æŸ¥æ‰¾æµ‹è¯•å›¾åƒ\n",
    "    test_dirs = [\n",
    "        'data/processed/images/test',\n",
    "        'data/processed/images/val', \n",
    "        'data/processed/images/train'\n",
    "    ]\n",
    "    \n",
    "    test_images = []\n",
    "    for test_dir in test_dirs:\n",
    "        if os.path.exists(test_dir):\n",
    "            images = list(Path(test_dir).glob('*.jpg'))\n",
    "            test_images.extend(images)\n",
    "            if len(test_images) >= 10:  # é™åˆ¶æµ‹è¯•å›¾åƒæ•°é‡\n",
    "                test_images = test_images[:10]\n",
    "                break\n",
    "    \n",
    "    if not test_images:\n",
    "        print(\"âŒ æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒ\")\n",
    "    else:\n",
    "        print(f\"ğŸ“¸ æ‰¾åˆ°æµ‹è¯•å›¾åƒ: {len(test_images)} å¼ \")\n",
    "        \n",
    "        # æ‰§è¡Œä¸“ä¸šæ¨ç†æµ‹è¯•\n",
    "        results = professional_inference(available_model, test_images)\n",
    "        \n",
    "        if results:\n",
    "            print(f\"\\nğŸ‰ æ¨ç†æµ‹è¯•å®Œæˆ!\")\n",
    "            \n",
    "            # å¯è§†åŒ–ç»“æœ\n",
    "            visualize_results(results)\n",
    "            \n",
    "            # æ˜¾ç¤ºç¤ºä¾‹ç»“æœ\n",
    "            show_sample_results(results)\n",
    "            \n",
    "            # æ€»ç»“æŠ¥å‘Š\n",
    "            print(f\"\\nğŸ“‹ æ¨ç†æµ‹è¯•æ€»ç»“æŠ¥å‘Š:\")\n",
    "            print(f\"   æ¨¡å‹: {available_model}\")\n",
    "            print(f\"   æµ‹è¯•å›¾åƒ: {len(test_images)}\")\n",
    "            print(f\"   æ€»æ£€æµ‹æ•°: {results['total_detections']}\")\n",
    "            if results['total_detections'] > 0:\n",
    "                print(f\"   å¹³å‡ç½®ä¿¡åº¦: {results['avg_confidence']:.3f}\")\n",
    "                print(f\"   é«˜è´¨é‡æ£€æµ‹(>0.7): {results['high_conf_count']}\")\n",
    "            print(f\"   ç»“æœç›®å½•: {results['results_dir']}\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ æ¨ç†æµ‹è¯•å¤±è´¥\")\n",
    "\n",
    "# æœ€ç»ˆæ¸…ç†\n",
    "comprehensive_cleanup()\n",
    "print(\"âœ… ä¸“ä¸šæ¨¡å‹æ¨ç†æµ‹è¯•å®Œæˆ\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. YOLOv5æ¨¡å‹è¯„ä¼°å’Œæ€§èƒ½åˆ†æ - YOLOv5 Model Evaluation and Performance Analysis\n",
    "def evaluate_yolov5_performance(model_path, test_images):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°YOLOv5æ¨¡å‹æ€§èƒ½\n",
    "    Evaluate YOLOv5 model performance\n",
    "    \"\"\"\n",
    "    print(\"ğŸ“Š å¼€å§‹YOLOv5æ¨¡å‹æ€§èƒ½è¯„ä¼° Starting YOLOv5 model performance evaluation...\")\n",
    "    \n",
    "    total_images = len(test_images)\n",
    "    total_detections = 0\n",
    "    confidence_scores = []\n",
    "    processing_times = []\n",
    "    \n",
    "    # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜å‚¨è¯„ä¼°ç»“æœ Create temporary directory for evaluation results\n",
    "    eval_dir = 'temp_eval'\n",
    "    os.makedirs(eval_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"ğŸ” è¯„ä¼° {total_images} å¼ å›¾åƒ Evaluating {total_images} images...\")\n",
    "    \n",
    "    # å¯¹æ¯å¼ å›¾åƒè¿›è¡Œæ£€æµ‹ Detect on each image\n",
    "    for i, img_path in enumerate(test_images):\n",
    "        try:\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # æ„å»ºæ£€æµ‹å‘½ä»¤ Build detection command\n",
    "            detect_command = [\n",
    "                sys.executable, 'yolov5/detect.py',\n",
    "                '--weights', model_path,\n",
    "                '--source', str(img_path),\n",
    "                '--project', eval_dir,\n",
    "                '--name', f'eval_{i}',\n",
    "                '--save-txt',  # ä¿å­˜æ£€æµ‹ç»“æœ save detection results\n",
    "                '--save-conf', # ä¿å­˜ç½®ä¿¡åº¦ save confidence scores\n",
    "                '--exist-ok',\n",
    "                '--nosave'     # ä¸ä¿å­˜å›¾åƒï¼Œåªè¦æ–‡æœ¬ç»“æœ don't save images, only text results\n",
    "            ]\n",
    "            \n",
    "            # è¿è¡Œæ£€æµ‹ Run detection\n",
    "            result = subprocess.run(detect_command, \n",
    "                                  capture_output=True, \n",
    "                                  text=True, \n",
    "                                  cwd='.')\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            processing_times.append(processing_time)\n",
    "            \n",
    "            if result.returncode == 0:\n",
    "                # è¯»å–æ£€æµ‹ç»“æœ Read detection results\n",
    "                result_dir = Path(eval_dir) / f'eval_{i}' / 'labels'\n",
    "                if result_dir.exists():\n",
    "                    for label_file in result_dir.glob('*.txt'):\n",
    "                        with open(label_file, 'r') as f:\n",
    "                            lines = f.readlines()\n",
    "                            for line in lines:\n",
    "                                if line.strip():\n",
    "                                    parts = line.strip().split()\n",
    "                                    if len(parts) >= 6:  # class x y w h conf\n",
    "                                        confidence = float(parts[5])\n",
    "                                        confidence_scores.append(confidence)\n",
    "                                        total_detections += 1\n",
    "                                        \n",
    "            if (i + 1) % 10 == 0 or i == total_images - 1:\n",
    "                print(f\"   å·²å¤„ç† {i+1}/{total_images} å¼ å›¾åƒ Processed {i+1}/{total_images} images\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"è¯„ä¼°å›¾åƒ {i+1} æ—¶å‡ºé”™ Error evaluating image {i+1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # æ¸…ç†ä¸´æ—¶ç›®å½• Clean up temporary directory\n",
    "    import shutil\n",
    "    if os.path.exists(eval_dir):\n",
    "        shutil.rmtree(eval_dir)\n",
    "    \n",
    "    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ Calculate statistics\n",
    "    avg_detections_per_image = total_detections / total_images if total_images > 0 else 0\n",
    "    avg_confidence = np.mean(confidence_scores) if confidence_scores else 0\n",
    "    max_confidence = np.max(confidence_scores) if confidence_scores else 0\n",
    "    min_confidence = np.min(confidence_scores) if confidence_scores else 0\n",
    "    avg_processing_time = np.mean(processing_times) if processing_times else 0\n",
    "    \n",
    "    print(\"\\nğŸ“ˆ YOLOv5æ¨¡å‹æ€§èƒ½ç»Ÿè®¡ YOLOv5 Model Performance Statistics\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"æ€»æµ‹è¯•å›¾åƒæ•°é‡ Total test images: {total_images}\")\n",
    "    print(f\"æ€»æ£€æµ‹æ•°é‡ Total detections: {total_detections}\")\n",
    "    print(f\"å¹³å‡æ¯å¼ å›¾åƒæ£€æµ‹æ•°é‡ Average detections per image: {avg_detections_per_image:.2f}\")\n",
    "    print(f\"å¹³å‡ç½®ä¿¡åº¦ Average confidence: {avg_confidence:.3f}\")\n",
    "    print(f\"æœ€é«˜ç½®ä¿¡åº¦ Maximum confidence: {max_confidence:.3f}\")\n",
    "    print(f\"æœ€ä½ç½®ä¿¡åº¦ Minimum confidence: {min_confidence:.3f}\")\n",
    "    print(f\"å¹³å‡å¤„ç†æ—¶é—´ Average processing time: {avg_processing_time:.3f} ç§’/å¼  seconds per image\")\n",
    "    \n",
    "    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ Create visualization charts\n",
    "    if confidence_scores or processing_times:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # ç½®ä¿¡åº¦åˆ†å¸ƒå›¾ Confidence distribution\n",
    "        if confidence_scores:\n",
    "            axes[0].hist(confidence_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "            axes[0].set_title('ğŸ¯ æ£€æµ‹ç½®ä¿¡åº¦åˆ†å¸ƒ Detection Confidence Distribution', fontsize=12, fontweight='bold')\n",
    "            axes[0].set_xlabel('ç½®ä¿¡åº¦ Confidence Score')\n",
    "            axes[0].set_ylabel('é¢‘æ¬¡ Frequency')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # æ·»åŠ ç»Ÿè®¡çº¿ Add statistical lines\n",
    "            axes[0].axvline(avg_confidence, color='red', linestyle='--', \n",
    "                          label=f'å¹³å‡å€¼ Mean: {avg_confidence:.3f}')\n",
    "            axes[0].legend()\n",
    "        else:\n",
    "            axes[0].text(0.5, 0.5, 'æ— ç½®ä¿¡åº¦æ•°æ®\\nNo Confidence Data', \n",
    "                       ha='center', va='center', transform=axes[0].transAxes)\n",
    "        \n",
    "        # å¤„ç†æ—¶é—´åˆ†å¸ƒå›¾ Processing time distribution\n",
    "        if processing_times:\n",
    "            axes[1].hist(processing_times, bins=20, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "            axes[1].set_title('â±ï¸ å¤„ç†æ—¶é—´åˆ†å¸ƒ Processing Time Distribution', fontsize=12, fontweight='bold')\n",
    "            axes[1].set_xlabel('å¤„ç†æ—¶é—´ Processing Time (seconds)')\n",
    "            axes[1].set_ylabel('é¢‘æ¬¡ Frequency')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # æ·»åŠ ç»Ÿè®¡çº¿ Add statistical lines\n",
    "            axes[1].axvline(avg_processing_time, color='red', linestyle='--', \n",
    "                          label=f'å¹³å‡å€¼ Mean: {avg_processing_time:.3f}s')\n",
    "            axes[1].legend()\n",
    "        else:\n",
    "            axes[1].text(0.5, 0.5, 'æ— å¤„ç†æ—¶é—´æ•°æ®\\nNo Processing Time Data', \n",
    "                       ha='center', va='center', transform=axes[1].transAxes)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'total_images': total_images,\n",
    "        'total_detections': total_detections,\n",
    "        'avg_detections_per_image': avg_detections_per_image,\n",
    "        'confidence_scores': confidence_scores,\n",
    "        'avg_confidence': avg_confidence,\n",
    "        'processing_times': processing_times,\n",
    "        'avg_processing_time': avg_processing_time\n",
    "    }\n",
    "\n",
    "def run_yolov5_validation(model_path, dataset_yaml):\n",
    "    \"\"\"\n",
    "    è¿è¡ŒYOLOv5å®˜æ–¹éªŒè¯è„šæœ¬\n",
    "    Run YOLOv5 official validation script\n",
    "    \"\"\"\n",
    "    print(\"ğŸ”¬ è¿è¡ŒYOLOv5å®˜æ–¹éªŒè¯ Running YOLOv5 official validation...\")\n",
    "    \n",
    "    try:\n",
    "        # æ„å»ºéªŒè¯å‘½ä»¤ Build validation command\n",
    "        val_command = [\n",
    "            sys.executable, 'yolov5/val.py',\n",
    "            '--weights', model_path,\n",
    "            '--data', dataset_yaml,\n",
    "            '--img', '640',\n",
    "            '--batch', '8',\n",
    "            '--conf', '0.001',  # ä½ç½®ä¿¡åº¦é˜ˆå€¼ä»¥è·å¾—æ›´å¤šæ£€æµ‹ low confidence threshold for more detections\n",
    "            '--iou', '0.6',     # IoUé˜ˆå€¼ IoU threshold\n",
    "            '--task', 'val',\n",
    "            '--device', '0' if torch.cuda.is_available() else 'cpu',\n",
    "            '--save-txt',\n",
    "            '--save-conf',\n",
    "            '--project', 'runs/val',\n",
    "            '--name', 'tree_detection_val',\n",
    "            '--exist-ok'\n",
    "        ]\n",
    "        \n",
    "        print(f\"éªŒè¯å‘½ä»¤ Validation command: {' '.join(val_command)}\")\n",
    "        \n",
    "        # è¿è¡ŒéªŒè¯ Run validation\n",
    "        result = subprocess.run(val_command, \n",
    "                              capture_output=True, \n",
    "                              text=True, \n",
    "                              cwd='.')\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"âœ… YOLOv5å®˜æ–¹éªŒè¯å®Œæˆ YOLOv5 official validation completed!\")\n",
    "            \n",
    "            # æ˜¾ç¤ºéªŒè¯è¾“å‡ºçš„å…³é”®ä¿¡æ¯ Show key information from validation output\n",
    "            output_lines = result.stdout.split('\\n')\n",
    "            print(\"\\nğŸ“Š éªŒè¯ç»“æœæ‘˜è¦ Validation Results Summary:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            for line in output_lines:\n",
    "                if any(keyword in line.lower() for keyword in ['precision', 'recall', 'map', 'f1']):\n",
    "                    print(f\"   {line.strip()}\")\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†ç»“æœæ–‡ä»¶ Check if result files were generated\n",
    "            val_results_dir = Path('runs/val/tree_detection_val')\n",
    "            if val_results_dir.exists():\n",
    "                print(f\"\\nğŸ“ éªŒè¯ç»“æœä¿å­˜åœ¨ Validation results saved in: {val_results_dir}\")\n",
    "                \n",
    "                # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶ List generated files\n",
    "                result_files = list(val_results_dir.glob('*'))\n",
    "                for f in result_files:\n",
    "                    if f.is_file():\n",
    "                        print(f\"   {f.name}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"âŒ YOLOv5éªŒè¯å¤±è´¥ YOLOv5 validation failed\")\n",
    "            print(f\"é”™è¯¯è¾“å‡º Error output: {result.stderr}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ Error during validation: {e}\")\n",
    "\n",
    "# å¦‚æœæœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè¿›è¡Œæ€§èƒ½è¯„ä¼° If trained model exists, perform performance evaluation\n",
    "if 'model_path' in locals() and model_path is not None and os.path.exists(model_path):\n",
    "    print(f\"ğŸ¯ ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ€§èƒ½è¯„ä¼° Using model for performance evaluation: {model_path}\")\n",
    "    \n",
    "    # è·å–æµ‹è¯•å›¾åƒ Get test images\n",
    "    test_image_dir = Path('data/processed/images/test')\n",
    "    eval_images = []\n",
    "    \n",
    "    if test_image_dir.exists():\n",
    "        eval_images = list(test_image_dir.glob('*.jpg'))\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰å¤„ç†è¿‡çš„æµ‹è¯•å›¾åƒï¼Œä½¿ç”¨åŸå§‹è¯„ä¼°å›¾åƒ If no processed test images, use original evaluation images\n",
    "    if not eval_images:\n",
    "        eval_rgb_dir = Path('data/raw/evaluation/RGB')\n",
    "        if eval_rgb_dir.exists():\n",
    "            eval_images = list(eval_rgb_dir.glob('*.tif'))[:20]  # é™åˆ¶ä¸º20å¼ å›¾åƒ\n",
    "            print(f\"ğŸ“¸ ä½¿ç”¨åŸå§‹è¯„ä¼°å›¾åƒ Using original evaluation images: {len(eval_images)}\")\n",
    "    \n",
    "    if eval_images:\n",
    "        # è¿›è¡Œæ€§èƒ½è¯„ä¼° Perform performance evaluation\n",
    "        performance_stats = evaluate_yolov5_performance(model_path, eval_images)\n",
    "        \n",
    "        # è¿è¡Œå®˜æ–¹éªŒè¯ï¼ˆå¦‚æœæ•°æ®é›†é…ç½®å­˜åœ¨ï¼‰Run official validation if dataset config exists\n",
    "        if 'dataset_yaml_path' in locals() and os.path.exists(dataset_yaml_path):\n",
    "            run_yolov5_validation(model_path, dataset_yaml_path)\n",
    "        \n",
    "    else:\n",
    "        print(\"âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•å›¾åƒï¼Œè·³è¿‡æ€§èƒ½è¯„ä¼° No test images found, skipping performance evaluation\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè·³è¿‡æ€§èƒ½è¯„ä¼° No trained model found, skipping performance evaluation\")\n",
    "    print(\"è¯·å…ˆå®Œæˆæ¨¡å‹è®­ç»ƒ Please complete model training first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ é¡¹ç›®æ€»ç»“ Project Summary\n",
    "\n",
    "### å®Œæˆçš„åŠŸèƒ½ Completed Features\n",
    "1. âœ… **ç¯å¢ƒæ£€æµ‹** - è‡ªåŠ¨æ£€æµ‹GPUå’ŒColabç¯å¢ƒ\n",
    "2. âœ… **æ•°æ®ä¸‹è½½** - è‡ªåŠ¨ä¸‹è½½NeonTreeEvaluationæ•°æ®é›†\n",
    "3. âœ… **æ•°æ®è½¬æ¢** - å°†NeonTreeæ ¼å¼è½¬æ¢ä¸ºYOLOæ ¼å¼\n",
    "4. âœ… **æ¨¡å‹è®­ç»ƒ** - ä½¿ç”¨YOLOv5è¿›è¡Œæ ‘æœ¨æ£€æµ‹è®­ç»ƒ\n",
    "5. âœ… **ç»“æœæ¨ç†** - å¯¹æµ‹è¯•å›¾åƒè¿›è¡Œæ ‘æœ¨æ£€æµ‹\n",
    "6. âœ… **ç»“æœå¯è§†åŒ–** - æ˜¾ç¤ºæ£€æµ‹ç»“æœå’Œæ€§èƒ½ç»Ÿè®¡\n",
    "\n",
    "### ä¸»è¦å‚æ•°è¯´æ˜ Key Parameter Explanations\n",
    "- **è®­ç»ƒè½®æ•° Epochs**: 50è½®ï¼ˆå¯æ ¹æ®æ•ˆæœè°ƒæ•´ï¼‰\n",
    "- **æ‰¹æ¬¡å¤§å° Batch Size**: 16ï¼ˆæ ¹æ®GPUå†…å­˜è°ƒæ•´ï¼‰\n",
    "- **å›¾åƒå°ºå¯¸ Image Size**: 640x640åƒç´ \n",
    "- **å­¦ä¹ ç‡ Learning Rate**: 0.001ï¼ˆAdamWä¼˜åŒ–å™¨ï¼‰\n",
    "- **æ•°æ®åˆ†å‰² Data Split**: 70%è®­ç»ƒ/20%éªŒè¯/10%æµ‹è¯•\n",
    "\n",
    "### ä½¿ç”¨è¯´æ˜ Usage Instructions\n",
    "1. åœ¨Google Colabä¸­è¿è¡Œæ‰€æœ‰ä»£ç å•å…ƒ\n",
    "2. ç¡®ä¿GPUç¯å¢ƒå¯ç”¨ä»¥åŠ é€Ÿè®­ç»ƒ\n",
    "3. æ ¹æ®éœ€è¦è°ƒæ•´è®­ç»ƒå‚æ•°\n",
    "4. æŸ¥çœ‹resultsç›®å½•ä¸­çš„æ£€æµ‹ç»“æœå›¾åƒ\n",
    "\n",
    "### æ•…éšœæ’é™¤ Troubleshooting\n",
    "- å¦‚æœå†…å­˜ä¸è¶³ï¼Œå‡å°batch_sizeå‚æ•°\n",
    "- å¦‚æœè®­ç»ƒæ—¶é—´è¿‡é•¿ï¼Œå‡å°‘epochsæ•°é‡\n",
    "- å¦‚æœæ£€æµ‹æ•ˆæœä¸ä½³ï¼Œå°è¯•å¢åŠ è®­ç»ƒè½®æ•°\n",
    "- ç¡®ä¿æ•°æ®é›†æ­£ç¡®ä¸‹è½½å’Œè§£å‹\n",
    "\n",
    "### è¿›ä¸€æ­¥æ”¹è¿› Further Improvements\n",
    "- æ•°æ®å¢å¼ºæŠ€æœ¯æå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›\n",
    "- è¶…å‚æ•°è°ƒä¼˜ä¼˜åŒ–æ¨¡å‹æ€§èƒ½\n",
    "- å¤šå°ºåº¦è®­ç»ƒæé«˜æ£€æµ‹ç²¾åº¦\n",
    "- æ¨¡å‹é›†æˆæå‡æ•´ä½“æ•ˆæœ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}